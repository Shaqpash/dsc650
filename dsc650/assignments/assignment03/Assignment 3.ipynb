{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting fastavro\n",
      "  Downloading fastavro-1.7.3-cp39-cp39-win_amd64.whl (457 kB)\n",
      "     -------------------------------------- 457.6/457.6 kB 5.7 MB/s eta 0:00:00\n",
      "Installing collected packages: fastavro\n",
      "Successfully installed fastavro-1.7.3\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install fastavro"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pygeohash\n",
      "  Downloading pygeohash-1.2.0.tar.gz (5.0 kB)\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Building wheels for collected packages: pygeohash\n",
      "  Building wheel for pygeohash (setup.py): started\n",
      "  Building wheel for pygeohash (setup.py): finished with status 'done'\n",
      "  Created wheel for pygeohash: filename=pygeohash-1.2.0-py2.py3-none-any.whl size=6179 sha256=0beb8f67a6723029369f62a4bcda667144b8cf5c8b1abaec014fb8195fb7f69d\n",
      "  Stored in directory: c:\\users\\spashtunyar\\appdata\\local\\pip\\cache\\wheels\\0e\\15\\96\\0d48066e6c37ed790f81f42155a675946c7d325d3995e3a469\n",
      "Successfully built pygeohash\n",
      "Installing collected packages: pygeohash\n",
      "Successfully installed pygeohash-1.2.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install pygeohash"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import gzip\n",
    "import json\n",
    "from pathlib import Path\n",
    "import csv\n",
    "\n",
    "import pandas as pd\n",
    "import s3fs\n",
    "import pyarrow as pa\n",
    "from pyarrow.json import read_json\n",
    "import pyarrow.parquet as pq\n",
    "import fastavro\n",
    "import pygeohash\n",
    "import snappy\n",
    "import jsonschema\n",
    "from jsonschema.exceptions import ValidationError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "current_dir = Path(os.getcwd()).absolute()\n",
    "schema_dir = current_dir.joinpath('schemas')\n",
    "results_dir = current_dir.joinpath('results')\n",
    "results_dir.mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import libraries and define common helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#rewriting to pull from the path in my own directory\n",
    "def read_jsonl_data_sp():\n",
    "    src_data_path = r'C:\\Users\\spashtunyar\\Documents\\School\\dsc650\\data\\processed\\openflights\\routes.jsonl.gz'\n",
    "    with open(src_data_path, 'rb') as f_gz:\n",
    "        with gzip.open(src_data_path, 'rb') as f:\n",
    "            records = [json.loads(line) for line in f.readlines()]\n",
    "        \n",
    "\n",
    "    return records"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the records from https://storage.budsc.midwest-datascience.com/data/processed/openflights/routes.jsonl.gz "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "records = read_jsonl_data_sp()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'airline': {'airline_id': 410,\n",
       "   'name': 'Aerocondor',\n",
       "   'alias': 'ANA All Nippon Airways',\n",
       "   'iata': '2B',\n",
       "   'icao': 'ARD',\n",
       "   'callsign': 'AEROCONDOR',\n",
       "   'country': 'Portugal',\n",
       "   'active': True},\n",
       "  'src_airport': {'airport_id': 2965,\n",
       "   'name': 'Sochi International Airport',\n",
       "   'city': 'Sochi',\n",
       "   'country': 'Russia',\n",
       "   'iata': 'AER',\n",
       "   'icao': 'URSS',\n",
       "   'latitude': 43.449902,\n",
       "   'longitude': 39.9566,\n",
       "   'altitude': 89,\n",
       "   'timezone': 3.0,\n",
       "   'dst': 'N',\n",
       "   'tz_id': 'Europe/Moscow',\n",
       "   'type': 'airport',\n",
       "   'source': 'OurAirports'},\n",
       "  'dst_airport': {'airport_id': 2990,\n",
       "   'name': 'Kazan International Airport',\n",
       "   'city': 'Kazan',\n",
       "   'country': 'Russia',\n",
       "   'iata': 'KZN',\n",
       "   'icao': 'UWKD',\n",
       "   'latitude': 55.606201171875,\n",
       "   'longitude': 49.278701782227,\n",
       "   'altitude': 411,\n",
       "   'timezone': 3.0,\n",
       "   'dst': 'N',\n",
       "   'tz_id': 'Europe/Moscow',\n",
       "   'type': 'airport',\n",
       "   'source': 'OurAirports'},\n",
       "  'codeshare': False,\n",
       "  'equipment': ['CR2']}]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#val\n",
    "records[0:1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1.a JSON Schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def validate_jsonl_data(records):\n",
    "    schema_path = schema_dir.joinpath('routes-schema.json')\n",
    "    with open(schema_path) as f:\n",
    "        schema = json.load(f)\n",
    "        \n",
    "    with open('validation_csv_path', 'w', encoding='utf-8') as f:    \n",
    "        for i, record in enumerate(records):\n",
    "            try:\n",
    "                ## TODO: Validate record \n",
    "                jsonschema.validate(record, schema)\n",
    "                pass\n",
    "            except ValidationError as e:\n",
    "                ## Print message if invalid record\n",
    "                f.write(f\"Error: {e.message}; failed validating {e.validator} in schema {e.schema_path}\\r\\n\")\n",
    "                print(e)\n",
    "                pass\n",
    "            \n",
    "\n",
    "validate_jsonl_data(records)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1.b Avro"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_avro_dataset(records):\n",
    "    schema_path = schema_dir.joinpath('routes.avsc')\n",
    "    data_path = results_dir.joinpath('routes.avro')\n",
    "    ## TODO: Use fastavro to create Avro dataset\n",
    "    \n",
    "        \n",
    "create_avro_dataset(records)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1.c Parquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_parquet_dataset():\n",
    "    src_data_path = 'data/processed/openflights/routes.jsonl.gz'\n",
    "    parquet_output_path = results_dir.joinpath('routes.parquet')\n",
    "    s3 = s3fs.S3FileSystem(\n",
    "        anon=True,\n",
    "        client_kwargs={\n",
    "            'endpoint_url': endpoint_url\n",
    "        }\n",
    "    )\n",
    "    \n",
    "    with s3.open(src_data_path, 'rb') as f_gz:\n",
    "        with gzip.open(f_gz, 'rb') as f:\n",
    "            pass\n",
    "            ## TODO: Use Apache Arrow to create Parquet table and save the dataset\n",
    "\n",
    "create_parquet_dataset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1.d Protocol Buffers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.insert(0, os.path.abspath('routes_pb2'))\n",
    "\n",
    "import routes_pb2\n",
    "\n",
    "def _airport_to_proto_obj(airport):\n",
    "    obj = routes_pb2.Airport()\n",
    "    if airport is None:\n",
    "        return None\n",
    "    if airport.get('airport_id') is None:\n",
    "        return None\n",
    "\n",
    "    obj.airport_id = airport.get('airport_id')\n",
    "    if airport.get('name'):\n",
    "        obj.name = airport.get('name')\n",
    "    if airport.get('city'):\n",
    "        obj.city = airport.get('city')\n",
    "    if airport.get('iata'):\n",
    "        obj.iata = airport.get('iata')\n",
    "    if airport.get('icao'):\n",
    "        obj.icao = airport.get('icao')\n",
    "    if airport.get('altitude'):\n",
    "        obj.altitude = airport.get('altitude')\n",
    "    if airport.get('timezone'):\n",
    "        obj.timezone = airport.get('timezone')\n",
    "    if airport.get('dst'):\n",
    "        obj.dst = airport.get('dst')\n",
    "    if airport.get('tz_id'):\n",
    "        obj.tz_id = airport.get('tz_id')\n",
    "    if airport.get('type'):\n",
    "        obj.type = airport.get('type')\n",
    "    if airport.get('source'):\n",
    "        obj.source = airport.get('source')\n",
    "\n",
    "    obj.latitude = airport.get('latitude')\n",
    "    obj.longitude = airport.get('longitude')\n",
    "\n",
    "    return obj\n",
    "\n",
    "\n",
    "def _airline_to_proto_obj(airline):\n",
    "    obj = routes_pb2.Airline()\n",
    "    ## TODO: Create an Airline obj using Protocol Buffers API\n",
    "    return obj\n",
    "\n",
    "\n",
    "def create_protobuf_dataset(records):\n",
    "    routes = routes_pb2.Routes()\n",
    "    for record in records:\n",
    "        route = routes_pb2.Route()\n",
    "        ## TODO: Implement the code to create the Protocol Buffers Dataset\n",
    "\n",
    "        routes.route.append(route)\n",
    "\n",
    "    data_path = results_dir.joinpath('routes.pb')\n",
    "\n",
    "    with open(data_path, 'wb') as f:\n",
    "        f.write(routes.SerializeToString())\n",
    "        \n",
    "    compressed_path = results_dir.joinpath('routes.pb.snappy')\n",
    "    \n",
    "    with open(compressed_path, 'wb') as f:\n",
    "        f.write(snappy.compress(routes.SerializeToString()))\n",
    "        \n",
    "create_protobuf_dataset(records)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2.a Simple Geohash Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_hash_dirs(records):\n",
    "    geoindex_dir = results_dir.joinpath('geoindex')\n",
    "    geoindex_dir.mkdir(exist_ok=True, parents=True)\n",
    "    hashes = []\n",
    "    ## TODO: Create hash index\n",
    "            \n",
    "create_hash_dirs(records)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2.b Simple Search Feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def airport_search(latitude, longitude):\n",
    "    ## TODO: Create simple search to return nearest airport\n",
    "    pass\n",
    "    \n",
    "airport_search(41.1499988, -95.91779)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
