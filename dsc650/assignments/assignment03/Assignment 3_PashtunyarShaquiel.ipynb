{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pyarrow in c:\\users\\spashtunyar\\anaconda3\\lib\\site-packages (8.0.0)\n",
      "Requirement already satisfied: numpy>=1.16.6 in c:\\users\\spashtunyar\\anaconda3\\lib\\site-packages (from pyarrow) (1.23.5)\n",
      "Requirement already satisfied: fastavro in c:\\users\\spashtunyar\\anaconda3\\lib\\site-packages (1.7.3)\n",
      "Requirement already satisfied: pygeohash in c:\\users\\spashtunyar\\anaconda3\\lib\\site-packages (1.2.0)\n",
      "Collecting snappy\n",
      "  Downloading snappy-3.0.3-cp39-cp39-win_amd64.whl (11.6 MB)\n",
      "     --------------------------------------- 11.6/11.6 MB 10.1 MB/s eta 0:00:00\n",
      "Collecting plink>=2.4.1\n",
      "  Downloading plink-2.4.1-py3-none-any.whl (339 kB)\n",
      "     ------------------------------------- 339.5/339.5 kB 10.6 MB/s eta 0:00:00\n",
      "Requirement already satisfied: decorator in c:\\users\\spashtunyar\\anaconda3\\lib\\site-packages (from snappy) (5.1.1)\n",
      "Collecting FXrays>=1.3\n",
      "  Downloading FXrays-1.3.5-cp39-cp39-win_amd64.whl (25 kB)\n",
      "Collecting snappy-manifolds>=1.1.2\n",
      "  Downloading snappy_manifolds-1.1.2-py3-none-any.whl (45.0 MB)\n",
      "     --------------------------------------- 45.0/45.0 MB 18.7 MB/s eta 0:00:00\n",
      "Collecting pypng\n",
      "  Downloading pypng-0.20220715.0-py3-none-any.whl (58 kB)\n",
      "     ---------------------------------------- 58.1/58.1 kB 3.0 MB/s eta 0:00:00\n",
      "Collecting spherogram>=2.1\n",
      "  Downloading spherogram-2.1-cp39-cp39-win_amd64.whl (319 kB)\n",
      "     ------------------------------------- 319.5/319.5 kB 19.3 MB/s eta 0:00:00\n",
      "Requirement already satisfied: ipython>=5.0 in c:\\users\\spashtunyar\\anaconda3\\lib\\site-packages (from snappy) (8.10.0)\n",
      "Collecting cypari>=2.3\n",
      "  Downloading cypari-2.4.1-cp39-cp39-win_amd64.whl (5.2 MB)\n",
      "     ---------------------------------------- 5.2/5.2 MB 19.5 MB/s eta 0:00:00\n",
      "Requirement already satisfied: future in c:\\users\\spashtunyar\\anaconda3\\lib\\site-packages (from cypari>=2.3->snappy) (0.18.2)\n",
      "Requirement already satisfied: six in c:\\users\\spashtunyar\\anaconda3\\lib\\site-packages (from cypari>=2.3->snappy) (1.16.0)\n",
      "Requirement already satisfied: pickleshare in c:\\users\\spashtunyar\\anaconda3\\lib\\site-packages (from ipython>=5.0->snappy) (0.7.5)\n",
      "Requirement already satisfied: traitlets>=5 in c:\\users\\spashtunyar\\anaconda3\\lib\\site-packages (from ipython>=5.0->snappy) (5.7.1)\n",
      "Requirement already satisfied: colorama in c:\\users\\spashtunyar\\anaconda3\\lib\\site-packages (from ipython>=5.0->snappy) (0.4.6)\n",
      "Requirement already satisfied: backcall in c:\\users\\spashtunyar\\anaconda3\\lib\\site-packages (from ipython>=5.0->snappy) (0.2.0)\n",
      "Requirement already satisfied: prompt-toolkit<3.1.0,>=3.0.30 in c:\\users\\spashtunyar\\anaconda3\\lib\\site-packages (from ipython>=5.0->snappy) (3.0.36)\n",
      "Requirement already satisfied: pygments>=2.4.0 in c:\\users\\spashtunyar\\anaconda3\\lib\\site-packages (from ipython>=5.0->snappy) (2.11.2)\n",
      "Requirement already satisfied: jedi>=0.16 in c:\\users\\spashtunyar\\anaconda3\\lib\\site-packages (from ipython>=5.0->snappy) (0.18.1)\n",
      "Requirement already satisfied: matplotlib-inline in c:\\users\\spashtunyar\\anaconda3\\lib\\site-packages (from ipython>=5.0->snappy) (0.1.6)\n",
      "Requirement already satisfied: stack-data in c:\\users\\spashtunyar\\anaconda3\\lib\\site-packages (from ipython>=5.0->snappy) (0.2.0)\n",
      "Requirement already satisfied: networkx in c:\\users\\spashtunyar\\anaconda3\\lib\\site-packages (from spherogram>=2.1->snappy) (2.8.4)\n",
      "Collecting knot-floer-homology>=1.1\n",
      "  Downloading knot_floer_homology-1.2-cp39-cp39-win_amd64.whl (67 kB)\n",
      "     ---------------------------------------- 67.9/67.9 kB ? eta 0:00:00\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.0 in c:\\users\\spashtunyar\\anaconda3\\lib\\site-packages (from jedi>=0.16->ipython>=5.0->snappy) (0.8.3)\n",
      "Requirement already satisfied: wcwidth in c:\\users\\spashtunyar\\anaconda3\\lib\\site-packages (from prompt-toolkit<3.1.0,>=3.0.30->ipython>=5.0->snappy) (0.2.5)\n",
      "Requirement already satisfied: pure-eval in c:\\users\\spashtunyar\\anaconda3\\lib\\site-packages (from stack-data->ipython>=5.0->snappy) (0.2.2)\n",
      "Requirement already satisfied: asttokens in c:\\users\\spashtunyar\\anaconda3\\lib\\site-packages (from stack-data->ipython>=5.0->snappy) (2.0.5)\n",
      "Requirement already satisfied: executing in c:\\users\\spashtunyar\\anaconda3\\lib\\site-packages (from stack-data->ipython>=5.0->snappy) (0.8.3)\n",
      "Installing collected packages: snappy-manifolds, pypng, knot-floer-homology, spherogram, plink, FXrays, cypari, snappy\n",
      "Successfully installed FXrays-1.3.5 cypari-2.4.1 knot-floer-homology-1.2 plink-2.4.1 pypng-0.20220715.0 snappy-3.0.3 snappy-manifolds-1.1.2 spherogram-2.1\n",
      "Requirement already satisfied: jsonschema in c:\\users\\spashtunyar\\anaconda3\\lib\\site-packages (4.17.3)\n",
      "Requirement already satisfied: pyrsistent!=0.17.0,!=0.17.1,!=0.17.2,>=0.14.0 in c:\\users\\spashtunyar\\anaconda3\\lib\\site-packages (from jsonschema) (0.18.0)\n",
      "Requirement already satisfied: attrs>=17.4.0 in c:\\users\\spashtunyar\\anaconda3\\lib\\site-packages (from jsonschema) (22.1.0)\n",
      "Collecting google\n",
      "  Downloading google-3.0.0-py2.py3-none-any.whl (45 kB)\n",
      "     ---------------------------------------- 45.3/45.3 kB 1.1 MB/s eta 0:00:00\n",
      "Requirement already satisfied: beautifulsoup4 in c:\\users\\spashtunyar\\anaconda3\\lib\\site-packages (from google) (4.11.1)\n",
      "Requirement already satisfied: soupsieve>1.2 in c:\\users\\spashtunyar\\anaconda3\\lib\\site-packages (from beautifulsoup4->google) (2.3.2.post1)\n",
      "Installing collected packages: google\n",
      "Successfully installed google-3.0.0\n",
      "Requirement already satisfied: protobuf in c:\\users\\spashtunyar\\anaconda3\\lib\\site-packages (3.20.3)\n"
     ]
    }
   ],
   "source": [
    "! pip install pyarrow\n",
    "! pip install fastavro\n",
    "! pip install pygeohash\n",
    "! pip install snappy\n",
    "! pip install jsonschema\n",
    "! pip install google\n",
    "! pip install protobuf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import gzip\n",
    "import json\n",
    "from pathlib import Path\n",
    "import csv\n",
    "\n",
    "import pandas as pd\n",
    "import s3fs\n",
    "import pyarrow as pa\n",
    "from pyarrow.json import read_json\n",
    "import pyarrow.parquet as pq\n",
    "import fastavro\n",
    "import pygeohash\n",
    "import snappy\n",
    "import jsonschema\n",
    "from jsonschema.exceptions import ValidationError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "current_dir = Path(os.getcwd()).absolute()\n",
    "schema_dir = current_dir.joinpath('schemas')\n",
    "results_dir = current_dir.joinpath('results')\n",
    "results_dir.mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import libraries and define common helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#rewriting to pull from the path in my own directory\n",
    "def read_jsonl_data_sp():\n",
    "    src_data_path = r'C:\\Users\\spashtunyar\\Documents\\School\\dsc650\\data\\processed\\openflights\\routes.jsonl.gz'\n",
    "    with open(src_data_path, 'rb') as f_gz:\n",
    "        with gzip.open(src_data_path, 'rb') as f:\n",
    "            records = [json.loads(line) for line in f.readlines()]\n",
    "        \n",
    "\n",
    "    return records"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the records from local directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "records = read_jsonl_data_sp()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'airline': {'airline_id': 410,\n",
       "   'name': 'Aerocondor',\n",
       "   'alias': 'ANA All Nippon Airways',\n",
       "   'iata': '2B',\n",
       "   'icao': 'ARD',\n",
       "   'callsign': 'AEROCONDOR',\n",
       "   'country': 'Portugal',\n",
       "   'active': True},\n",
       "  'src_airport': {'airport_id': 2965,\n",
       "   'name': 'Sochi International Airport',\n",
       "   'city': 'Sochi',\n",
       "   'country': 'Russia',\n",
       "   'iata': 'AER',\n",
       "   'icao': 'URSS',\n",
       "   'latitude': 43.449902,\n",
       "   'longitude': 39.9566,\n",
       "   'altitude': 89,\n",
       "   'timezone': 3.0,\n",
       "   'dst': 'N',\n",
       "   'tz_id': 'Europe/Moscow',\n",
       "   'type': 'airport',\n",
       "   'source': 'OurAirports'},\n",
       "  'dst_airport': {'airport_id': 2990,\n",
       "   'name': 'Kazan International Airport',\n",
       "   'city': 'Kazan',\n",
       "   'country': 'Russia',\n",
       "   'iata': 'KZN',\n",
       "   'icao': 'UWKD',\n",
       "   'latitude': 55.606201171875,\n",
       "   'longitude': 49.278701782227,\n",
       "   'altitude': 411,\n",
       "   'timezone': 3.0,\n",
       "   'dst': 'N',\n",
       "   'tz_id': 'Europe/Moscow',\n",
       "   'type': 'airport',\n",
       "   'source': 'OurAirports'},\n",
       "  'codeshare': False,\n",
       "  'equipment': ['CR2']}]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#val\n",
    "records[0:1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1.a JSON Schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def validate_jsonl_data(records):\n",
    "    schema_path = schema_dir.joinpath('routes-schema.json')\n",
    "    with open(schema_path) as f:\n",
    "        schema = json.load(f)\n",
    "        \n",
    "    with open('validation_csv_path', 'w', encoding='utf-8') as f:    \n",
    "        for i, record in enumerate(records):\n",
    "            try:\n",
    "                ## TODO: Validate record \n",
    "                jsonschema.validate(record, schema)\n",
    "                pass\n",
    "            except ValidationError as e:\n",
    "                ## Print message if invalid record\n",
    "                f.write(f\"Error: {e.message}; failed validating {e.validator} in schema {e.schema_path}\\r\\n\")\n",
    "                print(e)\n",
    "                pass\n",
    "            \n",
    "\n",
    "validate_jsonl_data(records)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1.b Avro"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastavro.schema import load_schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'type': 'record',\n",
       " 'name': 'Route',\n",
       " 'namespace': 'edu.bellevue.dsc650',\n",
       " 'fields': [{'name': 'airline',\n",
       "   'type': {'type': 'record',\n",
       "    'name': 'Airline',\n",
       "    'fields': [{'name': 'airline_id', 'type': 'int', 'default': -1},\n",
       "     {'name': 'name', 'type': 'string', 'default': 'NONE'},\n",
       "     {'name': 'alias', 'type': 'string', 'default': 'NONE'},\n",
       "     {'name': 'iata', 'type': 'string', 'default': 'NONE'},\n",
       "     {'name': 'icao', 'type': 'string', 'default': 'NONE'},\n",
       "     {'name': 'callsign', 'type': 'string', 'default': 'NONE'},\n",
       "     {'name': 'country', 'type': 'string', 'default': 'NONE'},\n",
       "     {'name': 'active', 'type': 'boolean', 'default': False}]},\n",
       "   'default': 'NONE'},\n",
       "  {'name': 'src_airport',\n",
       "   'type': [{'type': 'record',\n",
       "     'name': 'Airport',\n",
       "     'fields': [{'name': 'airport_id', 'type': 'int', 'default': -1},\n",
       "      {'name': 'name', 'type': 'string', 'default': 'NONE'},\n",
       "      {'name': 'city', 'type': 'string', 'default': 'NONE'},\n",
       "      {'name': 'iata', 'type': 'string', 'default': 'NONE'},\n",
       "      {'name': 'icao', 'type': 'string', 'default': 'NONE'},\n",
       "      {'name': 'latitude', 'type': 'double'},\n",
       "      {'name': 'longitude', 'type': 'double'},\n",
       "      {'name': 'timezone', 'type': 'double'},\n",
       "      {'name': 'dst', 'type': 'string', 'default': 'NONE'},\n",
       "      {'name': 'tz_id', 'type': 'string', 'default': 'NONE'},\n",
       "      {'name': 'type', 'type': 'string', 'default': 'NONE'},\n",
       "      {'name': 'source', 'type': 'string', 'default': 'NONE'}]},\n",
       "    'null'],\n",
       "   'default': 'NONE'},\n",
       "  {'name': 'dst_airport', 'type': ['Airport', 'null'], 'default': 'NONE'},\n",
       "  {'name': 'codeshare', 'type': 'boolean', 'default': False},\n",
       "  {'name': 'stops', 'type': 'int', 'default': 0},\n",
       "  {'name': 'equipment', 'type': {'type': 'array', 'items': 'string'}}]}"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastavro import writer, reader, parse_schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting fastavro==1.5.1\n",
      "  Downloading fastavro-1.5.1-cp39-cp39-win_amd64.whl (435 kB)\n",
      "     -------------------------------------- 435.4/435.4 kB 3.0 MB/s eta 0:00:00\n",
      "Installing collected packages: fastavro\n",
      "  Attempting uninstall: fastavro\n",
      "    Found existing installation: fastavro 1.7.3\n",
      "    Uninstalling fastavro-1.7.3:\n",
      "      Successfully uninstalled fastavro-1.7.3\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: Could not install packages due to an OSError: [WinError 5] Access is denied: 'C:\\\\Users\\\\spashtunyar\\\\Anaconda3\\\\Lib\\\\site-packages\\\\~astavro\\\\_logical_readers.cp39-win_amd64.pyd'\n",
      "Consider using the `--user` option or check the permissions.\n",
      "\n"
     ]
    }
   ],
   "source": [
    " pip install fastavro==1.5.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "#got it to work FINALLY!!! Abed in the teams chat provided a fixed acsc schema file that I replaced. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_avro_dataset(records):\n",
    "    schema_path = schema_dir.joinpath('routes.avsc')\n",
    "    data_path = results_dir.joinpath('routes.avro')\n",
    "    ## TODO: Use fastavro to create Avro dataset\n",
    "    with open(schema_path, 'r') as f1:\n",
    "        schema = json.loads(f1.read())\n",
    "    parsed_schema = fastavro.parse_schema(schema)\n",
    "    ## create dataset\n",
    "    with open(data_path, 'wb') as out:\n",
    "        fastavro.writer(out, parsed_schema, records)\n",
    "        \n",
    "create_avro_dataset(records)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                             airline  \\\n",
      "0  {'airline_id': 410, 'name': 'Aerocondor', 'ali...   \n",
      "1  {'airline_id': 410, 'name': 'Aerocondor', 'ali...   \n",
      "2  {'airline_id': 410, 'name': 'Aerocondor', 'ali...   \n",
      "3  {'airline_id': 410, 'name': 'Aerocondor', 'ali...   \n",
      "4  {'airline_id': 410, 'name': 'Aerocondor', 'ali...   \n",
      "\n",
      "                                         src_airport  \\\n",
      "0  {'airport_id': 2965, 'name': 'Sochi Internatio...   \n",
      "1  {'airport_id': 2966, 'name': 'Astrakhan Airpor...   \n",
      "2  {'airport_id': 2966, 'name': 'Astrakhan Airpor...   \n",
      "3  {'airport_id': 2968, 'name': 'Chelyabinsk Bala...   \n",
      "4  {'airport_id': 2968, 'name': 'Chelyabinsk Bala...   \n",
      "\n",
      "                                         dst_airport  codeshare  stops  \\\n",
      "0  {'airport_id': 2990, 'name': 'Kazan Internatio...      False      0   \n",
      "1  {'airport_id': 2990, 'name': 'Kazan Internatio...      False      0   \n",
      "2  {'airport_id': 2962, 'name': 'Mineralnyye Vody...      False      0   \n",
      "3  {'airport_id': 2990, 'name': 'Kazan Internatio...      False      0   \n",
      "4  {'airport_id': 4078, 'name': 'Tolmachevo Airpo...      False      0   \n",
      "\n",
      "  equipment  \n",
      "0     [CR2]  \n",
      "1     [CR2]  \n",
      "2     [CR2]  \n",
      "3     [CR2]  \n",
      "4     [CR2]  \n"
     ]
    }
   ],
   "source": [
    "# validation of what I created\n",
    "data_path = results_dir.joinpath('routes.avro')\n",
    "with open(data_path, mode = 'rb') as f:\n",
    "    reader = fastavro.reader(f)\n",
    "    records = [r for r in reader]\n",
    "    df = pd.DataFrame.from_records(records)\n",
    "    print(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1.c Parquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_parquet_dataset():\n",
    "    src_data_path = r'C:\\Users\\spashtunyar\\Documents\\School\\dsc650\\data\\processed\\openflights\\routes.jsonl.gz'\n",
    "    parquet_output_path = results_dir.joinpath('routes.parquet')\n",
    "    with gzip.open(src_data_path, 'rb') as f:\n",
    "        table = read_json(f)\n",
    "    pq.write_table(table, parquet_output_path)\n",
    "\n",
    "create_parquet_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pyarrow._parquet.FileMetaData object at 0x000001E574929400>\n",
       "  created_by: parquet-cpp-arrow version 8.0.0\n",
       "  num_columns: 38\n",
       "  num_rows: 67663\n",
       "  num_row_groups: 1\n",
       "  format_version: 1.0\n",
       "  serialized_size: 7567"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Parquet validation\n",
    "parquet_output_path = results_dir.joinpath('routes.parquet')\n",
    "pqFile = pq.ParquetFile(parquet_output_path)\n",
    "pqFile.metadata"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1.d Protocol Buffers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.insert(0, os.path.abspath('routes_pb2'))\n",
    "\n",
    "import routes_pb2\n",
    "\n",
    "def _airport_to_proto_obj(airport):\n",
    "    obj = routes_pb2.Airport()\n",
    "    if airport is None:\n",
    "        return None\n",
    "    if airport.get('airport_id') is None:\n",
    "        return None\n",
    "\n",
    "    obj.airport_id = airport.get('airport_id')\n",
    "    if airport.get('name'):\n",
    "        obj.name = airport.get('name')\n",
    "    if airport.get('city'):\n",
    "        obj.city = airport.get('city')\n",
    "    if airport.get('iata'):\n",
    "        obj.iata = airport.get('iata')\n",
    "    if airport.get('icao'):\n",
    "        obj.icao = airport.get('icao')\n",
    "    if airport.get('altitude'):\n",
    "        obj.altitude = airport.get('altitude')\n",
    "    if airport.get('timezone'):\n",
    "        obj.timezone = airport.get('timezone')\n",
    "    if airport.get('dst'):\n",
    "        obj.dst = airport.get('dst')\n",
    "    if airport.get('tz_id'):\n",
    "        obj.tz_id = airport.get('tz_id')\n",
    "    if airport.get('type'):\n",
    "        obj.type = airport.get('type')\n",
    "    if airport.get('source'):\n",
    "        obj.source = airport.get('source')\n",
    "\n",
    "    obj.latitude = airport.get('latitude')\n",
    "    obj.longitude = airport.get('longitude')\n",
    "\n",
    "    return obj\n",
    "\n",
    "\n",
    "def _airline_to_proto_obj(airline):\n",
    "    obj = routes_pb2.Airline()\n",
    "    if not airline.get('name'):\n",
    "        return None\n",
    "    if not airline.get('airline_id'):\n",
    "        return None\n",
    "\n",
    "    obj.airline_id = airline.get('airline_id')\n",
    "    obj.name = airline.get('name')\n",
    "\n",
    "    if airline.get('alias'):\n",
    "        obj.alias = airline.get('alias')\n",
    "    if airline.get('iata'):\n",
    "        obj.iata = airline.get('iata')\n",
    "    if airline.get('icao'):\n",
    "        obj.icao = airline.get('icao')\n",
    "    if airline.get('callsign'):\n",
    "        obj.callsign = airline.get('callsign')\n",
    "    if airline.get('country'):\n",
    "        obj.country = airline.get('country')\n",
    "    if airline.get('active') is not None:\n",
    "        obj.active = airline.get('active')\n",
    "\n",
    "    return obj\n",
    "\n",
    "\n",
    "def create_protobuf_dataset(records):\n",
    "    routes = routes_pb2.Routes()\n",
    "    for record in records:\n",
    "        route = routes_pb2.Route()\n",
    "        airline = _airline_to_proto_obj(record.get('airline', {}))\n",
    "        if airline:\n",
    "            route.airline.CopyFrom(airline)\n",
    "        src_airport = _airport_to_proto_obj(record.get('src_airport', {}))\n",
    "        if src_airport:\n",
    "            route.src_airport.CopyFrom(src_airport)\n",
    "        dst_airport = _airport_to_proto_obj(record.get('dst_airport', {}))\n",
    "        if dst_airport:\n",
    "            route.dst_airport.CopyFrom(dst_airport)\n",
    "        if record.get('codeshare'):\n",
    "            route.codeshare = record.get('codeshare')\n",
    "        else:\n",
    "            route.codeshare = False\n",
    "        if record.get('stops') is not None:\n",
    "            route.stops = record.get('stops')\n",
    "        if record.get('equipment'):\n",
    "            route.equipment.extend(record.get('equipment'))\n",
    "\n",
    "        routes.route.append(route)\n",
    "\n",
    "    data_path = results_dir.joinpath('routes.pb')\n",
    "\n",
    "    with open(data_path, 'wb') as f:\n",
    "        f.write(routes.SerializeToString())\n",
    "        \n",
    "    compressed_path = results_dir.joinpath('routes.pb.snappy')\n",
    "    \n",
    "    with open(compressed_path, 'wb') as f:\n",
    "        f.write(snappy.compress(routes.SerializeToString()))\n",
    "        \n",
    "create_protobuf_dataset(records)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2.a Simple Geohash Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_hash_dirs(records):\n",
    "    geoindex_dir = results_dir.joinpath('geoindex')\n",
    "    geoindex_dir.mkdir(exist_ok=True, parents=True)\n",
    "    hashes = []\n",
    "    for record in records:\n",
    "        src_airport = record.get('src_airport', {})\n",
    "        if src_airport:\n",
    "            latitude = src_airport.get('latitude')\n",
    "            longitude = src_airport.get('longitude')\n",
    "            if latitude and longitude:\n",
    "                hashes.append(pygeohash.encode(latitude, longitude))\n",
    "    hashes.sort()\n",
    "    \n",
    "    three_letter = sorted(list(set([entry[:3] for entry in hashes])))\n",
    "    \n",
    "    hash_index = {value: [] for value in three_letter}\n",
    "    \n",
    "    for record in records:\n",
    "        geohash = record.get('geohash')\n",
    "        if geohash:\n",
    "            hash_index[geohash[:3]].append(record)\n",
    "            \n",
    "    for key, values in hash_index.items():\n",
    "        output_dir = geoindex_dir.joinpath(str(key[:1])).joinpath(str(key[:2]))\n",
    "        output_dir.mkdir(exist_ok=True, parents=True)\n",
    "        output_path = output_dir.joinpath('{}.jsonl.gz'.format(key))\n",
    "        with gzip.open(output_path, 'w') as f:\n",
    "            json_output = '\\n'.join([json.dumps(value) for value in values])\n",
    "            f.write(json_output.encode('utf-8'))\n",
    "            \n",
    "create_hash_dirs(records)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2.b Simple Search Feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def airport_search(latitude, longitude):\n",
    "    ## TODO: Create simple search to return nearest airport\n",
    "    a = pygeohash.encode(latitude, longitude)\n",
    "    dist = 0\n",
    "    name = ''\n",
    "    \n",
    "    for i, record in enumerate(records):\n",
    "        src_airport = record.get('src_airport', {})\n",
    "        if src_airport:\n",
    "            lat = src_airport.get('latitude')\n",
    "            long = src_airport.get('longitude')\n",
    "            airport_name = src_airport.get('name')\n",
    "            if lat and long:\n",
    "                a1 = pygeohash.encode(lat, long)\n",
    "                \n",
    "                dist_n = pygeohash.geohash_approximate_distance(a, a1)\n",
    "                if i==0:\n",
    "                    dist = dist_n\n",
    "                    name = airport_name\n",
    "                else:\n",
    "                    if dist > dist_n:\n",
    "                        dist = dist_n\n",
    "                        name = airport_name\n",
    "    print(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Validation searches, used google to pull examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eppley Airfield\n"
     ]
    }
   ],
   "source": [
    "airport_search(41.1499988, -95.91779)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vilnius International Airport\n"
     ]
    }
   ],
   "source": [
    "airport_search(54.8028, 23.9172)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "San Francisco International Airport\n"
     ]
    }
   ],
   "source": [
    "airport_search(37.61636, -122.391027)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chicago O'Hare International Airport\n"
     ]
    }
   ],
   "source": [
    "airport_search(41.9803, -87.9090)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
