{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"About This Course","text":"<p>Go to the setup page for instructions on how to setup your computer for this course. </p>"},{"location":"data/","title":"Data Sets","text":""},{"location":"data/#berkley-deepdrive","title":"Berkley DeepDrive","text":"<p>Data from Berkley DeepDrive is found in <code>data/external/bdd</code>.</p>"},{"location":"data/#license","title":"License","text":"<p>Copyright \u00a92018. The Regents of the University of California (Regents). All Rights Reserved. </p> <p>Permission to use, copy, modify, and distribute this software and its documentation for educational, research, and not-for-profit purposes, without fee and without a signed licensing agreement; and permission use, copy, modify and distribute this software for commercial purposes (such rights not subject to transfer) to BDD member and its affiliates, is hereby granted, provided that the above copyright notice, this paragraph and the following two paragraphs appear in all copies, modifications, and distributions. Contact The Office of Technology Licensing, UC Berkeley, 2150 Shattuck Avenue, Suite 510, Berkeley, CA 94720-1620, (510) 643-7201, otl@berkeley.edu, http://ipira.berkeley.edu/industry-info for commercial licensing opportunities.</p> <p>IN NO EVENT SHALL REGENTS BE LIABLE TO ANY PARTY FOR DIRECT, INDIRECT, SPECIAL, INCIDENTAL, OR CONSEQUENTIAL DAMAGES, INCLUDING LOST PROFITS, ARISING OUT OF THE USE OF THIS SOFTWARE AND ITS DOCUMENTATION, EVEN IF REGENTS HAS BEEN ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.</p> <p>REGENTS SPECIFICALLY DISCLAIMS ANY WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE. THE SOFTWARE AND ACCOMPANYING DOCUMENTATION, IF ANY, PROVIDED HEREUNDER IS PROVIDED \"AS IS\". REGENTS HAS NO OBLIGATION TO PROVIDE MAINTENANCE, SUPPORT, UPDATES, ENHANCEMENTS, OR MODIFICATIONS.</p>"},{"location":"data/#enron-emails","title":"Enron Emails","text":"<p>The folder <code>data/external/enron</code> contains a partial copy of the Enron Email dataset.</p>"},{"location":"data/#large-movie-review","title":"Large Movie Review","text":"<p>Large Movie Review Dataset contains 25,000 movie reviews from IMDB and can be found in <code>data/external/imdb</code>.</p>"},{"location":"data/#openflights","title":"OpenFlights","text":""},{"location":"data/#license_1","title":"License","text":"<p>The OpenFlights Airport, Airline, Plane and Route Databases are made available under the Open Database License. Any rights in individual contents of the database are licensed under the Database Contents License. In short, these mean that you are welcome to use the data as you wish, if and only if you both acknowledge the source and and license any derived works made available to the public with a free license as well.</p> <p>See OpenFlights Data for more detailed documentation. </p>"},{"location":"data/#data","title":"Data","text":"<p>OpenFlights data is found in <code>data/external/openflights</code>.  Data copied from the OpenFlights Github Repo.</p> <p>Note</p> <p>The special value \\N is used for \\\"NULL\\\" to indicate that no value is available</p>"},{"location":"data/#airports","title":"Airports","text":"Field Type Nullable? Notes airport_id int No Primary Key name text Yes city text Yes country text Yes iata varchar(3) Yes icao varchar(4) Yes latitude double No longitude double No altitude int Yes timezone float Yes dst char(1) Yes tz_id text Yes type text Yes source text Yes"},{"location":"data/#airlines","title":"Airlines","text":"Field Type Nullable? Notes airline_id int No Primary Key name text No alias text Yes iata varchar(2) Yes icao varchar(3) Yes callsign text Yes country text Yes active boolean No Default value FALSE"},{"location":"data/#routes","title":"Routes","text":"Field Type Nullable? Notes airline varchar(3) Yes airline_id int Yes src_airport varchar(4) Yes src_airport_id int Yes dst_airport varchar(4) Yes dst_airport_id int Yes codeshare boolean Yes Default value FALSE stops int Yes equipment text Yes <p><code>airline_id</code>, <code>src_airport_id</code>, and <code>dst_airport_id</code> form a unique key</p>"},{"location":"data/#planes","title":"Planes","text":"Field Type Nullable? Notes name text Yes iata varchar(3) Yes icao varchar(4) Yes"},{"location":"data/#countries","title":"Countries","text":"Field Type Nullable? Notes name text Yes iso_code varchar(2) Yes dafif_code varchar(2) Yes <p>Note</p> <p>Some entries have DAFIF codes, but not ISO codes. These are primarily uninhabited islands without airports, and can be ignored for most purposes.</p>"},{"location":"data/#tidynomicon","title":"Tidynomicon","text":"<p>Data copied from the Tidynomicon Github repository. </p>"},{"location":"data/#license_2","title":"License","text":"<p>This is a human-readable summary of (and not a substitute for) the license. Please see https://creativecommons.org/licenses/by/4.0/legalcode for the full legal text.</p> <p>This work is licensed under the Creative Commons Attribution 4.0 International license (CC-BY-4.0).</p> <p>You are free to:</p> <ul> <li> <p>Share---copy and redistribute the material in any medium or   format</p> </li> <li> <p>Remix---remix, transform, and build upon the material for any   purpose, even commercially.</p> </li> </ul> <p>The licensor cannot revoke these freedoms as long as you follow the license terms.</p> <p>Under the following terms:</p> <ul> <li> <p>Attribution---You must give appropriate credit, provide a link   to the license, and indicate if changes were made. You may do so in   any reasonable manner, but not in any way that suggests the licensor   endorses you or your use.</p> </li> <li> <p>No additional restrictions---You may not apply legal terms or   technological measures that legally restrict others from doing   anything the license permits.</p> </li> </ul> <p>Notices:</p> <p>You do not have to comply with the license for elements of the material in the public domain or where your use is permitted by an applicable exception or limitation.</p> <p>No warranties are given. The license may not give you all of the permissions necessary for your intended use. For example, other rights such as publicity, privacy, or moral rights may limit how you use the material.</p>"},{"location":"faq/neural-networks/","title":"Neural Networks","text":""},{"location":"faq/neural-networks/#number-of-training-epochs","title":"Number of Training Epochs","text":"<p>Question</p> <p>What is the optimal number of training epochs to use when training a model?</p> <p>A training epoch refers to one sweep through the entire training set.</p> <p>From Neural Networks Part 3: Learning and Evaluation</p> <p> </p> <p>Two possible cases are shown in the diagram on the left. The blue validation error curve shows very small validation accuracy compared to the training accuracy, indicating strong overfitting (note, it's possible for the validation accuracy to even start to go down after some point). When you see this in practice you probably want to increase regularization (stronger L2 weight penalty, more dropout, etc.) or collect more data. The other possible case is when the validation accuracy tracks the training accuracy fairly well. This case indicates that your model capacity is not high enough: make the model larger by increasing the number of parameters. </p>"},{"location":"faq/neural-networks/#number-of-hidden-units-and-layers","title":"Number of Hidden Units and Layers","text":"<p>Question</p> <p>What is the optimal number of hidden units and layers to use when training a model?</p> <p>From Elements of Statistical learning</p> <p>Generally speaking it is better to have too many hidden units than too few. With too few hidden units, the model might not have enough flexibility to capture the nonlinearities in the data; with too many hidden units, the extra weights can be shrunk toward zero if appropriate regularization is used. Typically the number of hidden units is somewhere in the range of 5 to 100, with the number increasing with the number of inputs and number of training cases. It is most common to put down a reasonably large number of units and train them with regularization. Some researchers use cross-validation to estimate the optimal number, but this seems unnecessary if cross-validation is used to estimate the regularization parameter. Choice of the number of hidden layers is guided by background knowledge and experimentation. Each layer extracts features of the input for regression or classification. Use of multiple hidden layers allows construction of hierarchical features at different levels of resolution.</p>"},{"location":"faq/neural-networks/#convolutional-networks","title":"Convolutional Networks","text":"<p>See layer sizing patterns for information on the number of layers for convolutional networks.</p> <p>From Neural Networks Part 1: Setting up the Architecture</p> <p>To give you some context, modern Convolutional Networks contain on orders of 100 million parameters and are usually made up of approximately 10-20 layers (hence deep learning). However, as we will see the number of effective connections is significantly greater due to parameter sharing.</p> <p>From Convolutional Neural Networks</p> <p>In practice: use whatever works best on ImageNet. If you\u2019re feeling a bit of a fatigue in thinking about the architectural decisions, you\u2019ll be pleased to know that in 90% or more of applications you should not have to worry about these. I like to summarize this point as \u201cdon\u2019t be a hero\u201d: Instead of rolling your own architecture for a problem, you should look at whatever architecture currently works best on ImageNet, download a pretrained model and finetune it on your data. You should rarely ever have to train a ConvNet from scratch or design one from scratch.</p> <p>From Deep Residual Learning for Image Recognition</p> <p>Exploring Over 1000 layers. We explore an aggressively deep model of over 1000 layers. We set n = 200 that leads to a 1202-layer network, which is trained as described above. Our method shows no optimization difficulty, and this 103-layer network is able to achieve training error &lt;0.1% (Fig. 6, right). Its test error is still fairly good (7.93%, Table 6).</p> <p>But there are still open problems on such aggressively deep models. The testing result of this 1202-layer network is worse than that of our 110-layer network, although both have similar training error. We argue that this is because of overfitting. The 1202-layer network may be unnecessarily large (19.4M) for this small dataset. Strong regularization such as maxout [10] or dropout [14] is applied to obtain the best results ([10, 25, 24, 35]) on this dataset. In this paper, we use no maxout/dropout and just simply impose regularization via deep and thin architectures by design, without distracting from the focus on the difficulties of optimization. But combining with stronger regularization may improve results, which we will study in the future.</p> <p>From Neural Networks Part 1: Setting up the Architecture</p> <p>The takeaway is that you should not be using smaller networks because you are afraid of overfitting. Instead, you should use as big of a neural network as your computational budget allows, and use other regularization techniques to control overfitting.</p>"},{"location":"faq/neural-networks/#densely-connected-networks","title":"Densely Connected Networks","text":"<p>From Neural Networks Part 1: Setting up the Architecture</p> <p>As an aside, in practice it is often the case that 3-layer neural networks will outperform 2-layer nets, but going even deeper (4,5,6-layer) rarely helps much more. This is in stark contrast to Convolutional Networks, where depth has been found to be an extremely important component for a good recognition system (e.g. on order of 10 learnable layers). One argument for this observation is that images contain hierarchical structure (e.g. faces are made up of eyes, which are made up of edges, etc.), so several layers of processing make intuitive sense for this data domain.</p>"},{"location":"faq/neural-networks/#large-number-of-categories","title":"Large Number of Categories","text":"<p>Question</p> <p>What is the best practices when trying to train models to classify large numbers of categories?</p> <p>From Elements of Statistical learning</p> <p>With NN observations, pp predictors,MM hidden units and LL training epochs, a neural network fit typically requires O(NpML)O(NpML) operations. </p> <p>From Neural Networks Part 2: Setting up the Data and the Loss</p> <p>Problem: Large number of classes. When the set of labels is very large (e.g. words in English dictionary, or ImageNet which contains 22,000 categories), computing the full softmax probabilities becomes expensive. For certain applications, approximate versions are popular. For instance, it may be helpful to use Hierarchical Softmax in natural language processing tasks (see one explanation here (pdf)). The hierarchical softmax decomposes words as labels in a tree. Each label is then represented as a path along the tree, and a Softmax classifier is trained at every node of the tree to disambiguate between the left and right branch. The structure of the tree strongly impacts the performance and is generally problem-dependent.</p>"},{"location":"faq/neural-networks/#dropout-and-regularization","title":"Dropout and Regularization","text":"<p>Question</p> <p>What is the best practice for using dropout and regularization?</p> <p>From Neural Networks Part 1: Setting up the Architecture</p> <p>. . . there are many other preferred ways to prevent overfitting in Neural Networks that we will discuss later (such as L2 regularization, dropout, input noise). In practice, it is always better to use these methods to control overfitting instead of the number of neurons.</p> <p>To reiterate, the regularization strength is the preferred way to control the overfitting of a neural network.</p> <p>From Neural Networks Part 2: Setting up the Data and the Loss</p> <p>In practice: It is most common to use a single, global L2 regularization strength that is cross-validated. It is also common to combine this with dropout applied after all layers. The value of p=0.5 is a reasonable default, but this can be tuned on validation data.</p> <p>From Dropout: A Simple Way to Prevent Neural Networks from Overfitting</p> <p>It is to be expected that dropping units will reduce the capacity of a neural network. If n is the number of hidden units in any layer and p is the probability of retaining a unit, then instead of n hidden units, only pn units will be present after dropout, in expectation. Moreover, this set of pn units will be different each time and the units are not allowed to build co-adaptations freely. Therefore, if an n-sized layer is optimal for a standard neural net on any given task, a good dropout net should have at least n=p units. We found this to be a useful heuristic for setting the number of hidden units in both convolutional and fully connected networks.</p>"},{"location":"faq/neural-networks/#hyperparameter-selection-and-optimization","title":"Hyperparameter Selection and Optimization","text":"<p>Question</p> <p>How do you select and optimize Hyperparameters? </p> <p>See hyperparameter optimization and Random Search for Hyper-Parameter Optimization for more information.</p> <p>Here is a short excerpt: </p> <p>Search for good hyperparameters with random search (not grid search). Stage your search from coarse (wide hyperparameter ranges, training only for 1-5 epochs), to fine (narrower rangers, training for many more epochs)</p> <p>You can also look into tools like hyperopt</p>"},{"location":"lessons/","title":"Overview","text":""},{"location":"lessons/10-week/","title":"Index","text":"<p>Documentation in Progress</p> <p>Check back soon for more updates.    </p>"},{"location":"lessons/10-week/week01/","title":"Week 1","text":"<p>To paraphrase the late Douglas Adams:</p> <p>Big data is big. You just won't believe how vastly, hugely, mind-bogglingly big it is.  I mean, you may think your collection of movies, pictures, and music is big, but that's just peanuts to big data. </p> <p>Big Data is big in two distinct ways. First, as the name suggests, Big Data is about how to deal with large amounts of data. Tech giants like Google and Facebook store exabytes of data. While multiple exabytes of data is an impressive amount of data, it is nowhere near the theoretical limits. </p> <p>Second, Big Data is a wide area of study that spans a wide range of technologies and concepts. Because of the size and rapid rate of change of the subject, we will only be able to cover a small fraction of Big Data topics in this course. </p> <p>In this course, we will focus on two main areas: the design of data-driven systems and deep learning algorithms.  The first area, the design of data-driven systems, takes a high-level look into how the different components of Big Data systems fit together.  The second area, deep learning, focuses on a specific approach to extracting information from large, usually unstructured datasets. </p>"},{"location":"lessons/10-week/week01/#objectives","title":"Objectives","text":"<p>After completing this week, you should be able to:</p> <ul> <li>Setup a development environment PySpark, Keras, and TensorFlow  and run a simple proof of concept</li> <li>Explain how reliability, scalability, and maintainability impacts data-driven systems</li> <li>Summarize how artificial intelligence, machine learning, and deep learning relate to one another</li> <li>Determine what problems deep learning and big data help solve</li> </ul>"},{"location":"lessons/10-week/week01/#readings","title":"Readings","text":"<ul> <li>Read chapter 1 in Designing Data-Intensive Applications</li> <li>Read chapter 1 in Deep Learning with Python</li> <li>Visit DSC 650 website and follow the getting started instructions to setup your development environment</li> <li>Watch CPU vs GPU What's the Difference?</li> <li>Watch How Much Information is in the Universe? | Space Time</li> </ul>"},{"location":"lessons/10-week/week01/#weekly-resources","title":"Weekly Resources","text":"<ul> <li>Backblaze Hard Drive Stats</li> <li>DSC 650 Website</li> <li>DSC 650 Github Repository</li> <li>CPU vs GPU What's the Difference?What's the Difference?</li> <li>How Much Information is in the Universe? | Space Time</li> </ul>"},{"location":"lessons/10-week/week01/#assignment-1","title":"Assignment 1","text":""},{"location":"lessons/10-week/week01/#assignment-11","title":"Assignment 1.1","text":"<p>Visit [DSC 650 website][dsc650] and follow the instructions for getting started.  To demonstrate your environment is in working order, run the examples in the <code>examples</code> folder, and copy the output to the <code>dsc650/assignment01/logs</code> folder.  </p>"},{"location":"lessons/10-week/week01/#a-run-keras-mnist-mlp-example","title":"a.  Run Keras MNIST MLP Example","text":"<p>If you are using Bash the following commands will write both stdout and stderr to a file. </p> <pre><code>$ python examples/mnist_mlp.py &gt; logs/keras-mnist.log 2&gt;&amp;1\n</code></pre> <p>If you are not using Bash, you can manually copy and paste the output into the log file using a text editor. </p>"},{"location":"lessons/10-week/week01/#b-run-pyspark-example","title":"b. Run PySpark Example","text":"<p>If you are using Bash the following commands will write both stdout and stderr to a file. </p> <pre><code>$ python examples/pi.py &gt; logs/spark-pi.log 2&gt;&amp;1\n</code></pre> <p>If you are not using Bash, you can manually copy and paste the output into the log file using a text editor. </p>"},{"location":"lessons/10-week/week01/#assignment-12","title":"Assignment 1.2","text":"<p>For the rest of the assignment, you will answer questions about scaling and maintaining data-driven systems. A Markdown template for this part of the assignment can be found at <code>dsc650/assignments/assignment01/Assignment 01.md</code>.</p>"},{"location":"lessons/10-week/week01/#a-data-sizes","title":"a. Data Sizes","text":"<p>Provide estimates for the size of various data items.  Please explain how you arrived at the estimates for the size of each item by citing references or providing calculations. </p> Data Item Size per Item 128 character message. ? Bytes 1024x768 PNG image ? MB 1024x768 RAW image ? MB HD (1080p) HEVC Video (15 minutes) ? MB HD (1080p) Uncompressed Video (15 minutes) ? MB 4K UHD HEVC Video (15 minutes) ? MB 4k UHD Uncompressed Video (15 minutes) ? MB Human Genome (Uncompressed) ? GB <ul> <li>Assume all videos are 30 frames per second</li> <li>HEVC stands for High Efficiency Video Coding</li> <li>See the Wikipedia article on display resolution for information on HD (1080p) and 4K UHD resolutions. </li> </ul>"},{"location":"lessons/10-week/week01/#b-scaling","title":"b. Scaling","text":"<p>Using the estimates for data sizes in the previous part, determine how much storage space you would need for the following items.  </p> Size # HD Daily Twitter Tweets (Uncompressed) ?? Daily Twitter Tweets (Snappy Compressed) ?? Daily Instagram Photos ?? Daily YouTube Videos ?? Yearly Twitter Tweets (Uncompressed) ?? Yearly Twitter Tweets (Snappy Compressed) ?? Yearly Instagram Photos ?? Yearly YouTube Videos ?? <ul> <li>For estimating the number of hard drives, assume you are using 10 TB and you are storing the data using the Hadoop Distributed File System (HDFS).  By default, HDFS stores three copies of each piece of data, so you will need to triple the amount storage required. </li> <li>Twitter statistics estimates 500 million tweets are sent each day. For simplicity, assume each tweet is 128 characters. </li> <li>See the Snappy Github repository for estimates of Snappy's performance. </li> <li>Instagram statistics estimates over 100 million videos and photos are uploaded to Instagram every day.   Assume that 75% of those items are 1024x768 PNG photos.  </li> <li>YouTube statistics estimates 500 hours of video is uploaded to YouTube every minute.  For simplicity, assume all videos are HD quality encoded using HEVC at 30 frames per second. </li> </ul>"},{"location":"lessons/10-week/week01/#c-reliability","title":"c. Reliability","text":"<p>Using the yearly estimates from the previous part, estimate the number of hard drive failures per year using data from Backblaze's hard drive statistics.</p> # HD # Failures Twitter Tweets (Uncompressed) ?? Twitter Tweets (Snappy Compressed) ?? Instagram Photos ?? YouTube Videos ??"},{"location":"lessons/10-week/week01/#d-latency","title":"d. Latency","text":"<p>Provide estimates of the one way latency for each of the following items.  Please explain how you arrived at the estimates for each item by citing references or providing calculations. </p> One Way Latency Los Angeles to Amsterdam ? ms Low Earth Orbit Satellite ? ms Geostationary Satellite ? ms Earth to the Moon ? ms Earth to Mars ? minutes"},{"location":"lessons/10-week/week01/#submission-instructions","title":"Submission Instructions","text":"<p>For this assignment, you will submit a zip archive containing the contents of the <code>dsc650/assignments/assignment01/</code> directory. Use the naming convention of <code>assignment01_LastnameFirstname.zip</code> for the zip archive. You can create this archive in Bash (or a similar Unix shell) using the following commands. </p> <pre><code>cd dsc650/assignments\nzip -r assignment01_DoeJane.zip assignment01\n</code></pre> <p>Likewise, you can create a zip archive using Windows PowerShell with the following command. </p> <pre><code>Compress-Archive -Path assignment01 -DestinationPath 'assignment01_DoeJane.zip\n</code></pre> <p>When decompressed, the output should have the following directory structure. </p> <pre><code>\u251c\u2500\u2500 assignment01\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 Assignment\\ 01.md\n\u2502\u00a0\u00a0 \u2514\u2500\u2500 logs\n\u2502\u00a0\u00a0     \u251c\u2500\u2500 keras-mnist.log\n\u2502\u00a0\u00a0     \u2514\u2500\u2500 spark-pi.log\n</code></pre>"},{"location":"lessons/10-week/week01/#discussion","title":"Discussion","text":"<p>For the first discussion, write a 250 to 750-word discussion board post about a Big Data and/or deep learning use case.  Try to focus on a use case relevant to your professional or personal interests.  Use the DSC 650 Slack channel for discussion and replies.  For grading purposes, copy and paste your initial post  and at least two replies to the Blackboard discussion board. </p>"},{"location":"lessons/10-week/week02/","title":"Week 2","text":"<p>In the previous lesson, we learned about the fundamentals of deep learning and data-driven systems. Now that we have a high-level overview, we will dive into examples of how to model, query, and process data using different paradigms. </p>"},{"location":"lessons/10-week/week02/#objectives","title":"Objectives","text":"<p>After completing this week, you should be able to:</p> <ul> <li>Query and process data using multiple paradigms including graph processing, map-reduce, and SQL</li> <li>Compare and contrast different data models including identifying prime use cases for different data models</li> <li>Demonstrate how to represent data as tensors and apply tensor mathematical operations</li> </ul>"},{"location":"lessons/10-week/week02/#readings","title":"Readings","text":"<ul> <li>Read chapters 2 and 3 in Designing Data-Intensive Applications</li> <li>Read chapter 2 in Deep Learning with Python</li> </ul>"},{"location":"lessons/10-week/week02/#weekly-resources","title":"Weekly Resources","text":"<ul> <li>TinyDB</li> <li>OrientDB Getting Started</li> <li>OrientDB Download</li> <li>Keras</li> <li>Multi-Dimensional Data as used in Tensors</li> <li>SQL Tutorial</li> <li>TensorFlow Quickstart</li> </ul>"},{"location":"lessons/10-week/week02/#assignment-2","title":"Assignment 2","text":"<p>The <code>dsc650/assignments/assignment02</code> folder contains skeleton code for this assignment. Provide the code to implement the functions in the <code>run_assignment.py</code> file. For this assignment, we will be working with the CSV data found in the <code>data/external/tidynomicon</code> folder.  Specifically, we will be using with the <code>measurements.csv</code>, <code>person.csv</code>, <code>site.csv</code>, and <code>visited.csv</code> files. </p>"},{"location":"lessons/10-week/week02/#assignment-21","title":"Assignment 2.1","text":"<p>Complete the code in <code>kvdb.py</code> to implement a basic key-value database that saves its state to a pickle file.  Use that code to create databases that store each of CSV files by key. The pickle files should be stored in the <code>dsc650/assignments/assignment02/results/kvdb/</code> folder. </p> Input File Output File Key <code>measurements.csv</code> <code>measurements.pickle</code> Composite key <code>person.csv</code> <code>people.pickle</code> <code>person_id</code> <code>site.csv</code> <code>sites.pickle</code> <code>site_id</code> <code>visited.csv</code> <code>visits.pickle</code> Composite key <p>The <code>measurements.csv</code> and <code>visited.csv</code> have composite keys that use multiple columns. For <code>measurements.csv</code> those fields are <code>visit_id</code>, <code>person_id</code>, and <code>quantity</code>.  For <code>visited.csv</code> those fields are <code>visit_id</code> and <code>site_id</code>.  The following is an example of code that sets and gets the value using a composite key. </p> <pre><code>kvdb_path = 'visits.pickle'\nkvdb = KVDB(kvdb_path)\nkey = (619, 'DR-1')\nvalue = dict(\n    visit_id=619,\n    site_id='DR-1',\n    visit_date='1927-02-08'\n)\nkvdb.set_value(key, value)\nretrieved_value = kvdb.get_value(key)\n# Retrieved should be the same as value\n</code></pre>"},{"location":"lessons/10-week/week02/#assignment-22","title":"Assignment 2.2","text":"<p>Now we will create a simple document database using the <code>tinydb</code> library. TinyDB stores its data as a JSON file. For this assignment, you will store the TinyDB database in <code>dsc650/assignments/assignment02/results/patient-info.json</code>.  You will store a document for each person in the database which should look like this. </p> <pre><code>{\n\"person_id\": \"dyer\",\n\"personal_name\": \"William\",\n\"family_name\": \"Dyer\",\n\"visits\": [\n{\n\"visit_id\": 619,\n\"site_id\": \"DR-1\",\n\"visit_date\": \"1927-02-08\",\n\"site\": {\n\"site_id\": \"DR-1\",\n\"latitude\": -49.85,\n\"longitude\": -128.57\n},\n\"measurements\": [\n{\n\"visit_id\": 619,\n\"person_id\": \"dyer\",\n\"quantity\": \"rad\",\n\"reading\": 9.82\n},\n{\n\"visit_id\": 619,\n\"person_id\": \"dyer\",\n\"quantity\": \"sal\",\n\"reading\": 0.13\n}\n]\n},\n{\n\"visit_id\": 622,\n\"site_id\": \"DR-1\",\n\"visit_date\": \"1927-02-10\",\n\"site\": {\n\"site_id\": \"DR-1\",\n\"latitude\": -49.85,\n\"longitude\": -128.57\n},\n\"measurements\": [\n{\n\"visit_id\": 622,\n\"person_id\": \"dyer\",\n\"quantity\": \"rad\",\n\"reading\": 7.8\n},\n{\n\"visit_id\": 622,\n\"person_id\": \"dyer\",\n\"quantity\": \"sal\",\n\"reading\": 0.09\n}\n]\n}\n]\n}\n</code></pre> <p>The <code>dsc650/assignments/assignment02/documentdb.py</code> file contains code that should assist you in this task. </p>"},{"location":"lessons/10-week/week02/#assignment-23","title":"Assignment 2.3","text":"<p>Complete the code in <code>dsc650/assignments/assignment02/objectdb</code>  to implement an object database using ZODB.  You will store the database in <code>dsc650/assignments/assignment02/results/patient-info.fs</code> </p>"},{"location":"lessons/10-week/week02/#assignment-24","title":"Assignment 2.4","text":"<p>In this part, you will create a SQLite database which you will store in <code>dsc650/assignments/assignment02/results/patient-info.db</code>.  The <code>dsc650/assignments/assignment02/rdbms.py</code> file should contain code to assist you in the creation of this database. </p>"},{"location":"lessons/10-week/week02/#assignment-25","title":"Assignment 2.5","text":"<p>Go to the Wikidata Query Service website and perform the following SPARQL query. </p> <pre><code>#Recent Events\nSELECT ?event ?eventLabel ?date\nWHERE\n{\n    # find events\n    ?event wdt:P31/wdt:P279* wd:Q1190554.\n    # with a point in time or start date\n    OPTIONAL { ?event wdt:P585 ?date. }\n    OPTIONAL { ?event wdt:P580 ?date. }\n    # but at least one of those\n    FILTER(BOUND(?date) &amp;&amp; DATATYPE(?date) = xsd:dateTime).\n    # not in the future, and not more than 31 days ago\n    BIND(NOW() - ?date AS ?distance).\n    FILTER(0 &lt;= ?distance &amp;&amp; ?distance &lt; 31).\n    # and get a label as well\n    OPTIONAL {\n        ?event rdfs:label ?eventLabel.\n        FILTER(LANG(?eventLabel) = \"en\").\n    }\n}\n# limit to 10 results so we don't timeout\nLIMIT 10\n</code></pre> <p>Modify the query so that the column order is <code>date</code>, <code>event</code>, and <code>eventLabel</code> instead of <code>event</code>, <code>eventLabel</code>, and <code>date. Download the results as a JSON file and copy the results to</code>dsc650/assignments/assignment02/results/wikidata-query.json`.  </p>"},{"location":"lessons/10-week/week02/#submission-instructions","title":"Submission Instructions","text":"<p>For this assignment, you will submit a zip archive containing the contents of the <code>dsc650/assignments/assignment02/</code> directory. Use the naming convention of <code>assignment02_LastnameFirstname.zip</code> for the zip archive. You can create this archive in Bash (or a similar Unix shell) using the following commands. </p> <pre><code>cd dsc650/assignments\nzip -r assignment02_DoeJane.zip assignment02\n</code></pre> <p>Likewise, you can create a zip archive using Windows PowerShell with the following command. </p> <pre><code>Compress-Archive -Path assignment02 -DestinationPath 'assignment02_DoeJane.zip\n</code></pre> <p>When decompressed, the output should have the following directory structure. </p> <pre><code>\u251c\u2500\u2500 assignment02\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 __init__.py\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 documentdb.py\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 kvdb.py\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 objectdb.py\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 rdbms.py\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 results\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 kvdb\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 measurements.pickle\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 people.pickle\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 sites.pickle\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2514\u2500\u2500 visits.pickle\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 patient-info.db\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 patient-info.fs\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 patient-info.json\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2514\u2500\u2500 wikidata-query.json\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 run_assignment.py\n\u2502\u00a0\u00a0 \u2514\u2500\u2500 util.py\n</code></pre>"},{"location":"lessons/10-week/week02/#discussion","title":"Discussion","text":"<p>For this discussion, write a 250 to 750-word discussion board post about use cases from different data models.  As an example, how could you use a graph database in one of your professional or personal projects? Try to focus on a use case relevant to your professional or personal interests.  Use the DSC 650 Slack channel for discussion and replies.  For grading purposes, copy and paste your initial post and at least two replies to the Blackboard discussion board. </p>"},{"location":"lessons/10-week/week03/","title":"Week 3","text":"<p>In this lesson, we learn about the data structures, encodings, and schemas used to store data and data indexes.</p>"},{"location":"lessons/10-week/week03/#objectives","title":"Objectives","text":"<p>After completing this week, you should be able to:</p> <ul> <li>Compare indexing algorithms including hash indexes and B-Trees</li> <li>Determine use cases for different methods of organizing data including column-oriented storage and snowflake schemas</li> <li>Make use of different encoding formats including Avro, Thrift, JSON, XML, and Protocol Buffers</li> <li>Describe the different methods of using data schemas including schema-on-read, schema-on-write, and different methods of schema evolution</li> </ul>"},{"location":"lessons/10-week/week03/#readings","title":"Readings","text":"<ul> <li>Read chapters 3 and 4 in Designing Data-Intensive Applications</li> </ul>"},{"location":"lessons/10-week/week03/#weekly-resources","title":"Weekly Resources","text":"<ul> <li>Apache Arrow</li> <li>Apache Parquet</li> <li>Apache Thrift</li> <li>Apache Avro</li> <li>JSON Schema</li> <li>Protocol Buffers</li> </ul>"},{"location":"lessons/10-week/week03/#assignment-3","title":"Assignment 3","text":"<p>For this assignment, you will be working with data from OpenFlights.  This data was originally obtained from the OpenFlights Github repository and a copy of the original data is found in <code>data/external/openflights/</code>. For this assignment, you will use a dataset derived from that original data. You can find this data in <code>data/processed/openflights/routes.jsonl.gz</code>.   The data is compressed with gzip and encoded in the JSON Lines format.  Each line represents a single airline route. </p> <p>The <code>dsc650/assignments/assignment03</code> directory contains placeholder code and data outputs for this assignment. </p>"},{"location":"lessons/10-week/week03/#assignment-31","title":"Assignment 3.1","text":"<p>In the first part of the assignment, you will be creating schemas for the route data and encoding the <code>routes.jsonl.gz</code> using Protocol Buffers, Avro, and Parquet. </p>"},{"location":"lessons/10-week/week03/#a-json-schema","title":"a. JSON Schema","text":"<p>Create a JSON Schema in the <code>schemas/routes-schema.json</code> file to describe a route and validate the data in <code>routes.jsonl.gz</code> using the jsonschema library. </p>"},{"location":"lessons/10-week/week03/#b-avro","title":"b. Avro","text":"<p>Create an Avro schema in the <code>schemas/routes.avsc</code> file to describe a route and validate the data in <code>routes.jsonl.gz</code>. Use Avro's Python library or fastavro library to create <code>results/routes.avro</code> with the schema you created.  Do not use any compression on the output at this stage. </p>"},{"location":"lessons/10-week/week03/#c-parquet","title":"c. Parquet","text":"<p>Create a Parquet dataset in <code>results/routes.parquet</code> using Apache Arrow and Pandas. Do not use any compression on the output at this stage. </p>"},{"location":"lessons/10-week/week03/#d-protocol-buffers","title":"d. Protocol Buffers","text":"<p>Define a Protocol Buffers message format in <code>schemas/routes.proto</code>. Using the Protocol Buffers Python tutorial as a guide, compile the <code>routes.proto</code> into Python classes.  Output the generated code into <code>dsc650/assignment/assignment03/routes_pb2.py</code>. Use this generated code to create <code>results/routes.pb</code> using Protocol Buffers. Do not use any compression on the output at this stage. </p>"},{"location":"lessons/10-week/week03/#e-output-sizes","title":"e. Output Sizes","text":"<p>Compare the output sizes of the different formats.  Populate the results in <code>results/comparison.csv</code>.</p>"},{"location":"lessons/10-week/week03/#assignment-32","title":"Assignment 3.2","text":"<p>This part of the assignment involves developing a rudimentary database index for our routes dataset.  Filesystems, databases, and NoSQL datastores use various indexing mechanisms to speed queries. </p> <p>The implementation of advanced data structures such as B-Trees, R-Trees, GiST, SSTables, and LSM trees is beyond the scope of this course. Instead, you will implement a rudimentary geospatial index using geohashes and pygeohash. </p> <p>Without going into too much detail, a geohash converts geospatial coordinates (i.e. latitude and longitude) into single, usually, base64 or base32 encoded integer.  Below is an example of the geohashed value for Bellevue University. </p> <pre><code>import pygeohash\npygeohash.encode(41.1499988, -95.91779)\n'9z7f174u17zb'\n</code></pre> <p>Geohashes have the useful property that when they are sorted, entries that are near one another in the sorted list are usually close to one another in space. The following image shows how this gird looks. </p> <p></p> <p>The following table gives cell width and height values for each level of precision of the geohash.</p> Geohash Coordinates Cell Width1 Cell Height 9 22.0, -112.0 \u2264 5,000km 5,000km 9z 42.0, -96.0 \u2264 1,250km 625km 9z7 41.0, -96.0 \u2264 156km 156km 9z7f 41.0, -96.0 \u2264 39.1km 19.5km 9z7f1 41.2, -95.9 \u2264 4.89km 4.89km 9z7f174u 41.15, -95.918 \u2264 38.2m 19.1m 9z7f174u17zb 41.149999, -95.91779 \u2264 4.77m 4.77m <p>As you can see, it only takes about four levels characters to get to a 40 km by 20 km area.  Another level gives 5 km by 5 km. In most cases, going past 12 units of precision is pointless as very few applications require that degree of accuracy. </p> <p></p>"},{"location":"lessons/10-week/week03/#a-create-a-simple-geohash-index","title":"a. Create a Simple Geohash Index","text":"<p>Using <code>pygeohash</code> create a simple index for the <code>routes.jsonl.gz</code> data using the source airport latitude and longitude. Output the index and values to the <code>results/geoindex</code> directory. The output looks like the following directory structure.  </p> <pre><code>geoindex\n\u251c\u2500\u2500 2\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 2e\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 2eg.jsonl.gz\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 2ev.jsonl.gz\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2514\u2500\u2500 2ey.jsonl.gz\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 2h\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 2h5.jsonl.gz\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 2hb.jsonl.gz\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2514\u2500\u2500 2hx.jsonl.gz\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 2j\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 2j0.jsonl.gz\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 2j3.jsonl.gz\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 2jd.jsonl.gz\n. \n. \n.\n\u2502\u00a0\u00a0 \u2514\u2500\u2500 yu\n\u2502\u00a0\u00a0     \u2514\u2500\u2500 yue.jsonl.gz\n\u2514\u2500\u2500 z\n    \u251c\u2500\u2500 z0\n    \u2502\u00a0\u00a0 \u251c\u2500\u2500 z08.jsonl.gz\n    \u2502\u00a0\u00a0 \u251c\u2500\u2500 z0h.jsonl.gz\n    \u2502\u00a0\u00a0 \u2514\u2500\u2500 z0m.jsonl.gz\n    \u251c\u2500\u2500 z6\n    \u2502\u00a0\u00a0 \u2514\u2500\u2500 z6e.jsonl.gz\n    \u251c\u2500\u2500 z9\n    \u2502\u00a0\u00a0 \u2514\u2500\u2500 z92.jsonl.gz\n    \u251c\u2500\u2500 zg\n    \u2502\u00a0\u00a0 \u2514\u2500\u2500 zgw.jsonl.gz\n    \u251c\u2500\u2500 zk\n    \u2502\u00a0\u00a0 \u2514\u2500\u2500 zk9.jsonl.gz\n    \u251c\u2500\u2500 zs\n    \u2502\u00a0\u00a0 \u2514\u2500\u2500 zs4.jsonl.gz\n    \u2514\u2500\u2500 zu\n        \u2514\u2500\u2500 zu3.jsonl.gz\n</code></pre>"},{"location":"lessons/10-week/week03/#b-implement-a-simple-search-feature","title":"b. Implement a Simple Search Feature","text":"<p>Implement a simple geospatial search feature that finds airports within a specified distance of an input latitude and longitude. You can use the <code>geohash_approximate_distance</code> function in <code>pygeohash</code> to compute distances between geohash values.  It returns distances in meters, but your search function should use kilometers as input. </p> <pre><code>import pygeohash\npygeohash.geohash_approximate_distance('bcd3u', 'bc83n')\n# &gt;&gt;&gt; 625441\n</code></pre>"},{"location":"lessons/10-week/week03/#c-evolve-the-protocol-buffers-and-avro-schemas","title":"c.  Evolve the Protocol Buffers and Avro Schemas","text":"<p>Protocol Buffers, Avro, and Parquet all allow for schema evolution. Create an updated Avro schemas that include the field <code>geohash</code> for source and destination airports.   Create this in the <code>schemas</code> directory as <code>routesv2.avsc</code>. Use this new schema to validate the previously created <code>routes.avro</code> dataset. If you evolved your schema correctly, the new schema should be backward compatible. </p>"},{"location":"lessons/10-week/week03/#submission-instructions","title":"Submission Instructions","text":"<p>For this assignment, you will submit a zip archive containing the contents of the <code>dsc650/assignments/assignment03/</code> directory. Use the naming convention of <code>assignment03_LastnameFirstname.zip</code> for the zip archive. You can create this archive in Bash (or a similar Unix shell) using the following commands. </p> <pre><code>cd dsc650/assignments\nzip -r assignment03_DoeJane.zip assignment03\n</code></pre> <p>Likewise, you can create a zip archive using Windows PowerShell with the following command. </p> <pre><code>Compress-Archive -Path assignment03 -DestinationPath 'assignment03_DoeJane.zip\n</code></pre> <p>When decompressed, the output should have the following directory structure. </p> <pre><code>\u251c\u2500\u2500 __init__.py\n\u251c\u2500\u2500 results\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 comparison.csv\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 geoindex\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 routes.avro\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 routes.parquet\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 routes.pb\n\u2502\u00a0\u00a0 \u2514\u2500\u2500 validation-results.csv\n\u251c\u2500\u2500 routes_pb2.py\n\u251c\u2500\u2500 routesv2_pb2.py\n\u251c\u2500\u2500 run_assignment.py\n\u251c\u2500\u2500 schemas\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 routes-schema.json\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 routes.avsc\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 routes.proto\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 routesv2.avsc\n\u2502\u00a0\u00a0 \u2514\u2500\u2500 routesv2.proto\n\u2514\u2500\u2500 util.py\n</code></pre> <p>Your assignment may also contain additional files depending on you choose to implement your code. </p>"},{"location":"lessons/10-week/week03/#discussion","title":"Discussion","text":"<p>For this discussion, pick one of the following topics and write a 250 to 750-word discussion board post. Use the DSC 650 Slack channel for discussion and replies.  For grading purposes, copy and paste your initial post and at least two replies to the Blackboard discussion board. </p>"},{"location":"lessons/10-week/week03/#topic-1","title":"Topic 1","text":"<p>Describe a real-world use case for snowflake schemas and data cubes. In particular, why would you want to use a snowflake schema instead of the presumably normalized schemas of an operational transactional database? </p>"},{"location":"lessons/10-week/week03/#topic-2","title":"Topic 2","text":"<p>Parquet is a column-oriented data storage format, while Avro is a row-oriented format. Describe a use case where you would choose a column-oriented format over a row-oriented format. Similarly, describe a use case where you would choose a row-oriented format. </p>"},{"location":"lessons/10-week/week03/#topic-3","title":"Topic 3","text":"<p>Describe the trade-offs associated with different data compression algorithms. Why would one choose a compression algorithm like Snappy over an algorithm like Gzip or Bzip2? Should you use these algorithms with audio or video data? How should you use compression with encrypted data? </p>"},{"location":"lessons/10-week/week03/#topic-4","title":"Topic 4","text":"<p>We briefly talked about different data indexing strategies including B-Trees, LSM trees, and SSTables. Provide examples of other data indexing algorithms and how you might use them in a production environment. </p> <ol> <li> <p>Cell width and height numbers taken from https://www.movable-type.co.uk/scripts/geohash.html \u21a9</p> </li> </ol>"},{"location":"lessons/10-week/week04/","title":"Week 4","text":"<p>In this lesson, you learned how to implement batch processing (i.e., not real-time) using a typical batch processing workflow techniques. You will also gain an understanding of how frameworks such as Hadoop, Spark, and TensorFlow parallelize certain computational tasks. </p>"},{"location":"lessons/10-week/week04/#objectives","title":"Objectives","text":"<p>After completing this week, you should be able to:</p> <ul> <li>Implement a rudimentary version of the MapReduce paradigm in Python</li> <li>Create a simple deep learning network using Keras and TensorFlow</li> <li>Design, implement and run a big data workflow using Luigi</li> </ul>"},{"location":"lessons/10-week/week04/#readings","title":"Readings","text":"<ul> <li>Read chapter 10 in Designing Data-Intensive Applications</li> <li>Read chapter 3 in Deep Learning with Python</li> </ul>"},{"location":"lessons/10-week/week04/#weekly-resources","title":"Weekly Resources","text":"<ul> <li>Celery Architecture</li> <li>Kubernetes Architecture</li> <li>HDFS Architecture</li> <li>YARN Architecture</li> </ul>"},{"location":"lessons/10-week/week04/#assignment-4","title":"Assignment 4","text":"<p>In this assignment, you will be creating a workflow to process emails from Enron that were made available by the Federal Energy Regulatory Commission during its investigation of the company. The original data is not in a machine-friendly format, so we will use Python\u2019s built-in <code>email</code> package to read the emails and create a machine-friendly dataset. </p> <p>The <code>data/external/enron</code> folder contains a partial copy of the original Enron email dataset (you can download the full dataset here). Each folder represents a single users' email account. Each one of those folders contains that user's top-level folders and those folders contain the individual emails. The following is the directory structure of a single user folder. </p> <pre><code>enron/zipper-a\n\u251c\u2500\u2500 all_documents\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 1.\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 10.\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 11.\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 12.\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 13.\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 14.\n.\n.\n.\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 8.\n\u2502\u00a0\u00a0 \u2514\u2500\u2500 9.\n\u2514\u2500\u2500 tss\n    \u251c\u2500\u2500 1.\n    \u251c\u2500\u2500 10.\n    \u251c\u2500\u2500 11.\n    \u251c\u2500\u2500 12.\n        .\n        .\n        .\n    \u251c\u2500\u2500 4.\n    \u251c\u2500\u2500 5.\n    \u251c\u2500\u2500 6.\n    \u251c\u2500\u2500 7.\n    \u251c\u2500\u2500 8.\n    \u2514\u2500\u2500 9.\n</code></pre> <p>Looking at the example of <code>/enron/zipper-a/inbox/114.</code> demonstrates the email structure. The email starts with standard email headers and then includes a plain text message body.  This is typical of most of the emails except some email bodies being encoded in HTML. </p> <pre><code>Message-ID: &lt;6742786.1075845426893.JavaMail.evans@thyme&gt;\nDate: Thu, 7 Jun 2001 11:05:33 -0700 (PDT)\nFrom: jeffrey.hammad@enron.com\nTo: andy.zipper@enron.com\nSubject: Thanks for the interview\nMime-Version: 1.0\nContent-Type: text/plain; charset=us-ascii\nContent-Transfer-Encoding: 7bit\nX-From: Hammad, Jeffrey &lt;/O=ENRON/OU=NA/CN=RECIPIENTS/CN=NOTESADDR/CN=CBBE377A-24F58854-862567DD-591AE7&gt;\nX-To: Zipper, Andy &lt;/O=ENRON/OU=NA/CN=RECIPIENTS/CN=AZIPPER&gt;\nX-cc: \nX-bcc: \nX-Folder: \\Zipper, Andy\\Zipper, Andy\\Inbox\nX-Origin: ZIPPER-A\nX-FileName: Zipper, Andy.pst\n\nAndy,\n\nThanks for giving me the opportunity to meet with you about the Analyst/ Associate program.  I enjoyed talking to you, and look forward to contributing to the success that the program has enjoyed.  \n\nThanks and Best Regards,\n\nJeff Hammad\n</code></pre> <p>For this assignment, you will be parsing each one of those emails into a structured format. The following are the fields that should appear in the structured output. </p> Field Description username The username of this mailbox (the name of email folder) original_msg The original, unparsed email message payload The unparsed payload of the email Message-ID 'Message-ID' from email header Date Parsed datetime from email header From 'From' from email header To 'To' from email header Subject 'Subject' from email header Mime-Version 'Mime-Version' from email header Content-Type 'Content-Type' from email header Content-Transfer-Encoding 'Content-Transfer-Encoding' from email header X-From 'X-From' from email header X-To 'X-To' from email header X-cc 'X-cc' from email header X-bcc 'X-bcc' from email header X-Folder 'X-Folder' from email header X-Origin 'X-Origin' from email header X-FileName 'X-FileName' from email header Cc 'Cc' from email header Bcc 'Bcc' from email header <p>The <code>dsc650/assignments/assignment04</code> folder contains partially completed code and placeholder files for this assignment.</p>"},{"location":"lessons/10-week/week04/#assignment-41","title":"Assignment 4.1","text":"<p>The first part of the assignment is to implement a single function that takes the path to an email file and returns a dictionary containing the fields listed in the previous table. </p> <p>The folder <code>dsc650/assignments/assignment04/examples</code> contains examples of messages with both plain and HTML message payloads. It is recommended that you start by parsing these examples first to ensure your <code>read_email</code> function is working properly.  </p>"},{"location":"lessons/10-week/week04/#assignment-42","title":"Assignment 4.2","text":"<p>Next, you will be creating a workflow using the Luigi Python library. This assignment uses Luigi because it is a self-contained Python package and does not require any additional configuration to run. There are many other workflow managers including Apache Airflow, Apache Oozie, LinkedIn's Azkaban, Netflix's Conductor, and Argo for Kubernetes. </p> <p>Luigi, like most workflow engines, breaks workflows into discrete tasks. Individual tasks chain together into workflows by telling the workflow engine which tasks depend on one another. These task dependencies take the form of a directed acyclic graph(DAG). While this may sound complicated, a DAG is a flowchart of which tasks should be executed in what order (the order means it is directed) with the constraint that later tasks cannot loop back and depend on earlier tasks (hence the acyclic requirement). Read the Luigi documentation on its execution model for more information. </p> <p>To start with, you will have one wrapper task that triggers a task to process each folder. Later, we will add tasks that process the outputs of those tasks. The following code provides a rough outline of this workflow. </p> <pre><code>import luigi \n\nclass ProcessMailbox(luigi.Task):\n    mailbox_directory = luigi.Parameter()\n    processed_directory = luigi.Parameter()\n\n    def output(self):\n        pass\n\n    def run(self):\n      pass\n\nclass ProcessEnronEmails(luigi.WrapperTask):\n    emails_directory = luigi.Parameter()\n    processed_directory = luigi.Parameter()\n\n    def requires(self):\n        for directory in directories:\n            yield ProcessMailbox(\n                mailbox_directory=str(directory),\n                processed_directory=str(self.processed_directory)\n            )\n\ndef main():\n    tasks = [\n        ProcessEnronEmails(emails_directory=emails_directory, processed_directory=processed_directory)\n    ]\n    luigi.build(tasks, workers=8, local_scheduler=True)\n\n\nif __name__ == '__main__':\n    main()\n</code></pre> <p>Create and run this workflow.  The following is the example of a workflow that completed with one failed task.  Failures could be caused by problems with the workflow or problems with the data. The advantage of using a tool like Luigi is that you don't need to re-run all the tasks, only the ones that failed. </p> <pre><code>===== Luigi Execution Summary =====\n\nScheduled 151 tasks of which:\n* 23 complete ones were encountered:\n    - 23 ProcessMailbox(mailbox_directory=dsc650/data/external/enron/dean-c, processed_directory=dsc650/data/processed/enron) ...\n* 126 ran successfully:\n    - 126 ProcessMailbox(mailbox_directory=dsc650/data/external/enron/allen-p, processed_directory=dsc650/data/processed/enron) ...\n* 1 failed:\n    - 1 ProcessMailbox(mailbox_directory=dsc650/data/external/enron/stokley-c, processed_directory=dsc650/data/processed/enron)\n* 1 were left pending, among these:\n    * 1 had failed dependencies:\n        - 1 ProcessEnronEmails(emails_directory=dsc650/data/external/enron, processed_directory=dsc650/data/processed/enron)\n\nThis progress looks :( because there were failed tasks\n\n===== Luigi Execution Summary =====\n</code></pre> <p>This is an example of a workflow that ran without errors.</p> <pre><code>INFO: ===== Luigi Execution Summary =====\n\nScheduled 12 tasks of which:\n* 12 ran successfully:\n    - 1 ProcessEnronEmails(emails_directory=dsc650/data/external/enron, processed_directory=dsc650/data/processed/enron)\n- 11 ProcessMailbox(mailbox_directory=dsc650/data/external/enron/davis-d, processed_directory=dsc650/data/processed/enron) ...\n\nThis progress looks :) because there were no failed tasks or missing dependencies\n\n===== Luigi Execution Summary =====\n</code></pre> <p>Additionally, Luigi has a web-based central scheduler that you use to view and manage the progress of your workflows.  </p>"},{"location":"lessons/10-week/week04/#assignment-43","title":"Assignment 4.3","text":"<p>Now that you have processed the emails and extracted the text payload, you are going to further process them with a simple MapReduce program. We will start by performing a simple word count on text data; the hello world of MapReduce. </p> <p>The Spark Examples page has a simple example of a word count in Python. We will walk through this example together and then you will implement your version of MapReduce in Python. </p> <pre><code>text_file = sc.textFile(\"hdfs://...\")\ncounts = text_file.flatMap(lambda line: line.split(\" \")) \\\n             .map(lambda word: (word, 1)) \\\n             .reduceByKey(lambda a, b: a + b)\ncounts.saveAsTextFile(\"hdfs://...\")\n</code></pre> <p>While the example may look complicated, the entire program consists of applying three functions to a text file. After the program loads the text file and assigns it to the <code>text_file</code> variable, it applies a function to each line in the text file. And outputs a list of words.  The program uses the <code>flatMap</code> because it produces multiple outputs for every input parameter (i.e., multiple words for every line). The <code>map</code> function then takes each of those words and outputs a key-value pair for each word.  If we had the sentence <code>The quick brown fox jumps over the lazy dog</code>, the map function would output the following key-value pairs. </p> <pre><code>def split_to_words(line):\n  for word in line.split(\" \")\n    yield (word, 1)\n\nsplit_to_words('The quick brown fox jumps over the lazy dog')\n\n(The, 1)\n(quick, 1)\n(brown, 1)\n(fox, 1)\n(jumps, 1)\n(over, 1)\n(the, 1)\n(lazy, 1)\n(dog, 1)\n</code></pre> <p>These steps are part of the map stage of the MapReduce programming paradigm. Map functions scale extremely well to large datasets as they can be run in parallel without any coordination. Each mapper takes a different chunk of the data and outputs a series of key-value pairs. </p> <p>By contrast, the reduce step requires sorting the key-value pairs and combine the values with common keys.  To illustrate, suppose we have two mappers with the following output from the first mapper.</p> <pre><code>(the, 1)\n(data, 1)\n(the, 1)\n(data, 1)\n(jump, 1)\n(data, 1)\n</code></pre> <p>Then suppose the second mapper has this output.</p> <pre><code>(the, 1)\n(hide, 1)\n(data, 1)\n(the, 1)\n(data, 1)\n</code></pre> <p>The reduce phase sorts each of the mapper outputs by their keys and combines.  Thus, the reducer would act on the outputs as follow. </p> <pre><code>reducer(\n (the, 1), (the, 1), (the, 1), (the, 1)\n) -&gt; (the, 4)\nreducer(\n    (data, 1), (data, 1), (data, 1), (data, 1), (data, 1)\n) -&gt; (data, 5)\nreducer((jump, 1)) -&gt; (jump, 1)\nreducer((hide, 1)) -&gt; (hide, 1)\n</code></pre> <p>After combining all the results of all the mappers with the reducers, we then have a count for each of the words. </p> <p>Implement a mapper as a Luigi task that outputs the words and counts for each of the mailboxes. Then implement a reducer that aggregates all of those counts into a combined count. The <code>dsc650/assignments/assignment04/results/</code> should look like as follows. </p> <pre><code>results\n\u251c\u2500\u2500 count.txt\n\u2514\u2500\u2500 words\n    \u251c\u2500\u2500 davis-d.txt\n    \u251c\u2500\u2500 gay-r.txt\n    \u251c\u2500\u2500 may-l.txt\n    . \n    . \n    .\n    \u2514\u2500\u2500 zipper-a.txt\n</code></pre> <p>These tasks should be a part of the previous workflow you created. </p>"},{"location":"lessons/10-week/week04/#submission-instructions","title":"Submission Instructions","text":"<p>For this assignment, you will submit a zip archive containing the contents of the <code>dsc650/assignments/assignment04/</code> directory. Use the naming convention of <code>assignment04_LastnameFirstname.zip</code> for the zip archive. You can create this archive in Bash (or a similar Unix shell) using the following commands. </p> <pre><code>cd dsc650/assignments\nzip -r assignment04_DoeJane.zip assignment04\n</code></pre> <p>Likewise, you can create a zip archive using Windows PowerShell with the following command. </p> <pre><code>Compress-Archive -Path assignment04 -DestinationPath 'assignment04_DoeJane.zip\n</code></pre>"},{"location":"lessons/10-week/week04/#discussion","title":"Discussion","text":"<p>For this discussion, describe a batch workflow use case that you would run on a daily, weekly, or monthly basis. What are the inputs and the outputs? Use the DSC 650 Slack channel for discussion and replies.  For grading purposes, copy and paste your initial post and at least two replies to the Blackboard discussion board. </p>"},{"location":"lessons/10-week/week05/","title":"Week 5","text":"<p>In this lesson you will create a batch machine-learning workflow using deep learning examples from Deep Learning with Python.  This workflow should be similar to real-world machine-learning workflows that you may encounter in professional or personal projects. </p>"},{"location":"lessons/10-week/week05/#objectives","title":"Objectives","text":"<p>After completing this week, you should be able to:</p> <ul> <li>Create deep learning models that perform machine learning tasks including binary classification, multi-label classification, and regression</li> <li>Create workflows that train deep learning models and then produce validation and metrics on those models</li> </ul>"},{"location":"lessons/10-week/week05/#readings","title":"Readings","text":"<ul> <li>Read chapters 3 and 4  Deep Learning with Python</li> </ul>"},{"location":"lessons/10-week/week05/#weekly-resources","title":"Weekly Resources","text":""},{"location":"lessons/10-week/week05/#assignment-5","title":"Assignment 5","text":"<p>In this assignment, you will be reproducing the models described in the examples from chapter three of Deep Learning with Python. You will use that code to create a Luigi pipeline that trains the model, uses the model to perform model validation, and output model metrics. </p>"},{"location":"lessons/10-week/week05/#assignment-51","title":"Assignment 5.1","text":"<p>Implement the movie review classifier found in section 3.4 of Deep Learning with Python as a Luigi workflow. Example code and results can be found in <code>dsc650/assignments/assignment05/</code>.  </p>"},{"location":"lessons/10-week/week05/#assignment-52","title":"Assignment 5.2","text":"<p>Implement the news classifier found in section 3.5 of Deep Learning with Python as a Luigi workflow. Example code and results can be found in <code>dsc650/assignments/assignment05/</code>.  </p>"},{"location":"lessons/10-week/week05/#assignment-53","title":"Assignment 5.3","text":"<p>Implement the housing price regression model found in section 3.6 of Deep Learning with Python as a Luigi workflow. Example code and results can be found in <code>dsc650/assignments/assignment05/</code>.  </p>"},{"location":"lessons/10-week/week05/#submission-instructions","title":"Submission Instructions","text":"<p>For this assignment, you will submit a zip archive containing the contents of the <code>dsc650/assignments/assignment05/</code> directory. Use the naming convention of <code>assignment05_LastnameFirstname.zip</code> for the zip archive. You can create this archive in Bash (or a similar Unix shell) using the following commands. </p> <pre><code>cd dsc650/assignments\nzip -r assignment05_DoeJane.zip assignment05\n</code></pre> <p>Likewise, you can create a zip archive using Windows PowerShell with the following command. </p> <pre><code>Compress-Archive -Path assignment05 -DestinationPath 'assignment05_DoeJane.zip\n</code></pre>"},{"location":"lessons/10-week/week05/#discussion-board","title":"Discussion Board","text":"<p>For this discussion, write a 250 to 750-word discussion board post about how you would implement a similar deep learning workflow for a use case that is applicable to your professional or personal interests.  In this use case, how often would you need to train the models? How would you deploy the models? Use the DSC 650 Slack channel for discussion and replies.  For grading purposes, copy and paste your initial post and at least two replies to the Blackboard discussion board. </p>"},{"location":"lessons/10-week/week06/","title":"Week 6","text":"<p>In this lesson you will learn about Convolutional Neural Networks (ConvNets/CNNs). These are neural networks that are suited for a variety of image recognition tasks including image classification and object detection. </p>"},{"location":"lessons/10-week/week06/#objectives","title":"Objectives","text":"<p>After completing this week, you should be able to:</p> <ul> <li>Build a ConvNet from labeled image data to perform multiple category image classification</li> <li>Understand how to use existing models to classify images</li> <li>Describe how to fine-tune existing models for specific classification tasks</li> </ul>"},{"location":"lessons/10-week/week06/#readings","title":"Readings","text":"<ul> <li>Read chapter 5 in Deep Learning with Python</li> </ul>"},{"location":"lessons/10-week/week06/#weekly-resources","title":"Weekly Resources","text":"<ul> <li>CIFAR-10 DataSet</li> <li>Common Objects in Context COCO Dataset </li> <li>TensorFlow Image Captioning</li> <li>TensorFlow Transfer Learning</li> <li>You Only Look Once: Unified, Real-Time Object Detection</li> </ul>"},{"location":"lessons/10-week/week06/#assignment-6","title":"Assignment 6","text":""},{"location":"lessons/10-week/week06/#assignment-61","title":"Assignment 6.1","text":"<p>Using section 5.1 in Deep Learning with Python as a guide (listing 5.3 in particular), create a ConvNet model that classifies images in the MNIST digit dataset. Save the model, predictions, metrics, and validation plots in the <code>dsc650/assignments/assignment06/results</code> directory. </p>"},{"location":"lessons/10-week/week06/#assignment-62","title":"Assignment 6.2","text":""},{"location":"lessons/10-week/week06/#a","title":"a.","text":"<p>Using section 5.2 in Deep Learning with Python as a guide, create a ConvNet model that classifies images CIFAR10 small images classification dataset. Do not use dropout or data-augmentation in this part. Save the model, predictions, metrics, and validation plots in the <code>dsc650/assignments/assignment06/results</code> directory. </p>"},{"location":"lessons/10-week/week06/#b","title":"b.","text":"<p>Using section 5.2 in Deep Learning with Python as a guide, create a ConvNet model that classifies images CIFAR10 small images classification dataset. This time includes dropout and data-augmentation. Save the model, predictions, metrics, and validation plots in the <code>dsc650/assignments/assignment06/results</code> directory. </p>"},{"location":"lessons/10-week/week06/#assignment-63","title":"Assignment 6.3","text":"<p>Load the ResNet50 model and classify the images found in the <code>data/raw/images</code> directory. Save the predictions <code>dsc650/assignments/assignment06/results/predictions/resnet50</code> directory. </p>"},{"location":"lessons/10-week/week06/#submission-instructions","title":"Submission Instructions","text":"<p>For this assignment, you will submit a zip archive containing the contents of the <code>dsc650/assignments/assignment06/</code> directory. Use the naming convention of <code>assignment06_LastnameFirstname.zip</code> for the zip archive. You can create this archive in Bash (or a similar Unix shell) using the following commands. </p> <pre><code>cd dsc650/assignments\nzip -r assignment06_DoeJane.zip assignment06\n</code></pre> <p>Likewise, you can create a zip archive using Windows PowerShell with the following command. </p> <pre><code>Compress-Archive -Path assignment06 -DestinationPath 'assignment06_DoeJane.zip\n</code></pre>"},{"location":"lessons/10-week/week06/#discussion-board","title":"Discussion Board","text":"<p>In this lesson, we focused on using ConvNets to classify entire images.  In real-world use cases, we often want to perform different tasks such as object detection, image captioning, or face detection.  For this discussion, pick one of the three topics below and write a 250 to 750-word discussion board post. Use the DSC 650 Slack channel for discussion and replies.  For grading purposes, copy and paste your initial post and at least two replies to the Blackboard discussion board. </p>"},{"location":"lessons/10-week/week06/#topic-1-transfer-learning","title":"Topic 1 - Transfer Learning","text":"<p>Transfer learning is a machine learning technique that uses a model trained to solve another problem as the basis to build a related model.  How would you implement transfer learning in a ConvNet or any other deep neural network? What is the benefit of fine-tuning an existing model instead of training your own from scratch? </p>"},{"location":"lessons/10-week/week06/#topic-2-object-detection","title":"Topic 2 - Object detection","text":"<p>In this lesson you trained models to perform simple image classification. In many use cases, we want to be able to pick out specific objects within an image.  What use cases do you see for object detection?  What techniques would you use to perform object detection? </p>"},{"location":"lessons/10-week/week06/#topic-3-face-detection","title":"Topic 3 - Face Detection","text":"<p>Face detection and recognition is one application of deep neural networks. What techniques are used to train models for face detection and recognition? Are there unsupervised techniques that do not require labeling the data? </p>"},{"location":"lessons/10-week/week07/","title":"Week 7","text":"<p>In previous lessons, we covered how to encode data in different formats and the basics of different query languages.  Now, we will discuss how to handle distributed datasets by replicated data across multiple nodes and partitioning data. </p>"},{"location":"lessons/10-week/week07/#objectives","title":"Objectives","text":"<p>After completing this week, you should be able to:</p> <ul> <li>Compare and contrast different data replication strategies and discuss their advantages and disadvantages</li> <li>Implement basic data partitioning paradigms using Python and Parquet</li> <li>Describe how partitioning and replication affects data queries</li> </ul>"},{"location":"lessons/10-week/week07/#readings","title":"Readings","text":"<ul> <li>Read chapters 5 and 6 in Designing Data-Intensive Applications</li> </ul>"},{"location":"lessons/10-week/week07/#weekly-resources","title":"Weekly Resources","text":"<ul> <li>Cassandra</li> <li>HDFS Architecture</li> <li>YARN Architecture</li> </ul>"},{"location":"lessons/10-week/week07/#assignment-7","title":"Assignment 7","text":""},{"location":"lessons/10-week/week07/#assignment-71","title":"Assignment 7.1","text":"<p>In this part of the assignment, you will partition a dataset using different strategies.  You will use the <code>routes.parquet</code> dataset you created in a previous assignment. For this dataset, the key for each route will be the three-letter source airport code concatenated with the three-letter destination airport code and the two-letter airline.  For instance, a route from Omaha Eppley Airfield (OMA) to Denver International Airport (DEN) on American Airlines (AA) has a key of <code>OMADENAA</code>.  </p>"},{"location":"lessons/10-week/week07/#a","title":"a.","text":"<p>Start by loading the dataset from the previous assignment using Pandas's read_parquet method. Next, add the concatenated key then using Panda's apply method to create a new column called <code>key</code>. For this part of the example, we will create 16 partitions so that we can compare it to the partitions we create from hashed keys in the next part of the assignment.  The partitions are determined by the first letter of the composite key using the following partitions. </p> <pre><code>    partitions = (\n        ('A', 'A'), ('B', 'B'), ('C', 'D'), ('E', 'F'),\n        ('G', 'H'), ('I', 'J'), ('K', 'L'), ('M', 'M'),\n        ('N', 'N'), ('O', 'P'), ('Q', 'R'), ('S', 'T'),\n        ('U', 'U'), ('V', 'V'), ('W', 'X'), ('Y', 'Z')\n    )\n</code></pre> <p>In this case <code>('A', 'A')</code> means the folder should contain all of the routes whose composite key starts with <code>A</code>.  Similarly, <code>('E', 'F')</code> should contain routes whose composite key starts with <code>E</code> or <code>F</code>. </p> <p>The <code>results/kv</code> directory should contain the following folders.</p> <pre><code>kv\n\u251c\u2500\u2500 kv_key=A\n\u251c\u2500\u2500 kv_key=B\n\u251c\u2500\u2500 kv_key=C-D\n\u251c\u2500\u2500 kv_key=E-F\n\u251c\u2500\u2500 kv_key=G-H\n\u251c\u2500\u2500 kv_key=I-J\n\u251c\u2500\u2500 kv_key=K-L\n\u251c\u2500\u2500 kv_key=M\n\u251c\u2500\u2500 kv_key=N\n\u251c\u2500\u2500 kv_key=O-P\n\u251c\u2500\u2500 kv_key=Q-R\n\u251c\u2500\u2500 kv_key=S-T\n\u251c\u2500\u2500 kv_key=U\n\u251c\u2500\u2500 kv_key=V\n\u251c\u2500\u2500 kv_key=W-X\n\u2514\u2500\u2500 kv_key=Y-Z\n</code></pre> <p>An easy way to create this directory structure is to create a new key called <code>kv_key</code> from the <code>key</code> column and use the to_parquet method with <code>partition_cols=['kv_key']</code> to save a partitioned dataset. </p>"},{"location":"lessons/10-week/week07/#b","title":"b.","text":"<p>Next, we are going to partition the dataset again, but this time we will partition by the hash value of the key.  The following is a function that will create a SHA256 hash of the input key and return a hexadecimal string representation of the hash. </p> <pre><code>import hashlib\n\ndef hash_key(key):\n    m = hashlib.sha256()\n    m.update(str(key).encode('utf-8'))\n    return m.hexdigest()\n</code></pre> <p>We will partition the data using the first character of the hexadecimal hash.  As such, there are 16 possible partitions. Create a new column called <code>hashed</code> that is a hashed value of the <code>key</code> column.  Next, create a partitioned dataset based on the first character of the hashed key and save the results to <code>results/hash</code>.  The directory should contain the following folders. </p> <pre><code>hash\n\u251c\u2500\u2500 hash_key=0\n\u251c\u2500\u2500 hash_key=1\n\u251c\u2500\u2500 hash_key=2\n\u251c\u2500\u2500 hash_key=3\n\u251c\u2500\u2500 hash_key=4\n\u251c\u2500\u2500 hash_key=5\n\u251c\u2500\u2500 hash_key=6\n\u251c\u2500\u2500 hash_key=7\n\u251c\u2500\u2500 hash_key=8\n\u251c\u2500\u2500 hash_key=9\n\u251c\u2500\u2500 hash_key=A\n\u251c\u2500\u2500 hash_key=B\n\u251c\u2500\u2500 hash_key=C\n\u251c\u2500\u2500 hash_key=D\n\u251c\u2500\u2500 hash_key=E\n</code></pre>"},{"location":"lessons/10-week/week07/#c","title":"c.","text":"<p>Finally, we will simulate multiple geographically distributed data centers. For this example, we will assume we have three data centers located in the western, central, and eastern United States.  Google lists the locations of their data centers and we will use the following locations for our three data centers.  * West     * The Dalles, Oregon     * Latitude: 45.5945645     * Longitude: -121.1786823 * Central     * Papillion, NE     * Latitude: 41.1544433     * Longitude: -96.0422378 * East     * Loudoun County, Virginia     * Latitude: 39.08344     * Longitude: -77.6497145</p> <p>Assume that you have an application that provides routes for each of the source airports and you want to store routes in the data center closest to the source airport.  The output folders should look as follows. </p> <pre><code>geo\n\u251c\u2500\u2500 location=central\n\u251c\u2500\u2500 location=east\n\u2514\u2500\u2500 location=west\n</code></pre>"},{"location":"lessons/10-week/week07/#d","title":"d.","text":"<p>Create a Python function that takes as input a list of keys and the number of partitions and returns a list of keys sorted into the specified number of partitions. The partitions should be roughly equal in size. Furthermore, the partitions should have the property that each partition contains all the keys between the least key in the partition and the greatest key in the partition.  In other words, the partitions should be ordered.  </p> <pre><code>def balance_partitions(keys, num_partitions):\n    partitions = []\n    return partitions\n</code></pre>"},{"location":"lessons/10-week/week07/#submission-instructions","title":"Submission Instructions","text":"<p>For this assignment, you will submit a zip archive containing the contents of the <code>dsc650/assignments/assignment07/</code> directory. Use the naming convention of <code>assignment07_LastnameFirstname.zip</code> for the zip archive. You can create this archive in Bash (or a similar Unix shell) using the following commands. </p> <pre><code>cd dsc650/assignments\nzip -r assignment07_DoeJane.zip assignment07\n</code></pre> <p>Likewise, you can create a zip archive using Windows PowerShell with the following command. </p> <pre><code>Compress-Archive -Path assignment07 -DestinationPath 'assignment07_DoeJane.zip\n</code></pre>"},{"location":"lessons/10-week/week07/#discussion-board","title":"Discussion Board","text":"<p>For this discussion, pick one of the following topics and write a 250 to 750-word discussion board post. Use the DSC 650 Slack channel for discussion and replies.  For grading purposes, copy and paste your initial post and at least two replies to the Blackboard discussion board. </p>"},{"location":"lessons/10-week/week07/#topic-1","title":"Topic 1","text":"<p>Compare and contrast the different replication and partitioning strategies used by different databases.  Examples include HBase, Cassandra, PostgreSQL, and DynamoDB.  What are the advantages and disadvantages associated with each strategy?  What use cases are best suited for each paradigm? </p>"},{"location":"lessons/10-week/week07/#topic-2","title":"Topic 2","text":"<p>Apache Zookeeper is a key component of many big data applications.  Provide examples of Zookeeper use cases.  How does Zookeeper compare to etcd? </p>"},{"location":"lessons/10-week/week07/#topic-3","title":"Topic 3","text":"<p>Provide a specific example of how HBase uses key-range partitioning to speed up data queries.  Describe a typical query pattern for HBase. </p>"},{"location":"lessons/10-week/week08/","title":"Week 8","text":"<p>In this lesson, we will learn about real-time data streams, message systems, and transactions in distributed systems. </p>"},{"location":"lessons/10-week/week08/#objectives","title":"Objectives","text":"<p>After completing this week, you should be able to:</p> <ul> <li>Implement scalable stream processing in Spark</li> <li>Explain different approaches to transactions in distributed systems and the associated trade-offs</li> </ul>"},{"location":"lessons/10-week/week08/#readings","title":"Readings","text":"<ul> <li>Read chapters 7, 9 and 11 in Designing Data-Intensive Applications</li> <li>(Optional) Read chapters 8 in Designing Data-Intensive Applications</li> <li>Read Kafka Use Cases</li> <li>Read The Log: What every software engineer should know about real-time data's unifying abstraction</li> </ul>"},{"location":"lessons/10-week/week08/#weekly-resources","title":"Weekly Resources","text":"<ul> <li>etcd</li> <li>Kafka Use Cases</li> <li>Kafka Introduction</li> <li>The Log: What every software engineer should know about real-time data's unifying abstraction</li> <li>RabbitMQ Semantics</li> <li>Representational State Transfer REST</li> <li>Spark Structured Streaming</li> <li>Zookeeper</li> </ul>"},{"location":"lessons/10-week/week08/#assignment-8","title":"Assignment 8","text":"<p>For this assignment, we will be using data from the Berkeley Deep Drive. We will only use a small fraction of the original dataset as the full dataset contains hundreds of gigabytes of video and other data. In particular, this assignment uses route data to simulate data collected from GPS and accelerometer sensors within a car. The data has already been pre-processed in a format and structure that is easy to use with Spark Streaming. </p> <p>The <code>data/processed/bdd/</code> folder contains the processed data for this assignment. The <code>accelerations</code> folder contains accelerometer data collected from each car and the <code>locations</code> contain the GPS data. Each folder contains sub-folders organized by the timestamp of the simulation. </p> <pre><code>bdd\n\u251c\u2500\u2500 accelerations\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 t=000.0\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 19b9aa10588646b3bf22c9b4865a7995.parquet\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 1f0490ec0c464285bebf75ddc77d55cd.parquet\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 dad7eae44e784b549c8c5a3aa051a8c7.parquet\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2514\u2500\u2500 ef5bf698308b481992c4e6b3fe952738.parquet\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 t=001.5\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 19b9aa10588646b3bf22c9b4865a7995.parquet\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 1f0490ec0c464285bebf75ddc77d55cd.parquet\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 8f4116d6de194a32a75ddfea5c4de733.parquet\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 dad7eae44e784b549c8c5a3aa051a8c7.parquet\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2514\u2500\u2500 ef5bf698308b481992c4e6b3fe952738.parquet\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 t=003.2\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 1\n.\n.\n.\n\u2514\u2500\u2500 locations\n    \u251c\u2500\u2500 t=000.0\n    \u2502\u00a0\u00a0 \u251c\u2500\u2500 19b9aa10588646b3bf22c9b4865a7995.parquet\n    \u2502\u00a0\u00a0 \u2514\u2500\u2500 dad7eae44e784b549c8c5a3aa051a8c7.parquet\n    \u251c\u2500\u2500 t=001.5\n    \u2502\u00a0\u00a0 \u251c\u2500\u2500 19b9aa10588646b3bf22c9b4865a7995.parquet\n    \u2502\u00a0\u00a0 \u251c\u2500\u2500 1f0490ec0c464285bebf75ddc77d55cd.parquet\n    \u2502\u00a0\u00a0 \u251c\u2500\u2500 8f4116d6de194a32a75ddfea5c4de733.parquet\n    \u2502\u00a0\u00a0 \u251c\u2500\u2500 dad7eae44e784b549c8c5a3aa051a8c7.parquet\n    \u2502\u00a0\u00a0 \u2514\u2500\u2500 ef5bf698308b481992c4e6b3fe952738.parquet\n    \u251c\u2500\u2500 t=003.2\n    \u2502\u00a0\u00a0 \u251c\u2500\u2500 19b9aa10588646b3bf22c9b4865a7995.parquet\n    \u2502\u00a0\u00a0 \u251c\u2500\u2500 1f0490ec0c464285bebf75ddc77d55cd.parquet\n    \u2502\u00a0\u00a0 \u251c\u2500\u2500 2bde3df6005e4dfe8dc4e6f924a7a1e9.parquet\n    .\n    .\n    .\n    \u2502\u00a0\u00a0 \u2514\u2500\u2500 e3b8748d226e4b54b85e06ec4a6387a6.parquet\n    \u251c\u2500\u2500 t=128.0\n    \u2502\u00a0\u00a0 \u251c\u2500\u2500 1fe7295294fd498385d1946140d40db1.parquet\n    \u2502\u00a0\u00a0 \u251c\u2500\u2500 2094231ab7af41658702c1a3a1da7272.parquet\n    \u2502\u00a0\u00a0 \u251c\u2500\u2500 771b57ec8774441ca65dacf1dca57edd.parquet\n    \u2502\u00a0\u00a0 \u2514\u2500\u2500 e3b8748d226e4b54b85e06ec4a6387a6.parquet\n    \u2514\u2500\u2500 t=128.8\n        \u251c\u2500\u2500 2094231ab7af41658702c1a3a1da7272.parquet\n        \u251c\u2500\u2500 771b57ec8774441ca65dacf1dca57edd.parquet\n        \u2514\u2500\u2500 e3b8748d226e4b54b85e06ec4a6387a6.parquet\n</code></pre> <p>In this example, the folder <code>t=000.0</code> is the start of the simulated data.  The folder <code>t=052.2</code> is 52.2 seconds into the simulation and <code>t=128.8</code> is 128.8 seconds into the simulation. </p>"},{"location":"lessons/10-week/week08/#assignment-81","title":"Assignment 8.1","text":"<p>The first part of the assignment involves creating a script, <code>dsc650/assignments/assignment08/stream_data.py</code>, that mimics a real-time streaming data feed. The following is the directory structure of the results directory for this assignment. </p> <pre><code>assignment08/results\n\u2514\u2500\u2500 stream\n    \u251c\u2500\u2500 input\n    \u251c\u2500\u2500 output\n    \u2514\u2500\u2500 staging\n</code></pre> <p>The basic loop for the <code>stream_data.py</code> script is simple.  The script should load each of the processed directories in the appropriate time order. </p> <p>For example, once your script has passed the 52.5-second mark it should perform the following steps. </p> <ol> <li>Load the data from the <code>t=052.5</code> directory.</li> <li>Calculate a new <code>timestamp</code> column value by adding the <code>offset</code> column to the datetime value of when you started the simulation. </li> <li>Write the updated parquet files to the <code>results/stream/staging</code> directory. </li> <li>Move the files from the <code>staging</code> directory to the <code>input</code> directory. This is necessary to prevent Spark from reading partially written files. </li> </ol> <p>Your <code>results</code> folder should look similar to the one below right before you move the data from <code>staging</code> to <code>input</code>. </p> <pre><code>results\n\u2514\u2500\u2500 stream\n    \u251c\u2500\u2500 input\n    \u2502\u00a0\u00a0 \u251c\u2500\u2500 accelerations\n    \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 t=000.0\n    \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 t=001.5\n    \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 t=003.2\n      . .\n      .\n    \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 t=050.6\n    \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2514\u2500\u2500 t=051.6\n    \u2502\u00a0\u00a0 \u2514\u2500\u2500 locations\n    \u2502\u00a0\u00a0     \u251c\u2500\u2500 t=000.0\n    \u2502\u00a0\u00a0     \u251c\u2500\u2500 t=001.5\n    \u2502\u00a0\u00a0     \u251c\u2500\u2500 t=003.2\n    \u2502\u00a0\u00a0     \u251c\u2500\u2500 t=004.5\n        .\n        . .\n    \u2502\u00a0\u00a0     \u251c\u2500\u2500 t=049.5\n    \u2502\u00a0\u00a0     \u251c\u2500\u2500 t=050.6\n    \u2502\u00a0\u00a0     \u2514\u2500\u2500 t=051.6\n    \u251c\u2500\u2500 output\n    \u2514\u2500\u2500 staging\n        \u251c\u2500\u2500 accelerations\n        \u2502\u00a0\u00a0 \u2514\u2500\u2500 t=052.5\n        \u2514\u2500\u2500 locations\n            \u2514\u2500\u2500 t=052.5\n</code></pre> <p>When your script starts, you will probably want to remove any existing files in the <code>staging</code> and <code>input</code> directories. </p>"},{"location":"lessons/10-week/week08/#assignment-82","title":"Assignment 8.2","text":"<p>In the second part of the exercise, you will create two streaming dataframes using the <code>accelerations</code> and <code>locations</code> folders. </p>"},{"location":"lessons/10-week/week08/#a","title":"a.","text":"<p>Start by creating a simple Spark Streaming application that reads data from the <code>accelerations</code> and <code>locations</code> folders in <code>results/input</code> and uses file sink to save the results to <code>results/output/simple</code>.   </p>"},{"location":"lessons/10-week/week08/#b","title":"b.","text":"<p>Define a watermark on both dataframes using the <code>timestamp</code> column. Set the threshold for the watermark at \"30 seconds\". Set a window of \"15 seconds\" and compute the mean speed of each route. Save the results in <code>results/output/windowed/</code> and set the output mode to <code>update</code>.  </p>"},{"location":"lessons/10-week/week08/#c","title":"c.","text":"<p>Join the two streams together on the UUID as an outer join.  Save the results in <code>results/output/stream-joined</code>.  </p>"},{"location":"lessons/10-week/week08/#submission-instructions","title":"Submission Instructions","text":"<p>For this assignment, you will submit a zip archive containing the contents of the <code>dsc650/assignments/assignment08/</code> directory. Use the naming convention of <code>assignment08_LastnameFirstname.zip</code> for the zip archive. You can create this archive in Bash (or a similar Unix shell) using the following commands. </p> <pre><code>cd dsc650/assignments\nzip -r assignment08_DoeJane.zip assignment08\n</code></pre> <p>Likewise, you can create a zip archive using Windows PowerShell with the following command. </p> <pre><code>Compress-Archive -Path assignment08 -DestinationPath 'assignment08_DoeJane.zip\n</code></pre>"},{"location":"lessons/10-week/week08/#discussion-board","title":"Discussion Board","text":"<p>For this discussion, pick one of the following topics and write a 250 to 750-word discussion board post. Use the DSC 650 Slack channel for discussion and replies.  For grading purposes, copy and paste your initial post and at least two replies to the Blackboard discussion board. </p>"},{"location":"lessons/10-week/week08/#topic-1","title":"Topic 1","text":"<p>Kafka and other data systems make heavy use of the log data structure. What is the log data structure? What problems does it solve that makes it useful for distributed systems.  What other data systems make use of this data structure? </p>"},{"location":"lessons/10-week/week08/#topic-2","title":"Topic 2","text":"<p>Representational State Transfer RESTREST is a software architectural style often used to create web services. One of the key properties of REST is an emphasis on not sharing state between the client and the server application. As Roy Fielding explained in his doctoral dissertation: </p> <p>We next add a constraint to the client-server interaction: communication must be stateless in nature, as in the client-stateless-server (CSS) style of Section 3.4.3 (Figure 5-3), such that each request from client to server must contain all of the information necessary to understand the request, and cannot take advantage of any stored context on the server. Session state is therefore kept entirely on the client.</p> <p>How does this style of architecture compare to synchronous architectures such as an AMQP message broker? What properties of REST make it suitable for web-scale applications? </p>"},{"location":"lessons/10-week/week08/#topic-3","title":"Topic 3","text":"<p>Describe how different database systems handle transactions.  Pick three or more different systems to compare and contrast.  </p>"},{"location":"lessons/10-week/week09/","title":"Week 9","text":"<p>In this lesson we learn how to preprocess text-based data and train deep learning models on that data.  </p>"},{"location":"lessons/10-week/week09/#objectives","title":"Objectives","text":"<p>After completing this week, you should be able to:</p> <ul> <li>Transform text input into tokens and convert those tokens into numeric vectors using one-hot encoding and feature hashing.</li> <li>Build basic text-processing models using recurrent neural networks (RNN)</li> <li>Understand how word embeddings such as Word2Vec can help improve the performance of text-processing models</li> </ul>"},{"location":"lessons/10-week/week09/#readings","title":"Readings","text":"<ul> <li>Read chapter 6 in Deep Learning with Python</li> </ul>"},{"location":"lessons/10-week/week09/#weekly-resources","title":"Weekly Resources","text":"<ul> <li>Global Vectors for Word Representation</li> <li>Large Movie Review Dataset</li> <li>Extracting, transforming and selecting features</li> </ul>"},{"location":"lessons/10-week/week09/#assignment-9","title":"Assignment 9","text":""},{"location":"lessons/10-week/week09/#91","title":"9.1","text":"<p>In the first part of the assignment, you will implement basic text-preprocessing functions in Python.  These functions do not need to scale to large text documents and will only need to handle small inputs. </p>"},{"location":"lessons/10-week/week09/#a","title":"a.","text":"<p>Create a <code>tokenize</code> function that splits a sentence into words. Ensure that your tokenizer removes basic punctuation. </p> <pre><code>def tokenize(sentence):\n    tokens = []\n    # tokenize the sentence\n    return tokens\n````\n\n#### b.  \n\nImplement an `ngram` function that splits tokens into N-grams. \n\n```python\ndef ngram(tokens, n):\n    ngrams = []\n    # Create ngrams\n    return ngrams\n</code></pre>"},{"location":"lessons/10-week/week09/#c","title":"c.","text":"<p>Implement an <code>one_hot_encode</code> function to create a vector from a numerical vector from a list of tokens. </p> <pre><code>def one_hot_encode(tokens, num_words):\n    token_index = {}\n    results = ''\n    return results\n</code></pre>"},{"location":"lessons/10-week/week09/#92","title":"9.2","text":"<p>Using listings 6.16, 6.17, and 6.18 in Deep Learning with Python as a guide, train a sequential model with embeddings on the IMDB data found in <code>data/external/imdb/</code>. Save the model performance metrics and training and validation accuracy curves in the <code>dsc650/assignments/assignment9/results/model_1</code> directory. </p>"},{"location":"lessons/10-week/week09/#93","title":"9.3","text":"<p>Using listing 6.27 in Deep Learning with Python as a guide, fit the same data with an LSTM layer. Save the model performance metrics and training and validation accuracy curves in the <code>dsc650/assignments/assignment9/results/model_2</code> directory. </p>"},{"location":"lessons/10-week/week09/#94","title":"9.4","text":"<p>Using listing 6.46 in Deep Learning with Python as a guide, fit the same data with a simple 1D convnet. Save the model performance metrics and training and validation accuracy curves in the <code>dsc650/assignments/assignment09/results/model_3</code> directory. </p>"},{"location":"lessons/10-week/week09/#submission-instructions","title":"Submission Instructions","text":"<p>For this assignment, you will submit a zip archive containing the contents of the <code>dsc650/assignments/assignment09/</code> directory. Use the naming convention of <code>assignment09_LastnameFirstname.zip</code> for the zip archive. You can create this archive in Bash (or a similar Unix shell) using the following commands. </p> <pre><code>cd dsc650/assignments\nzip -r assignment09_DoeJane.zip assignment09\n</code></pre> <p>Likewise, you can create a zip archive using Windows PowerShell with the following command. </p> <pre><code>Compress-Archive -Path assignment09 -DestinationPath 'assignment09_DoeJane.zip\n</code></pre>"},{"location":"lessons/10-week/week09/#discussion-board","title":"Discussion Board","text":"<p>For this discussion, pick one of the following topics and write a 250 to 750-word discussion board post. Use the DSC 650 Slack channel for discussion and replies.  For grading purposes, copy and paste your initial post and at least two replies to the Blackboard discussion board. </p>"},{"location":"lessons/10-week/week09/#topic-1","title":"Topic 1","text":"<p>Compare and contrast using MapReduce, Spark, and Deep Learning Frameworks (e.g. TensorFlow) for performing text preprocessing and building text-based models. Are there use cases where it makes sense to use one over another? </p>"},{"location":"lessons/10-week/week09/#topic-2","title":"Topic 2","text":"<p>How might you combine stream processing such as Spark's stream processing framework with deep learning models? Provide use cases that are relevant to your professional or personal interests. </p>"},{"location":"lessons/10-week/week10/","title":"Week 10","text":"<p>In this lesson, we will explore the future of big data and deep learning. </p>"},{"location":"lessons/10-week/week10/#objectives","title":"Objectives","text":"<p>After completing this week, you should be able to:</p> <ul> <li>Describe upcoming advances in big data and deep learning and their potential use cases</li> <li>Experiment with advanced deep learning use cases including text and image generation</li> </ul>"},{"location":"lessons/10-week/week10/#readings","title":"Readings","text":"<ul> <li>Chapter 12 in Designing Data-Intensive Applications</li> <li>Read chapters 8 and 9 in Deep Learning with Python</li> </ul>"},{"location":"lessons/10-week/week10/#weekly-resources","title":"Weekly Resources","text":""},{"location":"lessons/10-week/week10/#assignment-10","title":"Assignment 10","text":""},{"location":"lessons/10-week/week10/#assignment-101","title":"Assignment 10.1","text":"<p>Using section 8.1 in Deep Learning with Python as a guide, implement an LSTM text generator. Train the model on the Enron corpus or a text source of your choice. Save the model and generate 20 examples to the <code>results</code> directory of <code>dsc650/assignments/assignment10/</code>.  </p>"},{"location":"lessons/10-week/week10/#assignment-102","title":"Assignment 10.2","text":"<p>Using section 8.4 in Deep Learning with Python as a guide, implement a variational autoencoder using the MNIST data set and save a grid of 15 x 15 digits to the <code>results/vae</code> directory. If you would rather work on a more interesting dataset, you can use the CelebFaces Attributes Dataset instead. </p>"},{"location":"lessons/10-week/week10/#submission-instructions","title":"Submission Instructions","text":"<p>For this assignment, you will submit a zip archive containing the contents of the <code>dsc650/assignments/assignment10/</code> directory. Use the naming convention of <code>assignment10_LastnameFirstname.zip</code> for the zip archive. You can create this archive in Bash (or a similar Unix shell) using the following commands. </p> <pre><code>cd dsc650/assignments\nzip -r assignment10_DoeJane.zip assignment10\n</code></pre> <p>Likewise, you can create a zip archive using Windows PowerShell with the following command. </p> <pre><code>Compress-Archive -Path assignment10 -DestinationPath 'assignment10_DoeJane.zip\n</code></pre>"},{"location":"lessons/10-week/week10/#discussion-board","title":"Discussion Board","text":"<p>For this discussion, pick an area of big data or deep learning that we did discuss in-depth and explain why you find it exciting. Topics include, but are not limited to Kubernetes, deep learning hardware, and cloud computing. Write a 250 to 750-word discussion board post to describe this topic. Use the DSC 650 Slack channel for discussion and replies.  For grading purposes, copy and paste your initial post and at least two replies to the Blackboard discussion board. </p>"},{"location":"lessons/12-week/","title":"Index","text":"<p>Documentation in Progress</p> <p>Check back soon for more updates.    </p>"},{"location":"lessons/12-week/week01/","title":"Week 1","text":"<p>To paraphrase the late Douglas Adams:</p> <p>Big data is big. You just won't believe how vastly, hugely, mind-bogglingly big it is.  I mean, you may think your collection of movies, pictures, and music is big, but that's just peanuts to big data. </p> <p>Big Data is big in two distinct ways. First, as the name suggests, Big Data is about how to deal with large amounts of data. Tech giants like Google and Facebook store exabytes of data. While multiple exabytes of data is an impressive amount of data, it is nowhere near the theoretical limits. </p> <p>Second, Big Data is a wide area of study that spans a wide range of technologies and concepts. Because of the size and rapid rate of change of the subject, we will only be able to cover a small fraction of Big Data topics in this course. </p> <p>In this course, we will focus on two main areas: the design of data-driven systems and deep learning algorithms.  The first area, the design of data-driven systems, takes a high-level look into how the different components of Big Data systems fit together.  The second area, deep learning, focuses on a specific approach to extracting information from large, usually unstructured datasets. </p>"},{"location":"lessons/12-week/week01/#objectives","title":"Objectives","text":"<p>After completing this week, you should be able to:</p> <ul> <li>Setup a development environment PySpark, Keras, and TensorFlow  and run a simple proof of concept</li> <li>Explain how reliability, scalability, and maintainability impacts data-driven systems</li> <li>Summarize how artificial intelligence, machine learning, and deep learning relate to one another</li> <li>Determine what problems deep learning and big data help solve</li> </ul>"},{"location":"lessons/12-week/week01/#readings","title":"Readings","text":"<ul> <li>Read chapter 1 in Designing Data-Intensive Applications</li> <li>Read chapter 1 in Deep Learning with Python</li> <li>Visit DSC 650 website and follow the getting started instructions to setup your development environment</li> <li>Watch CPU vs GPU What's the Difference?</li> <li>Watch How Much Information is in the Universe? | Space Time</li> </ul>"},{"location":"lessons/12-week/week01/#weekly-resources","title":"Weekly Resources","text":"<ul> <li>Backblaze Hard Drive Stats</li> <li>DSC 650 Website</li> <li>DSC 650 Github Repository</li> <li>CPU vs GPU What's the Difference?What's the Difference?</li> <li>How Much Information is in the Universe? | Space Time</li> </ul>"},{"location":"lessons/12-week/week01/#assignment-1","title":"Assignment 1","text":""},{"location":"lessons/12-week/week01/#assignment-11","title":"Assignment 1.1","text":"<p>Visit [DSC 650 website][dsc650] and follow the instructions for getting started.  To demonstrate your environment is in working order, run the examples in the <code>examples</code> folder, and copy the output to the <code>dsc650/assignment01/logs</code> folder.  </p>"},{"location":"lessons/12-week/week01/#a-run-keras-mnist-mlp-example","title":"a.  Run Keras MNIST MLP Example","text":"<p>If you are using Bash the following commands will write both stdout and stderr to a file. </p> <pre><code>$ python examples/mnist_mlp.py &gt; logs/keras-mnist.log 2&gt;&amp;1\n</code></pre> <p>If you are not using Bash, you can manually copy and paste the output into the log file using a text editor. </p>"},{"location":"lessons/12-week/week01/#b-run-pyspark-example","title":"b. Run PySpark Example","text":"<p>If you are using Bash the following commands will write both stdout and stderr to a file. </p> <pre><code>$ python examples/pi.py &gt; logs/spark-pi.log 2&gt;&amp;1\n</code></pre> <p>If you are not using Bash, you can manually copy and paste the output into the log file using a text editor. </p>"},{"location":"lessons/12-week/week01/#assignment-12","title":"Assignment 1.2","text":"<p>For the rest of the assignment, you will answer questions about scaling and maintaining data-driven systems. A Markdown template for this part of the assignment can be found at <code>dsc650/assignments/assignment01/Assignment 01.md</code>.</p>"},{"location":"lessons/12-week/week01/#a-data-sizes","title":"a. Data Sizes","text":"<p>Provide estimates for the size of various data items.  Please explain how you arrived at the estimates for the size of each item by citing references or providing calculations. </p> Data Item Size per Item 128 character message. ? Bytes 1024x768 PNG image ? MB 1024x768 RAW image ? MB HD (1080p) HEVC Video (15 minutes) ? MB HD (1080p) Uncompressed Video (15 minutes) ? MB 4K UHD HEVC Video (15 minutes) ? MB 4k UHD Uncompressed Video (15 minutes) ? MB Human Genome (Uncompressed) ? GB <ul> <li>Assume all videos are 30 frames per second</li> <li>HEVC stands for High Efficiency Video Coding</li> <li>See the Wikipedia article on display resolution for information on HD (1080p) and 4K UHD resolutions. </li> </ul>"},{"location":"lessons/12-week/week01/#b-scaling","title":"b. Scaling","text":"<p>Using the estimates for data sizes in the previous part, determine how much storage space you would need for the following items.  </p> Size # HD Daily Twitter Tweets (Uncompressed) ?? Daily Twitter Tweets (Snappy Compressed) ?? Daily Instagram Photos ?? Daily YouTube Videos ?? Yearly Twitter Tweets (Uncompressed) ?? Yearly Twitter Tweets (Snappy Compressed) ?? Yearly Instagram Photos ?? Yearly YouTube Videos ?? <ul> <li>For estimating the number of hard drives, assume you are using 10 TB and you are storing the data using the Hadoop Distributed File System (HDFS).  By default, HDFS stores three copies of each piece of data, so you will need to triple the amount storage required. </li> <li>Twitter statistics estimates 500 million tweets are sent each day. For simplicity, assume each tweet is 128 characters. </li> <li>See the Snappy Github repository for estimates of Snappy's performance. </li> <li>Instagram statistics estimates over 100 million videos and photos are uploaded to Instagram every day.   Assume that 75% of those items are 1024x768 PNG photos.  </li> <li>YouTube statistics estimates 500 hours of video is uploaded to YouTube every minute.  For simplicity, assume all videos are HD quality encoded using HEVC at 30 frames per second. </li> </ul>"},{"location":"lessons/12-week/week01/#c-reliability","title":"c. Reliability","text":"<p>Using the yearly estimates from the previous part, estimate the number of hard drive failures per year using data from Backblaze's hard drive statistics.</p> # HD # Failures Twitter Tweets (Uncompressed) ?? Twitter Tweets (Snappy Compressed) ?? Instagram Photos ?? YouTube Videos ??"},{"location":"lessons/12-week/week01/#d-latency","title":"d. Latency","text":"<p>Provide estimates of the one way latency for each of the following items.  Please explain how you arrived at the estimates for each item by citing references or providing calculations. </p> One Way Latency Los Angeles to Amsterdam ? ms Low Earth Orbit Satellite ? ms Geostationary Satellite ? ms Earth to the Moon ? ms Earth to Mars ? minutes"},{"location":"lessons/12-week/week01/#submission-instructions","title":"Submission Instructions","text":"<p>For this assignment, you will submit a zip archive containing the contents of the <code>dsc650/assignments/assignment01/</code> directory. Use the naming convention of <code>assignment01_LastnameFirstname.zip</code> for the zip archive. You can create this archive in Bash (or a similar Unix shell) using the following commands. </p> <pre><code>cd dsc650/assignments\nzip -r assignment01_DoeJane.zip assignment01\n</code></pre> <p>Likewise, you can create a zip archive using Windows PowerShell with the following command. </p> <pre><code>Compress-Archive -Path assignment01 -DestinationPath 'assignment01_DoeJane.zip\n</code></pre> <p>When decompressed, the output should have the following directory structure. </p> <pre><code>\u251c\u2500\u2500 assignment01\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 Assignment\\ 01.md\n\u2502\u00a0\u00a0 \u2514\u2500\u2500 logs\n\u2502\u00a0\u00a0     \u251c\u2500\u2500 keras-mnist.log\n\u2502\u00a0\u00a0     \u2514\u2500\u2500 spark-pi.log\n</code></pre>"},{"location":"lessons/12-week/week01/#discussion","title":"Discussion","text":"<p>You are required to have a minimum of 10 posts each week.  Similar to previous courses, any topic counts towards your discussion count, as long as you are active more than 2 days per week with 10 posts, you will receive full credit. </p>"},{"location":"lessons/12-week/week02/","title":"Week 2","text":"<p>In the previous lesson, we learned about the fundamentals of deep learning and data-driven systems. Now that we have a high-level overview, we will dive into examples of how to model, query, and process data using different paradigms. </p>"},{"location":"lessons/12-week/week02/#objectives","title":"Objectives","text":"<p>After completing this week, you should be able to:</p> <ul> <li>Query and process data using multiple paradigms including graph processing, map-reduce, and SQL</li> <li>Compare and contrast different data models including identifying prime use cases for different data models</li> <li>Demonstrate how to represent data as tensors and apply tensor mathematical operations</li> </ul>"},{"location":"lessons/12-week/week02/#readings","title":"Readings","text":"<ul> <li>Read chapters 2 and 3 in Designing Data-Intensive Applications</li> <li>Read chapter 2 in Deep Learning with Python</li> </ul>"},{"location":"lessons/12-week/week02/#weekly-resources","title":"Weekly Resources","text":"<ul> <li>TinyDB</li> <li>OrientDB Getting Started</li> <li>OrientDB Download</li> <li>Keras</li> <li>Multi-Dimensional Data (as used in Tensors)</li> <li>SQL Tutorial</li> <li>TensorFlow Quickstart</li> </ul>"},{"location":"lessons/12-week/week02/#assignment-2","title":"Assignment 2","text":"<p>For this assignment, we will be working with the CSV data found in the <code>data/external/tidynomicon</code> folder.  Specifically, we will be using with the <code>measurements.csv</code>, <code>person.csv</code>, <code>site.csv</code>, and <code>visited.csv</code> files. </p> <p>If you are running on JupyterHub hosted on the BU Data Science Cluster, you can load data from the cluster's Amazon S3-compatible data storage.  The following code demonstrates how to load the <code>site.csv</code> data into a Pandas dataframe. </p> <pre><code> import pandas as pd\n import s3fs\n\n s3 = s3fs.S3FileSystem(\n    anon=True,\n    client_kwargs={\n        'endpoint_url': 'https://storage.budsc.midwest-datascience.com'\n      }\n   )\n\n df = pd.read_csv(\n    s3.open('data/external/tidynomicon/site.csv', mode='rb')\n )\n</code></pre> <p>The other files have the same names as the files located in the repositories <code>data/external/tidynomicon</code> folder. </p>"},{"location":"lessons/12-week/week02/#assignment-21","title":"Assignment 2.1","text":"<p>Complete the code in <code>kvdb.ipynb</code> to implement a basic key-value database that saves its state to a json file.  Use that code to create databases that store each of CSV files by key. The json files should be stored in the <code>dsc650/assignments/assignment02/results/kvdb/</code> folder. </p> Input File Output File Key <code>measurements.csv</code> <code>measurements.json</code> Composite key <code>person.csv</code> <code>people.json</code> <code>person_id</code> <code>site.csv</code> <code>sites.json</code> <code>site_id</code> <code>visited.csv</code> <code>visits.json</code> Composite key <p>The <code>measurements.csv</code> and <code>visited.csv</code> have composite keys that use multiple columns. For <code>measurements.csv</code> those fields are <code>visit_id</code>, <code>person_id</code>, and <code>quantity</code>.  For <code>visited.csv</code> those fields are <code>visit_id</code> and <code>site_id</code>.  The following is an example of code that sets and gets the value using a composite key. </p> <pre><code> kvdb_path = 'visits.json'\n kvdb = KVDB(kvdb_path)\n key = (619, 'DR-1')\n value = dict(\n    visit_id=619,\n    site_id='DR-1',\n    visit_date='1927-02-08'\n )\n kvdb.set_value(key, value)\n retrieved_value = kvdb.get_value(key)\n# Retrieved should be the same as value\n</code></pre>"},{"location":"lessons/12-week/week02/#assignment-22","title":"Assignment 2.2","text":"<p>Now we will create a simple document database using the <code>tinydb</code> library. TinyDB stores its data as a JSON file. For this assignment, you will store the TinyDB database in <code>dsc650/assignments/assignment02/results/patient-info.json</code>.  You will store a document for each person in the database which should look like this. </p> <pre><code> {\n\"person_id\": \"dyer\",\n\"personal_name\": \"William\",\n\"family_name\": \"Dyer\",\n\"visits\": [\n{\n\"visit_id\": 619,\n\"site_id\": \"DR-1\",\n\"visit_date\": \"1927-02-08\",\n\"site\": {\n\"site_id\": \"DR-1\",\n\"latitude\": -49.85,\n\"longitude\": -128.57\n},\n\"measurements\": [\n{\n\"visit_id\": 619,\n\"person_id\": \"dyer\",\n\"quantity\": \"rad\",\n\"reading\": 9.82\n},\n{\n\"visit_id\": 619,\n\"person_id\": \"dyer\",\n\"quantity\": \"sal\",\n\"reading\": 0.13\n}\n]\n},\n{\n\"visit_id\": 622,\n\"site_id\": \"DR-1\",\n\"visit_date\": \"1927-02-10\",\n\"site\": {\n\"site_id\": \"DR-1\",\n\"latitude\": -49.85,\n\"longitude\": -128.57\n},\n\"measurements\": [\n{\n\"visit_id\": 622,\n\"person_id\": \"dyer\",\n\"quantity\": \"rad\",\n\"reading\": 7.8\n},\n{\n\"visit_id\": 622,\n\"person_id\": \"dyer\",\n\"quantity\": \"sal\",\n\"reading\": 0.09\n}\n]\n}\n]\n}\n</code></pre> <p>The <code>dsc650/assignments/assignment02/documentdb.ipynb</code> file contains code that should assist you in this task. </p>"},{"location":"lessons/12-week/week02/#assignment-23","title":"Assignment 2.3","text":"<p>In this part, you will create a SQLite database that you will store in <code>dsc650/assignments/assignment02/results/patient-info.db</code>.  The <code>dsc650/assignments/assignment02/rdbms.ipynb</code> file should contain code to assist you in the creation of this database. </p>"},{"location":"lessons/12-week/week02/#assignment-24","title":"Assignment 2.4","text":"<p>Go to the Wikidata Query Service website and perform the following SPARQL query. </p> <pre><code>#Recent Events\n SELECT ?event ?eventLabel ?date\n WHERE\n {\n    # find events\n    ?event wdt:P31/wdt:P279* wd:Q1190554.\n    # with a point in time or start date\n    OPTIONAL { ?event wdt:P585 ?date. }\n    OPTIONAL { ?event wdt:P580 ?date. }\n    # but at least one of those\n    FILTER(BOUND(?date) &amp;&amp; DATATYPE(?date) = xsd:dateTime).\n    # not in the future, and not more than 31 days ago\n    BIND(NOW() - ?date AS ?distance).\n    FILTER(0 &lt;= ?distance &amp;&amp; ?distance &lt; 31).\n    # and get a label as well\n    OPTIONAL {\n        ?event rdfs:label ?eventLabel.\n        FILTER(LANG(?eventLabel) = \"en\").\n    }\n }\n# limit to 10 results so we don't timeout\n LIMIT 10\n</code></pre> <p>Modify the query so that the column order is <code>date</code>, <code>event</code>, and <code>eventLabel</code> instead of <code>event</code>, <code>eventLabel</code>, and <code>date</code>. Download the results as a JSON file and copy the results to <code>dsc650/assignments/assignment02/results/wikidata-query.json</code>. </p>"},{"location":"lessons/12-week/week02/#submission-instructions","title":"Submission Instructions","text":"<p>For this assignment, you will submit a zip archive containing the contents of the <code>dsc650/assignments/assignment02/</code> directory. Use the naming convention of <code>assignment02_LastnameFirstname.zip</code> for the zip archive.</p> <p>If you are using Jupyter, you can create a zip archive by running the <code>Package Assignments.ipynb</code> notebook. </p> <p>You can create this archive on your local machine using Bash (or a similar Unix shell) using the following commands. </p> <pre><code> cd dsc650/assignments\n zip -r assignment02_DoeJane.zip assignment02\n</code></pre> <p>Likewise, you can create a zip archive using Windows PowerShell with the following command. </p> <pre><code> Compress-Archive -Path assignment02 -DestinationPath 'assignment02_DoeJane.zip\n</code></pre> <p>When decompressed, the output should have the following directory structure. </p> <pre><code> \u251c\u2500\u2500 assignment02\n \u2502   \u251c\u2500\u2500 documentdb.ipynb\n \u2502   \u251c\u2500\u2500 kvdb.ipynb\n \u2502   \u251c\u2500\u2500 rdbms.ipynb\n \u2502   \u251c\u2500\u2500 results\n \u2502   \u2502   \u251c\u2500\u2500 kvdb\n \u2502   \u2502   \u2502   \u251c\u2500\u2500 measurements.json\n \u2502   \u2502   \u2502   \u251c\u2500\u2500 people.json\n \u2502   \u2502   \u2502   \u251c\u2500\u2500 sites.json\n \u2502   \u2502   \u2502   \u2514\u2500\u2500 visits.json\n \u2502   \u2502   \u251c\u2500\u2500 patient-info.db\n \u2502   \u2502   \u251c\u2500\u2500 patient-info.fs\n \u2502   \u2502   \u251c\u2500\u2500 patient-info.json\n \u2502   \u2502   \u2514\u2500\u2500 wikidata-query.json\n</code></pre>"},{"location":"lessons/12-week/week02/#discussion","title":"Discussion","text":"<p>You are required to have a minimum of 10 posts each week.  Similar to previous courses, any topic counts towards your discussion count, as long as you are active more than 2 days per week with 10 posts, you will receive full credit. </p>"},{"location":"lessons/12-week/week03/","title":"Week 3","text":"<p>In this lesson, we learn about the data structures, encodings, and schemas used to store data and data indexes.</p>"},{"location":"lessons/12-week/week03/#objectives","title":"Objectives","text":"<p>After completing this week, you should be able to:</p> <ul> <li>Compare indexing algorithms including hash indexes and B-Trees</li> <li>Determine use cases for different methods of organizing data including column-oriented storage and snowflake schemas</li> <li>Make use of different encoding formats including Avro, Thrift, JSON, XML, and Protocol Buffers</li> <li>Describe the different methods of using data schemas including schema-on-read, schema-on-write, and different methods of schema evolution</li> </ul>"},{"location":"lessons/12-week/week03/#readings","title":"Readings","text":"<ul> <li>Read chapters 3 and 4 in Designing Data-Intensive Applications</li> </ul>"},{"location":"lessons/12-week/week03/#weekly-resources","title":"Weekly Resources","text":"<ul> <li>Apache Arrow</li> <li>Apache Parquet</li> <li>Apache Thrift</li> <li>Apache Avro</li> <li>JSON Schema</li> <li>Protocol Buffers</li> </ul>"},{"location":"lessons/12-week/week03/#assignment-3","title":"Assignment 3","text":"<p>For this assignment, you will be working with data from OpenFlights.  This data was originally obtained from the OpenFlights Github repository and a copy of the original data is found in <code>data/external/openflights/</code>. For this assignment, you will use a dataset derived from that original data. You can find this data in <code>data/processed/openflights/routes.jsonl.gz</code>.   The data is compressed with gzip and encoded in the JSON Lines format.  Each line represents a single airline route. </p> <p>The <code>dsc650/assignments/assignment03</code> directory contains placeholder code and data outputs for this assignment. </p>"},{"location":"lessons/12-week/week03/#assignment-31","title":"Assignment 3.1","text":"<p>In the first part of the assignment, you will be creating schemas for the route data and encoding the <code>routes.jsonl.gz</code> using Protocol Buffers, Avro, and Parquet. </p>"},{"location":"lessons/12-week/week03/#a-json-schema","title":"a. JSON Schema","text":"<p>Create a JSON Schema in the <code>schemas/routes-schema.json</code> file to describe a route and validate the data in <code>routes.jsonl.gz</code> using the jsonschema library. </p>"},{"location":"lessons/12-week/week03/#b-avro","title":"b. Avro","text":"<p>Use the fastavro library to create <code>results/routes.avro</code> with the schema provided. </p>"},{"location":"lessons/12-week/week03/#c-parquet","title":"c. Parquet","text":"<p>Create a Parquet dataset in <code>results/routes.parquet</code> using Apache Arrow. </p>"},{"location":"lessons/12-week/week03/#d-protocol-buffers","title":"d. Protocol Buffers","text":"<p>Using the generated code found in <code>dsc650/assignment/assignment03/routes_pb2.py</code> create <code>results/routes.pb</code> using Protocol Buffers.</p>"},{"location":"lessons/12-week/week03/#e-output-sizes","title":"e. Output Sizes","text":"<p>Compare the output sizes of the different formats.  Populate the results in <code>results/comparison.csv</code>. Compare compressed and uncompressed sizes if possible. </p>"},{"location":"lessons/12-week/week03/#assignment-32","title":"Assignment 3.2","text":"<p>This part of the assignment involves developing a rudimentary database index for our routes dataset.  Filesystems, databases, and NoSQL datastores use various indexing mechanisms to speed queries. </p> <p>The implementation of advanced data structures such as B-Trees, R-Trees, GiST, SSTables, and LSM trees is beyond the scope of this course. Instead, you will implement a rudimentary geospatial index using geohashes and pygeohash. </p> <p>Without going into too much detail, a geohash converts geospatial coordinates (i.e. latitude and longitude) into single, usually, base64 or base32 encoded integer.  Below is an example of the geohashed value for Bellevue University. </p> <pre><code>import pygeohash\npygeohash.encode(41.1499988, -95.91779)\n'9z7f174u17zb'\n</code></pre> <p>Geohashes have the useful property that when they are sorted, entries that are near one another in the sorted list are usually close to one another in space. The following image shows how this gird looks. </p> <p></p> <p>The following table gives cell width and height values for each level of precision of the geohash.</p> Geohash Coordinates Cell Width1 Cell Height 9 22.0, -112.0 \u2264 5,000km 5,000km 9z 42.0, -96.0 \u2264 1,250km 625km 9z7 41.0, -96.0 \u2264 156km 156km 9z7f 41.0, -96.0 \u2264 39.1km 19.5km 9z7f1 41.2, -95.9 \u2264 4.89km 4.89km 9z7f174u 41.15, -95.918 \u2264 38.2m 19.1m 9z7f174u17zb 41.149999, -95.91779 \u2264 4.77m 4.77m <p>As you can see, it only takes about four levels characters to get to a 40 km by 20 km area.  Another level gives 5 km by 5 km. In most cases, going past 12 units of precision is pointless as very few applications require that degree of accuracy. </p> <p></p>"},{"location":"lessons/12-week/week03/#a-create-a-simple-geohash-index","title":"a. Create a Simple Geohash Index","text":"<p>Using <code>pygeohash</code> create a simple index for the <code>routes.jsonl.gz</code> data using the source airport latitude and longitude. Output the index and values to the <code>results/geoindex</code> directory. The output looks like the following directory structure.  </p> <pre><code>geoindex\n\u251c\u2500\u2500 2\n\u2502         \u251c\u2500\u2500 2e\n\u2502         \u2502         \u251c\u2500\u2500 2eg.jsonl.gz\n\u2502         \u2502         \u251c\u2500\u2500 2ev.jsonl.gz\n\u2502         \u2502         \u2514\u2500\u2500 2ey.jsonl.gz\n\u2502         \u251c\u2500\u2500 2h\n\u2502         \u2502         \u251c\u2500\u2500 2h5.jsonl.gz\n\u2502         \u2502         \u251c\u2500\u2500 2hb.jsonl.gz\n\u2502         \u2502         \u2514\u2500\u2500 2hx.jsonl.gz\n\u2502         \u251c\u2500\u2500 2j\n\u2502         \u2502         \u251c\u2500\u2500 2j0.jsonl.gz\n\u2502         \u2502         \u251c\u2500\u2500 2j3.jsonl.gz\n\u2502         \u2502         \u251c\u2500\u2500 2jd.jsonl.gz\n. \n. \n.\n\u2502         \u2514\u2500\u2500 yu\n\u2502             \u2514\u2500\u2500 yue.jsonl.gz\n\u2514\u2500\u2500 z\n    \u251c\u2500\u2500 z0\n    \u2502         \u251c\u2500\u2500 z08.jsonl.gz\n    \u2502         \u251c\u2500\u2500 z0h.jsonl.gz\n    \u2502         \u2514\u2500\u2500 z0m.jsonl.gz\n    \u251c\u2500\u2500 z6\n    \u2502         \u2514\u2500\u2500 z6e.jsonl.gz\n    \u251c\u2500\u2500 z9\n    \u2502         \u2514\u2500\u2500 z92.jsonl.gz\n    \u251c\u2500\u2500 zg\n    \u2502         \u2514\u2500\u2500 zgw.jsonl.gz\n    \u251c\u2500\u2500 zk\n    \u2502         \u2514\u2500\u2500 zk9.jsonl.gz\n    \u251c\u2500\u2500 zs\n    \u2502         \u2514\u2500\u2500 zs4.jsonl.gz\n    \u2514\u2500\u2500 zu\n        \u2514\u2500\u2500 zu3.jsonl.gz\n</code></pre>"},{"location":"lessons/12-week/week03/#b-implement-a-simple-search-feature","title":"b. Implement a Simple Search Feature","text":"<p>Implement a simple geospatial search feature that finds airports within a specified distance of an input latitude and longitude. You can use the <code>geohash_approximate_distance</code> function in <code>pygeohash</code> to compute distances between geohash values.  It returns distances in meters, but your search function should use kilometers as input. </p> <pre><code>import pygeohash\npygeohash.geohash_approximate_distance('bcd3u', 'bc83n')\n# &gt;&gt;&gt; 625441\n</code></pre>"},{"location":"lessons/12-week/week03/#submission-instructions","title":"Submission Instructions","text":"<p>For this assignment, you will submit a zip archive containing the contents of the <code>dsc650/assignments/assignment03/</code> directory. Use the naming convention of <code>assignment03_LastnameFirstname.zip</code> for the zip archive. </p> <p>If you are using Jupyter, you can create a zip archive by running the <code>Package Assignments.ipynb</code> notebook. </p> <p>You can create this archive in Bash (or a similar Unix shell) using the following commands. </p> <pre><code>cd dsc650/assignments\nzip -r assignment03_DoeJane.zip assignment03\n</code></pre> <p>Likewise, you can create a zip archive using Windows PowerShell with the following command. </p> <pre><code>Compress-Archive -Path assignment03 -DestinationPath 'assignment03_DoeJane.zip\n</code></pre>"},{"location":"lessons/12-week/week03/#discussion","title":"Discussion","text":"<p>You are required to have a minimum of 10 posts each week.  Similar to previous courses, any topic counts towards your discussion count, as long as you are active more than 2 days per week with 10 posts, you will receive full credit. Refer to the optional topics below as a starting place.</p>"},{"location":"lessons/12-week/week03/#topic-1","title":"Topic 1","text":"<p>Describe a real-world use case for snowflake schemas and data cubes. In particular, why would you want to use a snowflake schema instead of the presumably normalized schemas of an operational transactional database? </p>"},{"location":"lessons/12-week/week03/#topic-2","title":"Topic 2","text":"<p>Parquet is a column-oriented data storage format, while Avro is a row-oriented format. Describe a use case where you would choose a column-oriented format over a row-oriented format. Similarly, describe a use case where you would choose a row-oriented format. </p>"},{"location":"lessons/12-week/week03/#topic-3","title":"Topic 3","text":"<p>Describe the trade-offs associated with different data compression algorithms. Why would one choose a compression algorithm like Snappy over an algorithm like Gzip or Bzip2? Should you use these algorithms with audio or video data? How should you use compression with encrypted data? </p>"},{"location":"lessons/12-week/week03/#topic-4","title":"Topic 4","text":"<p>We briefly talked about different data indexing strategies including B-Trees, LSM trees, and SSTables. Provide examples of other data indexing algorithms and how you might use them in a production environment. </p> <ol> <li> <p>Cell width and height numbers taken from https://www.movable-type.co.uk/scripts/geohash.html \u21a9</p> </li> </ol>"},{"location":"lessons/12-week/week04/","title":"Week 4","text":"<p>In this lesson, you learned how to implement batch processing (i.e., not real-time) using a typical batch processing workflow techniques. You will also gain an understanding of how frameworks such as Hadoop, Spark, and TensorFlow parallelize certain computational tasks. </p>"},{"location":"lessons/12-week/week04/#objectives","title":"Objectives","text":"<p>After completing this week, you should be able to:</p> <ul> <li>Create a scalable, end-to-end machine learning pipeline using Spark</li> </ul>"},{"location":"lessons/12-week/week04/#readings","title":"Readings","text":"<ul> <li>Read chapter 10 in Designing Data-Intensive Applications</li> <li>Read chapter 3 in Deep Learning with Python</li> </ul>"},{"location":"lessons/12-week/week04/#weekly-resources","title":"Weekly Resources","text":"<ul> <li>Celery Architecture</li> <li>Kubernetes Architecture</li> <li>HDFS Architecture</li> <li>YARN Architecture</li> </ul>"},{"location":"lessons/12-week/week04/#assignment-4","title":"Assignment 4","text":"<p>In this assignment, you will be creating a workflow to process emails from Enron that were made available by the Federal Energy Regulatory Commission during its investigation of the company. The original data is not in a machine-friendly format, so we will use Python\u2019s built-in <code>email</code> package to read the emails and create a machine-friendly dataset. </p> <p>The <code>data/external/enron</code> folder contains a partial copy of the original Enron email dataset (you can download the full dataset here). Each folder represents a single users' email account. Each one of those folders contains that user's top-level folders and those folders contain the individual emails. The following is the directory structure of a single user folder. </p> <pre><code>enron/zipper-a\n\u251c\u2500\u2500 all_documents\n\u2502   \u251c\u2500\u2500 1.\n\u2502   \u251c\u2500\u2500 10.\n\u2502   \u251c\u2500\u2500 11.\n\u2502   \u251c\u2500\u2500 12.\n\u2502   \u251c\u2500\u2500 13.\n\u2502   \u251c\u2500\u2500 14.\n.\n.\n.\n\u2502   \u251c\u2500\u2500 8.\n\u2502   \u2514\u2500\u2500 9.\n\u2514\u2500\u2500 tss\n    \u251c\u2500\u2500 1.\n    \u251c\u2500\u2500 10.\n    \u251c\u2500\u2500 11.\n    \u251c\u2500\u2500 12.\n        .\n        .\n        .\n    \u251c\u2500\u2500 4.\n    \u251c\u2500\u2500 5.\n    \u251c\u2500\u2500 6.\n    \u251c\u2500\u2500 7.\n    \u251c\u2500\u2500 8.\n    \u2514\u2500\u2500 9.\n</code></pre> <p>Looking at the example of <code>/enron/zipper-a/inbox/114.</code> demonstrates the email structure. The email starts with standard email headers and then includes a plain text message body.  This is typical of most of the emails except some email bodies being encoded in HTML. </p> <pre><code>Message-ID: &lt;6742786.1075845426893.JavaMail.evans@thyme&gt;\nDate: Thu, 7 Jun 2001 11:05:33 -0700 (PDT)\nFrom: jeffrey.hammad@enron.com\nTo: andy.zipper@enron.com\nSubject: Thanks for the interview\nMime-Version: 1.0\nContent-Type: text/plain; charset=us-ascii\nContent-Transfer-Encoding: 7bit\nX-From: Hammad, Jeffrey &lt;/O=ENRON/OU=NA/CN=RECIPIENTS/CN=NOTESADDR/CN=CBBE377A-24F58854-862567DD-591AE7&gt;\nX-To: Zipper, Andy &lt;/O=ENRON/OU=NA/CN=RECIPIENTS/CN=AZIPPER&gt;\nX-cc: \nX-bcc: \nX-Folder: \\Zipper, Andy\\Zipper, Andy\\Inbox\nX-Origin: ZIPPER-A\nX-FileName: Zipper, Andy.pst\n\nAndy,\n\nThanks for giving me the opportunity to meet with you about the Analyst/ Associate program.  I enjoyed talking to you, and look forward to contributing to the success that the program has enjoyed.  \n\nThanks and Best Regards,\n\nJeff Hammad\n</code></pre> <p>For this assignment, you will be parsing each one of those emails into a structured format. The following are the fields that should appear in the structured output. </p> Field Description username The username of this mailbox (the name of email folder) original_msg The original, unparsed email message payload The unparsed payload of the email Message-ID 'Message-ID' from email header Date Parsed datetime from email header From 'From' from email header To 'To' from email header Subject 'Subject' from email header Mime-Version 'Mime-Version' from email header Content-Type 'Content-Type' from email header Content-Transfer-Encoding 'Content-Transfer-Encoding' from email header X-From 'X-From' from email header X-To 'X-To' from email header X-cc 'X-cc' from email header X-bcc 'X-bcc' from email header X-Folder 'X-Folder' from email header X-Origin 'X-Origin' from email header X-FileName 'X-FileName' from email header Cc 'Cc' from email header Bcc 'Bcc' from email header <p>The <code>dsc650/assignments/assignment04</code> folder contains partially completed code and placeholder files for this assignment.</p>"},{"location":"lessons/12-week/week04/#assignment-41","title":"Assignment 4.1","text":"<p>Load the data from the <code>enron.zip</code> dataset into a Spark dataframe. See the Spark SQL Getting Started Guide for information details on how to create a dataframe. The final dataframe should have the following schema. </p> <pre><code>df.printSchema()\n\nroot\n |-- id: string (nullable = true)\n |-- username: string (nullable = true)\n |-- original_msg: string (nullable = true)\n</code></pre>"},{"location":"lessons/12-week/week04/#assignment-42","title":"Assignment 4.2","text":"<p>Implement a function that takes the path to an email file and returns a dictionary containing the fields listed in the previous table. The folder <code>dsc650/assignments/assignment04/examples</code> contains examples of messages with both plain and HTML message payloads. It is recommended that you start by parsing these examples first to ensure your <code>read_email</code> function is working properly. </p> <p>The following is an example of parsing a plain email message. </p> <pre><code>plain_msg_example = \"\"\"\nMessage-ID: &lt;6742786.1075845426893.JavaMail.evans@thyme&gt;\nDate: Thu, 7 Jun 2001 11:05:33 -0700 (PDT)\nFrom: jeffrey.hammad@enron.com\nTo: andy.zipper@enron.com\nSubject: Thanks for the interview\nMime-Version: 1.0\nContent-Type: text/plain; charset=us-ascii\nContent-Transfer-Encoding: 7bit\nX-From: Hammad, Jeffrey &lt;/O=ENRON/OU=NA/CN=RECIPIENTS/CN=NOTESADDR/CN=CBBE377A-24F58854-862567DD-591AE7&gt;\nX-To: Zipper, Andy &lt;/O=ENRON/OU=NA/CN=RECIPIENTS/CN=AZIPPER&gt;\nX-cc: \nX-bcc: \nX-Folder: \\Zipper, Andy\\Zipper, Andy\\Inbox\nX-Origin: ZIPPER-A\nX-FileName: Zipper, Andy.pst\n\nAndy,\n\nThanks for giving me the opportunity to meet with you about the Analyst/ Associate program.  I enjoyed talking to you, and look forward to contributing to the success that the program has enjoyed.  \n\nThanks and Best Regards,\n\nJeff Hammad\n\"\"\"\nparsed_msg = parse_email(plain_msg_example)\nprint(parsed_msg.text)\n</code></pre> <p>Which yields the following.</p> <p>Andy,</p> <p>Thanks for giving me the opportunity to meet with you about the Analyst/ Associate program.  I enjoyed talking to you, and look forward to contributing to the success that the program has enjoyed. </p> <p>Thanks and Best Regards,</p> <p>Jeff Hammad</p>"},{"location":"lessons/12-week/week04/#assignment-43","title":"Assignment 4.3","text":"<p>Finally, you will put together a feature extraction workflow using Spark Pipelines. You will use Spark's MLlib to extract words from the email text and then convert those words into numeric features using a count vectorizor. </p> <pre><code>result.select('id', 'words', 'features').show()\n\n+--------------------+--------------------+--------------------+\n|                  id|               words|            features|\n+--------------------+--------------------+--------------------+\n|        shively-h/2_|[, what, is, this...|      (3,[0],[16.0])|\n|        shively-h/1_|[can, you, please...|(3,[0,1,2],[5.0,1...|\n|shively-h/peoples...|[pgl, and, north,...|(3,[0,1,2],[10.0,...|\n|shively-h/peoples...|[, pgl, and, nort...|(3,[0,1,2],[51.0,...|\n|shively-h/peoples...|[, pgl, and, nort...|(3,[0,1,2],[53.0,...|\n|shively-h/peoples...|[pgl, and, north,...|(3,[0,1,2],[10.0,...|\n|shively-h/peoples...|[, pgl, and, nort...|(3,[0,1,2],[96.0,...|\n|shively-h/peoples...|[pgl, and, north,...|(3,[0,1,2],[13.0,...|\n|shively-h/peoples...|[pgl, and, north,...|(3,[0,1,2],[13.0,...|\n| shively-h/inbox/60_|[i, just, wanted,...|(3,[0,1,2],[47.0,...|\n|shively-h/inbox/134_|[this, is, an, al...|(3,[0,1,2],[6.0,3...|\n| shively-h/inbox/70_|[, please, find, ...|(3,[0,1,2],[8.0,7...|\n|shively-h/inbox/118_|[frank, ermis, -,...| (3,[0,2],[9.0,1.0])|\n| shively-h/inbox/28_|[dear, body, shop...|(3,[0,1,2],[8.0,3...|\n|shively-h/inbox/178_|[, as, you, know,...|(3,[0,1,2],[10.0,...|\n| shively-h/inbox/73_|[hunter, shively,...|(3,[0,1],[13.0,3.0])|\n|  shively-h/inbox/7_|[hunter, --, this...|(3,[0,1,2],[11.0,...|\n| shively-h/inbox/31_|[this, is, very, ...|(3,[0,1,2],[27.0,...|\n| shively-h/inbox/74_|[, , , -----origi...|(3,[0,1,2],[33.0,...|\n| shively-h/inbox/97_|[hunter,, , i, wi...|(3,[0,1,2],[11.0,...|\n+--------------------+--------------------+--------------------+\nonly showing top 20 rows\n</code></pre>"},{"location":"lessons/12-week/week04/#submission-instructions","title":"Submission Instructions","text":"<p>If you are using Jupyter, you can create a zip archive by running the <code>Package Assignments.ipynb</code> notebook. </p> <p>You can create this archive in Bash (or a similar Unix shell) using the following commands.</p> <pre><code>cd dsc650/assignments\nzip -r assignment04_DoeJane.zip assignment04\n</code></pre> <p>Likewise, you can create a zip archive using Windows PowerShell with the following command. </p> <pre><code>Compress-Archive -Path assignment04 -DestinationPath 'assignment04_DoeJane.zip\n</code></pre>"},{"location":"lessons/12-week/week04/#discussion","title":"Discussion","text":"<p>You are required to have a minimum of 10 posts each week.  Similar to previous courses, any topic counts towards your discussion count, as long as you are active more than 2 days per week with 10 posts, you will receive full credit. </p>"},{"location":"lessons/12-week/week05/","title":"Week 5","text":"<p>](https://hackmd.io/BEx2SHHWQXGvtj-9WBlGTA)</p> <p>In this lesson you will create a batch machine-learning workflow using deep learning examples from Deep Learning with Python.  This workflow should be similar to real-world machine-learning workflows that you may encounter in professional or personal projects. </p>"},{"location":"lessons/12-week/week05/#objectives","title":"Objectives","text":"<p>After completing this week, you should be able to:</p> <ul> <li>Create deep learning models that perform machine learning tasks including binary classification, multi-label classification, and regression</li> <li>Create workflows that train deep learning models and then produce validation and metrics on those models</li> </ul>"},{"location":"lessons/12-week/week05/#readings","title":"Readings","text":"<ul> <li>Read chapters 3 and 4  Deep Learning with Python</li> </ul>"},{"location":"lessons/12-week/week05/#weekly-resources","title":"Weekly Resources","text":""},{"location":"lessons/12-week/week05/#assignment-5","title":"Assignment 5","text":"<p>In this assignment, you will be reproducing the models described in the examples from chapter three of Deep Learning with Python. You will use that code to create a workflow that trains the model, uses the model to perform model validation, and output model metrics. </p>"},{"location":"lessons/12-week/week05/#assignment-51","title":"Assignment 5.1","text":"<p>Implement the movie review classifier found in section 3.4 of Deep Learning with Python. </p>"},{"location":"lessons/12-week/week05/#assignment-52","title":"Assignment 5.2","text":"<p>Implement the news classifier found in section 3.5 of Deep Learning with Python. </p>"},{"location":"lessons/12-week/week05/#assignment-53","title":"Assignment 5.3","text":"<p>Implement the housing price regression model found in section 3.6 of Deep Learning with Python. </p>"},{"location":"lessons/12-week/week05/#submission-instructions","title":"Submission Instructions","text":"<p>For this assignment, you will submit a zip archive containing the contents of the <code>dsc650/assignments/assignment05/</code> directory. Use the naming convention of <code>assignment05_LastnameFirstname.zip</code> for the zip archive. </p> <p>If you are using Jupyter, you can create a zip archive by running the <code>Package Assignments.ipynb</code> notebook. </p> <p>You can create this archive in Bash (or a similar Unix shell) using the following commands. </p> <pre><code>cd dsc650/assignments\nzip -r assignment05_DoeJane.zip assignment05\n</code></pre> <p>Likewise, you can create a zip archive using Windows PowerShell with the following command. </p> <pre><code>Compress-Archive -Path assignment05 -DestinationPath 'assignment05_DoeJane.zip\n</code></pre>"},{"location":"lessons/12-week/week05/#discussion-board","title":"Discussion Board","text":"<p>You are required to have a minimum of 10 posts each week.  Similar to previous courses, any topic counts towards your discussion count, as long as you are active more than 2 days per week with 10 posts, you will receive full credit. Refer to the optional topic below as a starting place. </p> <p>Post about how you would implement a similar deep learning workflow for a use case that is applicable to your professional or personal interests.  In this use case, how often would you need to train the models? How would you deploy the models? </p>"},{"location":"lessons/12-week/week06/","title":"DSC 650 (12 Week) Week 06","text":""},{"location":"lessons/12-week/week06/#week-6","title":"Week 6","text":"<p>In this lesson, you will learn about Convolutional Neural Networks (ConvNets/CNNs). These are neural networks that are suited for a variety of image recognition tasks including image classification and object detection. </p>"},{"location":"lessons/12-week/week06/#objectives","title":"Objectives","text":"<p>After completing this week, you should be able to:</p> <ul> <li>Build a ConvNet from labeled image data to perform multiple category image classification</li> <li>Understand how to use existing models to classify images</li> <li>Describe how to fine-tune existing models for specific classification tasks</li> </ul>"},{"location":"lessons/12-week/week06/#readings","title":"Readings","text":"<ul> <li>Read chapter 5 in Deep Learning with Python</li> </ul>"},{"location":"lessons/12-week/week06/#weekly-resources","title":"Weekly Resources","text":"<ul> <li>CIFAR-10 DataSet</li> <li>Common Objects in Context COCO Dataset </li> <li>TensorFlow Image Captioning</li> <li>TensorFlow Transfer Learning</li> <li>You Only Look Once: Unified, Real-Time Object Detection</li> </ul>"},{"location":"lessons/12-week/week06/#assignment-6","title":"Assignment 6","text":""},{"location":"lessons/12-week/week06/#assignment-61","title":"Assignment 6.1","text":"<p>Using section 5.1 in Deep Learning with Python as a guide (listing 5.3 in particular), create a ConvNet model that classifies images in the MNIST digit dataset. Save the model, predictions, metrics, and validation plots in the <code>dsc650/assignments/assignment06/results</code> directory. If you are using JupyterHub, you can include those plots in your Jupyter notebook.</p>"},{"location":"lessons/12-week/week06/#assignment-62","title":"Assignment 6.2","text":""},{"location":"lessons/12-week/week06/#assignment-62a","title":"Assignment 6.2.a","text":"<p>Using section 5.2 in Deep Learning with Python as a guide, create a ConvNet model that classifies images CIFAR10 small images classification dataset. Do not use dropout or data-augmentation in this part. Save the model, predictions, metrics, and validation plots in the <code>dsc650/assignments/assignment06/results</code> directory. If you are using JupyterHub, you can include those plots in your Jupyter notebook.</p>"},{"location":"lessons/12-week/week06/#assignment-62b","title":"Assignment 6.2.b","text":"<p>Using section 5.2 in Deep Learning with Python as a guide, create a ConvNet model that classifies images CIFAR10 small images classification dataset. This time includes dropout and data-augmentation. Save the model, predictions, metrics, and validation plots in the <code>dsc650/assignments/assignment06/results</code> directory. If you are using JupyterHub, you can include those plots in your Jupyter notebook.</p>"},{"location":"lessons/12-week/week06/#assignment-63","title":"Assignment 6.3","text":"<p>Load the ResNet50 model. Perform image classification on five to ten images of your choice. They can be personal images or publically available images. Include the images in <code>dsc650/assignments/assignment06/images/</code>.  Save the predictions <code>dsc650/assignments/assignment06/results/predictions/resnet50</code> directory. If you are using JupyterHub, you can include those plots in your Jupyter notebook.</p>"},{"location":"lessons/12-week/week06/#submission-instructions","title":"Submission Instructions","text":"<p>For this assignment, you will submit a zip archive containing the contents of the <code>dsc650/assignments/assignment06/</code> directory. Use the naming convention of <code>assignment06_LastnameFirstname.zip</code> for the zip archive. </p> <p>If you are using Jupyter, you can create a zip archive by running the <code>Package Assignments.ipynb</code> notebook. </p> <p>You can create this archive in Bash (or a similar Unix shell) using the following commands. </p> <pre><code>cd dsc650/assignments\nzip -r assignment06_DoeJane.zip assignment06\n</code></pre> <p>Likewise, you can create a zip archive using Windows PowerShell with the following command. </p> <pre><code>Compress-Archive -Path assignment06 -DestinationPath 'assignment06_DoeJane.zip\n</code></pre>"},{"location":"lessons/12-week/week06/#discussion-board","title":"Discussion Board","text":"<p>You are required to have a minimum of 10 posts each week.  Similar to previous courses, any topic counts towards your discussion count, as long as you are active more than 2 days per week with 10 posts, you will receive full credit. Refer to the optional topics below as a starting place.</p>"},{"location":"lessons/12-week/week06/#topic-1-transfer-learning","title":"Topic 1 - Transfer Learning","text":"<p>Transfer learning is a machine learning technique that uses a model trained to solve another problem as the basis to build a related model.  How would you implement transfer learning in a ConvNet or any other deep neural network? What is the benefit of fine-tuning an existing model instead of training your own from scratch? </p>"},{"location":"lessons/12-week/week06/#topic-2-object-detection","title":"Topic 2 - Object detection","text":"<p>In this lesson, you trained models to perform simple image classification. In many use cases, we want to be able to pick out specific objects within an image.  What use cases do you see for object detection?  What techniques would you use to perform object detection? </p>"},{"location":"lessons/12-week/week06/#topic-3-face-detection","title":"Topic 3 - Face Detection","text":"<p>Face detection and recognition is one application of deep neural networks. What techniques are used to train models for face detection and recognition? Are there unsupervised techniques that do not require labeling the data? </p>"},{"location":"lessons/12-week/week07/","title":"DSC 650 (12 Week) Week 07","text":""},{"location":"lessons/12-week/week07/#week-7","title":"Week 7","text":"<p>In previous lessons, we covered how to encode data in different formats and the basics of different query languages.  Now, we will discuss how to handle distributed datasets by replicated data across multiple nodes and partitioning data. </p>"},{"location":"lessons/12-week/week07/#objectives","title":"Objectives","text":"<p>After completing this week, you should be able to:</p> <ul> <li>Compare and contrast different data replication strategies and discuss their advantages and disadvantages</li> <li>Implement basic data partitioning paradigms using Python and Parquet</li> <li>Describe how partitioning and replication affects data queries</li> </ul>"},{"location":"lessons/12-week/week07/#readings","title":"Readings","text":"<ul> <li>Read chapters 5 and 6 in Designing Data-Intensive Applications</li> </ul>"},{"location":"lessons/12-week/week07/#weekly-resources","title":"Weekly Resources","text":"<ul> <li>Cassandra</li> <li>HDFS Architecture</li> <li>YARN Architecture</li> </ul>"},{"location":"lessons/12-week/week07/#assignment-7","title":"Assignment 7","text":""},{"location":"lessons/12-week/week07/#assignment-71","title":"Assignment 7.1","text":"<p>In this part of the assignment, you will partition a dataset using different strategies.  You will use the <code>routes.parquet</code> dataset you created in a previous assignment. For this dataset, the key for each route will be the three-letter source airport code concatenated with the three-letter destination airport code and the two-letter airline.  For instance, a route from Omaha Eppley Airfield (OMA) to Denver International Airport (DEN) on American Airlines (AA) has a key of <code>OMADENAA</code>.  </p>"},{"location":"lessons/12-week/week07/#a","title":"a.","text":"<p>Start by loading the dataset from the previous assignment using Pandas's read_parquet method. Next, add the concatenated key then using Panda's apply method to create a new column called <code>key</code>. For this part of the example, we will create 16 partitions so that we can compare it to the partitions we create from hashed keys in the next part of the assignment.  The partitions are determined by the first letter of the composite key using the following partitions. </p> <pre><code>    partitions = (\n        ('A', 'A'), ('B', 'B'), ('C', 'D'), ('E', 'F'),\n        ('G', 'H'), ('I', 'J'), ('K', 'L'), ('M', 'M'),\n        ('N', 'N'), ('O', 'P'), ('Q', 'R'), ('S', 'T'),\n        ('U', 'U'), ('V', 'V'), ('W', 'X'), ('Y', 'Z')\n    )\n</code></pre> <p>In this case <code>('A', 'A')</code> means the folder should contain all of the routes whose composite key starts with <code>A</code>.  Similarly, <code>('E', 'F')</code> should contain routes whose composite key starts with <code>E</code> or <code>F</code>. </p> <p>The <code>results/kv</code> directory should contain the following folders.</p> <pre><code>kv\n\u251c\u2500\u2500 kv_key=A\n\u251c\u2500\u2500 kv_key=B\n\u251c\u2500\u2500 kv_key=C-D\n\u251c\u2500\u2500 kv_key=E-F\n\u251c\u2500\u2500 kv_key=G-H\n\u251c\u2500\u2500 kv_key=I-J\n\u251c\u2500\u2500 kv_key=K-L\n\u251c\u2500\u2500 kv_key=M\n\u251c\u2500\u2500 kv_key=N\n\u251c\u2500\u2500 kv_key=O-P\n\u251c\u2500\u2500 kv_key=Q-R\n\u251c\u2500\u2500 kv_key=S-T\n\u251c\u2500\u2500 kv_key=U\n\u251c\u2500\u2500 kv_key=V\n\u251c\u2500\u2500 kv_key=W-X\n\u2514\u2500\u2500 kv_key=Y-Z\n</code></pre> <p>An easy way to create this directory structure is to create a new key called <code>kv_key</code> from the <code>key</code> column and use the to_parquet method with <code>partition_cols=['kv_key']</code> to save a partitioned dataset. </p>"},{"location":"lessons/12-week/week07/#b","title":"b.","text":"<p>Next, we are going to partition the dataset again, but this time we will partition by the hash value of the key.  The following is a function that will create a SHA256 hash of the input key and return a hexadecimal string representation of the hash. </p> <pre><code>import hashlib\n\ndef hash_key(key):\n    m = hashlib.sha256()\n    m.update(str(key).encode('utf-8'))\n    return m.hexdigest()\n</code></pre> <p>We will partition the data using the first character of the hexadecimal hash.  As such, there are 16 possible partitions. Create a new column called <code>hashed</code> that is a hashed value of the <code>key</code> column.  Next, create a partitioned dataset based on the first character of the hashed key and save the results to <code>results/hash</code>.  The directory should contain the following folders. </p> <pre><code>hash\n\u251c\u2500\u2500 hash_key=0\n\u251c\u2500\u2500 hash_key=1\n\u251c\u2500\u2500 hash_key=2\n\u251c\u2500\u2500 hash_key=3\n\u251c\u2500\u2500 hash_key=4\n\u251c\u2500\u2500 hash_key=5\n\u251c\u2500\u2500 hash_key=6\n\u251c\u2500\u2500 hash_key=7\n\u251c\u2500\u2500 hash_key=8\n\u251c\u2500\u2500 hash_key=9\n\u251c\u2500\u2500 hash_key=A\n\u251c\u2500\u2500 hash_key=B\n\u251c\u2500\u2500 hash_key=C\n\u251c\u2500\u2500 hash_key=D\n\u251c\u2500\u2500 hash_key=E\n</code></pre>"},{"location":"lessons/12-week/week07/#c","title":"c.","text":"<p>Finally, we will simulate multiple geographically distributed data centers. For this example, we will assume we have three data centers located in the western, central, and eastern United States.  Google lists the locations of their data centers and we will use the following locations for our three data centers. </p> <ul> <li>West</li> <li>The Dalles, Oregon</li> <li>Latitude: 45.5945645</li> <li>Longitude: -121.1786823</li> <li>Central</li> <li>Papillion, NE</li> <li>Latitude: 41.1544433</li> <li>Longitude: -96.0422378</li> <li>East</li> <li>Loudoun County, Virginia</li> <li>Latitude: 39.08344</li> <li>Longitude: -77.6497145</li> </ul> <p>Assume that you have an application that provides routes for each of the source airports and you want to store routes in the data center closest to the source airport.  The output folders should look as follows. </p> <pre><code>geo\n\u251c\u2500\u2500 location=central\n\u251c\u2500\u2500 location=east\n\u2514\u2500\u2500 location=west\n</code></pre>"},{"location":"lessons/12-week/week07/#d","title":"d.","text":"<p>Create a Python function that takes as input a list of keys and the number of partitions and returns a list of keys sorted into the specified number of partitions. The partitions should be roughly equal in size. Furthermore, the partitions should have the property that each partition contains all the keys between the least key in the partition and the greatest key in the partition.  In other words, the partitions should be ordered.  </p> <pre><code>def balance_partitions(keys, num_partitions):\n    partitions = []\n    return partitions\n</code></pre>"},{"location":"lessons/12-week/week07/#submission-instructions","title":"Submission Instructions","text":"<p>For this assignment, you will submit a zip archive containing the contents of the <code>dsc650/assignments/assignment07/</code> directory. Use the naming convention of <code>assignment07_LastnameFirstname.zip</code> for the zip archive. You can create this archive in Bash (or a similar Unix shell) using the following commands. </p> <pre><code>cd dsc650/assignments\nzip -r assignment07_DoeJane.zip assignment07\n</code></pre> <p>Likewise, you can create a zip archive using Windows PowerShell with the following command. </p> <pre><code>Compress-Archive -Path assignment07 -DestinationPath 'assignment07_DoeJane.zip\n</code></pre>"},{"location":"lessons/12-week/week07/#discussion-board","title":"Discussion Board","text":"<p>You are required to have a minimum of 10 posts each week.  Similar to previous courses, any topic counts towards your discussion count, as long as you are active more than 2 days per week with 10 posts, you will receive full credit. Refer to the optional topics below as a starting place.</p>"},{"location":"lessons/12-week/week07/#topic-1","title":"Topic 1","text":"<p>Compare and contrast the different replication and partitioning strategies used by different databases.  Examples include HBase, Cassandra, PostgreSQL, and DynamoDB.  What are the advantages and disadvantages associated with each strategy?  What use cases are best suited for each paradigm? </p>"},{"location":"lessons/12-week/week07/#topic-2","title":"Topic 2","text":"<p>Apache Zookeeper is a key component of many big data applications.  Provide examples of Zookeeper use cases.  How does Zookeeper compare to etcd? </p>"},{"location":"lessons/12-week/week07/#topic-3","title":"Topic 3","text":"<p>Provide a specific example of how HBase uses key-range partitioning to speed up data queries.  Describe a typical query pattern for HBase. </p>"},{"location":"lessons/12-week/week08/","title":"Week 8","text":"<p>In this lesson, we will learn about real-time data streams, message systems, and transactions in distributed systems. </p>"},{"location":"lessons/12-week/week08/#objectives","title":"Objectives","text":"<p>After completing this week, you should be able to:</p> <ul> <li>Implement scalable stream processing in Spark</li> <li>Explain different approaches to transactions in distributed systems and the associated trade-offs</li> </ul>"},{"location":"lessons/12-week/week08/#readings","title":"Readings","text":"<ul> <li>Read chapters 7 and 9 in Designing Data-Intensive Applications</li> <li>(Optional) Read chapters 8 in Designing Data-Intensive Applications</li> <li>Read Kafka Use Cases</li> <li>Read The Log: What every software engineer should know about real-time data's unifying abstraction</li> </ul>"},{"location":"lessons/12-week/week08/#weekly-resources","title":"Weekly Resources","text":"<ul> <li>etcd</li> <li>Kafka Use Cases</li> <li>Kafka Introduction</li> <li>The Log: What every software engineer should know about real-time data's unifying abstraction</li> <li>Representational State Transfer (REST)</li> <li>Spark Structured Streaming</li> <li>Zookeeper</li> </ul>"},{"location":"lessons/12-week/week08/#assignment-8","title":"Assignment 8","text":"<p>For this assignment, we will be using data from the Berkeley Deep Drive. We will only use a small fraction of the original dataset as the full dataset contains hundreds of gigabytes of video and other data. In particular, this assignment uses route data to simulate data collected from GPS and accelerometer sensors within a car. The data has already been pre-processed in a format and structure that is easy to use with Spark Streaming. </p> <p>The <code>data/processed/bdd/</code> folder contains the processed data for this assignment. The <code>accelerations</code> folder contains accelerometer data collected from each car and the <code>locations</code> contain the GPS data. Each folder contains sub-folders organized by the timestamp of the simulation. </p> <pre><code>bdd\n\u251c\u2500\u2500 accelerations\n\u2502   \u251c\u2500\u2500 t=000.0\n\u2502   \u2502   \u251c\u2500\u2500 19b9aa10588646b3bf22c9b4865a7995.parquet\n\u2502   \u2502   \u251c\u2500\u2500 1f0490ec0c464285bebf75ddc77d55cd.parquet\n\u2502   \u2502   \u251c\u2500\u2500 dad7eae44e784b549c8c5a3aa051a8c7.parquet\n\u2502   \u2502   \u2514\u2500\u2500 ef5bf698308b481992c4e6b3fe952738.parquet\n\u2502   \u251c\u2500\u2500 t=001.5\n\u2502   \u2502   \u251c\u2500\u2500 19b9aa10588646b3bf22c9b4865a7995.parquet\n\u2502   \u2502   \u251c\u2500\u2500 1f0490ec0c464285bebf75ddc77d55cd.parquet\n\u2502   \u2502   \u251c\u2500\u2500 8f4116d6de194a32a75ddfea5c4de733.parquet\n\u2502   \u2502   \u251c\u2500\u2500 dad7eae44e784b549c8c5a3aa051a8c7.parquet\n\u2502   \u2502   \u2514\u2500\u2500 ef5bf698308b481992c4e6b3fe952738.parquet\n\u2502   \u251c\u2500\u2500 t=003.2\n\u2502   \u2502   \u251c\u2500\u2500 1\n.\n.\n.\n\u2514\u2500\u2500 locations\n    \u251c\u2500\u2500 t=000.0\n    \u2502   \u251c\u2500\u2500 19b9aa10588646b3bf22c9b4865a7995.parquet\n    \u2502   \u2514\u2500\u2500 dad7eae44e784b549c8c5a3aa051a8c7.parquet\n    \u251c\u2500\u2500 t=001.5\n    \u2502   \u251c\u2500\u2500 19b9aa10588646b3bf22c9b4865a7995.parquet\n    \u2502   \u251c\u2500\u2500 1f0490ec0c464285bebf75ddc77d55cd.parquet\n    \u2502   \u251c\u2500\u2500 8f4116d6de194a32a75ddfea5c4de733.parquet\n    \u2502   \u251c\u2500\u2500 dad7eae44e784b549c8c5a3aa051a8c7.parquet\n    \u2502   \u2514\u2500\u2500 ef5bf698308b481992c4e6b3fe952738.parquet\n    \u251c\u2500\u2500 t=003.2\n    \u2502   \u251c\u2500\u2500 19b9aa10588646b3bf22c9b4865a7995.parquet\n    \u2502   \u251c\u2500\u2500 1f0490ec0c464285bebf75ddc77d55cd.parquet\n    \u2502   \u251c\u2500\u2500 2bde3df6005e4dfe8dc4e6f924a7a1e9.parquet\n    .\n    .\n    .\n    \u2502   \u2514\u2500\u2500 e3b8748d226e4b54b85e06ec4a6387a6.parquet\n    \u251c\u2500\u2500 t=128.0\n    \u2502   \u251c\u2500\u2500 1fe7295294fd498385d1946140d40db1.parquet\n    \u2502   \u251c\u2500\u2500 2094231ab7af41658702c1a3a1da7272.parquet\n    \u2502   \u251c\u2500\u2500 771b57ec8774441ca65dacf1dca57edd.parquet\n    \u2502   \u2514\u2500\u2500 e3b8748d226e4b54b85e06ec4a6387a6.parquet\n    \u2514\u2500\u2500 t=128.8\n        \u251c\u2500\u2500 2094231ab7af41658702c1a3a1da7272.parquet\n        \u251c\u2500\u2500 771b57ec8774441ca65dacf1dca57edd.parquet\n        \u2514\u2500\u2500 e3b8748d226e4b54b85e06ec4a6387a6.parquet\n</code></pre> <p>In this example, the folder <code>t=000.0</code> is the start of the simulated data.  The folder <code>t=052.2</code> is 52.2 seconds into the simulation and <code>t=128.8</code> is 128.8 seconds into the simulation.</p>"},{"location":"lessons/12-week/week08/#assignment-8_1","title":"Assignment 8","text":"<p>The first part of the assignment involves creating a Jupyter notebook that mimics a real-time streaming data feed. The basic loop for the notebook is simple.  The notebook should load processed data and publish data at the appropriate time. You can use either the time given in the parquet partition or you can use the <code>offset</code> data found within the parquet data. For example, once your notebook has passed the 52.5-second mark it should load the data from the <code>t=052.5</code> directory and publish it to the appropriate Kafka topic. Similarly, you could example the <code>offset</code> column and publish the data at the appropriate time. </p> <p>Hint</p> <p>You may want to use the Python heapq library as an event queue. </p> <p>The DSC 650 Github contains example notebooks you can use to help you create topics, publish data to a Kafka broker, and consume the data. </p> <p>Use the following parameters when publishing simulated data to the Bellevue University Data Science Cluster Kafka broker. </p> Bootstrap Server kafka.kafka.svc.cluster.local:9092 Location Topic LastnameFirstname-locations Acceleration Topic LastnameFirstname-accelerations <p>The following code is an example of code that uses the <code>kafka-python</code> library to publish a message to Kafka topic using a JSON serializer. </p> <pre><code>import json\nfrom kafka import KafkaProducer\n\nbootstrap_server = 'kafka.kafka.svc.cluster.local:9092'\n\nproducer = KafkaProducer(\n  bootstrap_servers=[bootstrap_server],\n  value_serializer=lambda x: json.dumps(x).encode('utf-8')\n)\n\nproducer.send(\n  'DoeJohn-locations', \n  {\"dataObjectID\": \"test1\"}\n)\n</code></pre> <p>Hint</p> <p>When creating the notebook producer, you may want to automatically restart sending the data from the beginning when you reach the end of the dataset. This enables you to continue testing without having to manually restart the notebook.</p> <p>The following code is an example that uses the <code>kafka-python</code> library to consume messages from a Kafka topic. You should create another Jupyter notebook to consume messages from the Kafka producer to validate that you are properly publishing messages to the appropriate topic. </p> <pre><code>from kafka import KafkaConsumer\n\nbootstrap_server = 'kafka.kafka.svc.cluster.local:9092'\n\n# To consume latest messages and auto-commit offsets\nconsumer = KafkaConsumer(\n    'DoeJohn-locations',\n    bootstrap_servers=[bootstrap_server]\n)\n</code></pre> <p>Note</p> <p>While creating a separate notebook that acts as a Kafka consumer is not strictly necessary for the assignment, it is recommended that you create one to aid in debugging and testing. </p>"},{"location":"lessons/12-week/week08/#discussion-board","title":"Discussion Board","text":"<p>You are required to have a minimum of 10 posts each week.  Similar to previous courses, any topic counts towards your discussion count, as long as you are active more than 2 days per week with 10 posts, you will receive full credit. Refer to the optional topics below as a starting place.</p>"},{"location":"lessons/12-week/week08/#topic-1","title":"Topic 1","text":"<p>Kafka and other data systems make heavy use of the log data structure. What is the log data structure? What problems does it solve that makes it useful for distributed systems.  What other data systems make use of this data structure? </p>"},{"location":"lessons/12-week/week08/#topic-2","title":"Topic 2","text":"<p>Representational State Transfer (REST) is a software architectural style often used to create web services. One of the key properties of REST is an emphasis on not sharing state between the client and the server application. As Roy Fielding explained in his doctoral dissertation: </p> <p>We next add a constraint to the client-server interaction: communication must be stateless in nature, as in the client-stateless-server (CSS) style of Section 3.4.3 (Figure 5-3), such that each request from the client to server must contain all of the information necessary to understand the request, and cannot take advantage of any stored context on the server. Session state is therefore kept entirely on the client.</p> <p>How does this style of architecture compare to synchronous architectures such as an AMQP message broker? What properties of REST make it suitable for web-scale applications? </p>"},{"location":"lessons/12-week/week09/","title":"Week 9","text":"<p>In this lesson, we will learn about real-time data streams, message systems, and transactions in distributed systems. </p>"},{"location":"lessons/12-week/week09/#objectives","title":"Objectives","text":"<p>After completing this week, you should be able to:</p> <ul> <li>Implement scalable stream processing in Spark</li> <li>Explain different approaches to transactions in distributed systems and the associated trade-offs</li> </ul>"},{"location":"lessons/12-week/week09/#readings","title":"Readings","text":"<ul> <li>Read chapter 11 in Designing Data-Intensive Applications</li> <li>(Optional) Read chapters 8 in Designing Data-Intensive Applications</li> </ul>"},{"location":"lessons/12-week/week09/#weekly-resources","title":"Weekly Resources","text":"<ul> <li>etcd</li> <li>Kafka Use Cases</li> <li>Kafka Introduction</li> <li>The Log: What every software engineer should know about real-time data's unifying abstraction</li> <li>Representational State Transfer (REST)</li> <li>Spark Structured Streaming</li> <li>Zookeeper</li> </ul>"},{"location":"lessons/12-week/week09/#assignment-9","title":"Assignment 9","text":"<p>In the second part of the exercise, you will create two streaming dataframes using the <code>accelerations</code> and <code>locations</code> folders. </p>"},{"location":"lessons/12-week/week09/#assignment-91","title":"Assignment 9.1","text":"<p>Start by creating a simple Spark Streaming application that reads data from the <code>accelerations</code> and <code>locations</code> topics and uses the Kafka sink to save the results to the <code>LastnameFirstname-simple</code> topic. </p>"},{"location":"lessons/12-week/week09/#assignment-92","title":"Assignment 9.2","text":"<p>Define a watermark on the locations dataframe using the <code>timestamp</code> column. Set the threshold for the watermark at \"30 seconds\". Set a window of \"15 seconds\" and compute the mean speed of each ride defined by the <code>ride_id</code>. Save the results in <code>LastnameFirstname-windowed</code> and set the output mode to <code>update</code>.</p>"},{"location":"lessons/12-week/week09/#assignment-93","title":"Assignment 9.3","text":"<p>Join the two streams together on the <code>ride_id</code> as an inner join.  Save the results in <code>LastnameFirstname-joined</code>. </p>"},{"location":"lessons/12-week/week09/#submission-instructions","title":"Submission Instructions","text":"<p>For this assignment, you will submit a zip archive containing the contents of the <code>dsc650/assignments/assignment09/</code> directory. Use the naming convention of <code>assignment09_LastnameFirstname.zip</code> for the zip archive. You can create this archive in Bash (or a similar Unix shell) using the following commands. </p> <pre><code>cd dsc650/assignments\nzip -r assignment09_DoeJane.zip assignment08\n</code></pre> <p>Likewise, you can create a zip archive using Windows PowerShell with the following command. </p> <pre><code>Compress-Archive -Path assignment09 -DestinationPath 'assignment09_DoeJane.zip\n</code></pre>"},{"location":"lessons/12-week/week09/#discussion-board","title":"Discussion Board","text":"<p>You are required to have a minimum of 10 posts each week.  Similar to previous courses, any topic counts towards your discussion count, as long as you are active more than 2 days per week with 10 posts, you will receive full credit. Refer to the optional topic below as a starting place.</p> <p>Describe how different database systems handle transactions.  Pick three or more different systems to compare and contrast.</p>"},{"location":"lessons/12-week/week10/","title":"Week 10","text":"<p>In this lesson we learn how to preprocess text-based data and train deep learning models on that data.  </p>"},{"location":"lessons/12-week/week10/#objectives","title":"Objectives","text":"<p>After completing this week, you should be able to:</p> <ul> <li>Transform text input into tokens and convert those tokens into numeric vectors using one-hot encoding and feature hashing.</li> <li>Build basic text-processing models using recurrent neural networks (RNN)</li> <li>Understand how word embeddings such as Word2Vec can help improve the performance of text-processing models</li> </ul>"},{"location":"lessons/12-week/week10/#readings","title":"Readings","text":"<ul> <li>Read chapter 6 in Deep Learning with Python</li> </ul>"},{"location":"lessons/12-week/week10/#weekly-resources","title":"Weekly Resources","text":"<ul> <li>Global Vectors for Word Representation</li> <li>Large Movie Review Dataset</li> <li>Extracting, transforming and selecting features</li> </ul>"},{"location":"lessons/12-week/week10/#assignment-10","title":"Assignment 10","text":""},{"location":"lessons/12-week/week10/#assignment-101","title":"Assignment 10.1","text":"<p>In the first part of the assignment, you will implement basic text-preprocessing functions in Python.  These functions do not need to scale to large text documents and will only need to handle small inputs. </p>"},{"location":"lessons/12-week/week10/#assignment-101a","title":"Assignment 10.1.a","text":"<p>Create a <code>tokenize</code> function that splits a sentence into words. Ensure that your tokenizer removes basic punctuation. </p> <pre><code>def tokenize(sentence):\n    tokens = []\n    # tokenize the sentence\n    return tokens\n````\n\n#### Assignment 10.1.b\n\nImplement an `ngram` function that splits tokens into N-grams. \n\n```python\ndef ngram(tokens, n):\n    ngrams = []\n    # Create ngrams\n    return ngrams\n</code></pre>"},{"location":"lessons/12-week/week10/#assignment-101c","title":"Assignment 10.1.c","text":"<p>Implement an <code>one_hot_encode</code> function to create a vector from a numerical vector from a list of tokens. </p> <pre><code>def one_hot_encode(tokens, num_words):\n    token_index = {}\n    results = ''\n    return results\n</code></pre>"},{"location":"lessons/12-week/week10/#102","title":"10.2","text":"<p>Using listings 6.16, 6.17, and 6.18 in Deep Learning with Python as a guide, train a sequential model with embeddings on the IMDB data found in <code>data/external/imdb/</code>. Produce the model performance metrics and training and validation accuracy curves within the Jupyter notebook.</p>"},{"location":"lessons/12-week/week10/#103","title":"10.3","text":"<p>Using listing 6.27 in Deep Learning with Python as a guide, fit the same data with an LSTM layer. Produce the model performance metrics and training and validation accuracy curves within the Jupyter notebook.</p>"},{"location":"lessons/12-week/week10/#104","title":"10.4","text":"<p>Using listing 6.46 in Deep Learning with Python as a guide, fit the same data with a simple 1D convnet. Produce the model performance metrics and training and validation accuracy curves within the Jupyter notebook.</p>"},{"location":"lessons/12-week/week10/#submission-instructions","title":"Submission Instructions","text":"<p>For this assignment, you will submit a zip archive containing the contents of the <code>dsc650/assignments/assignment10/</code> directory. Use the naming convention of <code>assignment10_LastnameFirstname.zip</code> for the zip archive. You can create this archive in Bash (or a similar Unix shell) using the following commands. </p> <pre><code>cd dsc650/assignments\nzip -r assignment10_DoeJane.zip assignment09\n</code></pre> <p>Likewise, you can create a zip archive using Windows PowerShell with the following command. </p> <pre><code>Compress-Archive -Path assignment10 -DestinationPath 'assignment10_DoeJane.zip\n</code></pre>"},{"location":"lessons/12-week/week10/#discussion-board","title":"Discussion Board","text":"<p>You are required to have a minimum of 10 posts each week.  Similar to previous courses, any topic counts towards your discussion count, as long as you are active more than 2 days per week with 10 posts, you will receive full credit. Refer to the optional topics below as a starting place.</p>"},{"location":"lessons/12-week/week10/#topic-1","title":"Topic 1","text":"<p>Compare and contrast using MapReduce, Spark, and Deep Learning Frameworks (e.g. TensorFlow) for performing text preprocessing and building text-based models. Are there use cases where it makes sense to use one over another? </p>"},{"location":"lessons/12-week/week10/#topic-2","title":"Topic 2","text":"<p>How might you combine stream processing such as Spark's stream processing framework with deep learning models? Provide use cases that are relevant to your professional or personal interests. </p>"},{"location":"lessons/12-week/week11/","title":"Week 11","text":"<p>In this lesson, we will explore the future of big data and deep learning. </p>"},{"location":"lessons/12-week/week11/#objectives","title":"Objectives","text":"<p>After completing this week, you should be able to:</p> <ul> <li>Describe upcoming advances in big data and deep learning and their potential use cases</li> <li>Experiment with advanced deep learning use cases including text and image generation</li> </ul>"},{"location":"lessons/12-week/week11/#readings","title":"Readings","text":"<ul> <li>Chapter 12 in Designing Data-Intensive Applications</li> <li>Read chapters 8 and 9 in Deep Learning with Python</li> </ul>"},{"location":"lessons/12-week/week11/#weekly-resources","title":"Weekly Resources","text":""},{"location":"lessons/12-week/week11/#assignment-11","title":"Assignment 11","text":"<p>Using section 8.1 in Deep Learning with Python as a guide, implement an LSTM text generator. Train the model on the Enron corpus or a text source of your choice. Save the model and generate 20 examples to the <code>results</code> directory of <code>dsc650/assignments/assignment11/</code>.</p>"},{"location":"lessons/12-week/week11/#submission-instructions","title":"Submission Instructions","text":"<p>For this assignment, you will submit a zip archive containing the contents of the <code>dsc650/assignments/assignment11/</code> directory. Use the naming convention of <code>assignment10_LastnameFirstname.zip</code> for the zip archive. You can create this archive in Bash (or a similar Unix shell) using the following commands. </p> <pre><code>cd dsc650/assignments\nzip -r assignment10_DoeJane.zip assignment11\n</code></pre> <p>Likewise, you can create a zip archive using Windows PowerShell with the following command. </p> <pre><code>Compress-Archive -Path assignment10 -DestinationPath 'assignment11_DoeJane.zip\n</code></pre>"},{"location":"lessons/12-week/week11/#discussion-board","title":"Discussion Board","text":"<p>You are required to have a minimum of 10 posts each week.  Similar to previous courses, any topic counts towards your discussion count, as long as you are active more than 2 days per week with 10 posts, you will receive full credit. </p>"},{"location":"lessons/12-week/week12/","title":"Week 12","text":"<p>In this lesson, we will explore the future of big data and deep learning. </p>"},{"location":"lessons/12-week/week12/#objectives","title":"Objectives","text":"<p>After completing this week, you should be able to:</p> <ul> <li>Describe upcoming advances in big data and deep learning and their potential use cases</li> <li>Experiment with advanced deep learning use cases including text and image generation</li> </ul>"},{"location":"lessons/12-week/week12/#readings","title":"Readings","text":"<ul> <li>Chapter 12 in Designing Data-Intensive Applications</li> <li>Read chapters 8 and 9 in Deep Learning with Python</li> </ul>"},{"location":"lessons/12-week/week12/#weekly-resources","title":"Weekly Resources","text":""},{"location":"lessons/12-week/week12/#assignment-12","title":"Assignment 12","text":"<p>Using section 8.4 in Deep Learning with Python as a guide, implement a variational autoencoder using the MNIST data set and save a grid of 15 x 15 digits to the <code>results/vae</code> directory. If you would rather work on a more interesting dataset, you can use the CelebFaces Attributes Dataset instead. </p>"},{"location":"lessons/12-week/week12/#submission-instructions","title":"Submission Instructions","text":"<p>For this assignment, you will submit a zip archive containing the contents of the <code>dsc650/assignments/assignment12/</code> directory. Use the naming convention of <code>assignment12_LastnameFirstname.zip</code> for the zip archive. You can create this archive in Bash (or a similar Unix shell) using the following commands. </p> <pre><code>cd dsc650/assignments\nzip -r assignment12_DoeJane.zip assignment10\n</code></pre> <p>Likewise, you can create a zip archive using Windows PowerShell with the following command. </p> <pre><code>Compress-Archive -Path assignment10 -DestinationPath 'assignment12_DoeJane.zip\n</code></pre>"},{"location":"lessons/12-week/week12/#discussion-board","title":"Discussion Board","text":"<p>You are required to have a minimum of 10 posts each week.  Similar to previous courses, any topic counts towards your discussion count, as long as you are active more than 2 days per week with 10 posts, you will receive full credit. Refer to the optional topics below as a starting place.</p> <p>Topics include, but are not limited to Kubernetes, deep learning hardware, and cloud computing.</p>"},{"location":"lessons/fundamentals/latency/","title":"Latency","text":"<p>Latency Numbers Every Data Scientist Should Know</p> <p>In the 2000s, Jeff Dean, a Google Senior Fellow in their Research Group, presented a list of latency numbers that every programmer should know. These numbers describe how long it takes to perform certain actions within distributed programs. Since then, it has been updated and expanded upon.</p> <p>Below is yet another update on these numbers with data taken from Colin Scott, a Berkeley researcher. An interactive version of this repository can be found here.</p> Action Latency (ns)1 Latency (\u03bc)2 Latency (ms)3 L1 cache reference 1 ns Branch mispredict 3 ns L2 cache reference 4 ns Mutex lock/unlock 17 ns Main memory reference 100 ns Compress 1KB with Zippy 2,000 ns 2 \u03bcs Send 1KB over 1 Gbps network 10,000 ns 10 \u03bcs SSD random read 16,000 ns 16 \u03bcs Read 1 MB sequentially from SSD 49,000 ns 49 \u03bcs Read 1 MB sequentially from memory 250,000 ns 250 \u03bcs Round trip within same datacenter 500,000 ns 500 \u03bcs Read 1 MB sequentially from disk 825,000 ns 825 \u03bcs Disk seek 2,000,000 ns 2,000 \u03bcs 2 ms Send packet CA-&gt;Netherlands-&gt;CA 150,000,000 ns 150,000 \u03bcs 150 ms <p>Notes</p> <ol> <li> <p>1 ns = 10-9 seconds\u00a0\u21a9</p> </li> <li> <p>1 \u03bcs = 10-6 seconds = 1,000 ns\u00a0\u21a9</p> </li> <li> <p>1 ms = 10-3 seconds = 1,000 \u03bcs = 1,000,000 ns\u00a0\u21a9</p> </li> </ol>"},{"location":"lessons/fundamentals/size/","title":"Data Size","text":"<p>Metric</p> Value Unit Symbol Notes 1 Byte B 1 byte = 1 letter in computer memory 10^{3}10^{3} Kilobyte kB 2 kB = RAM on original NES 10^{6}10^{6} Megabyte MB 1 MB \u2248 1 HD quality photo 10^{9}10^{9} Gigabyte GB 1 GB \u2248 114 minutes of uncompressed CD audio 10^{12}10^{12} Terabyte TB 1.9 TB \u2248 Size of all multimedia files used in English wikipedia on May 2012 10^{15}10^{15} Petabyte PB 10 PB \u2248 Size of Library of Congress collection in 2005 10^{18}10^{18} Exabyte EB 15 EB \u2248 storage space at Google data warehouse as of 2013 10^{21}10^{21} Zettabyte ZB 6.9 ZB \u2248 amount of data accessed by Americans in 2012 10^{24}10^{24} Yottabyte YB 1 YB \u2248 131 TB for every person on Earth <p>Binary</p> Value Unit Symbol Notes 2^{10}2^{10} Kibibyte Ki 1024 bytes 2^{20}2^{20} Mebibyte Mi 1024 kibibytes 2^{30}2^{30} Gibibyte Gi 1024 mebibytes 2^{40}2^{40} Tebibyte Ti 1024 gibibytes 2^{50}2^{50} Pebibyte Pi 1024 tebibytes 2^{60}2^{60} Exbibyte Ei 1024 pebibytes 2^{70}2^{70} Zebibyte Zi 1024 exbibytes 2^{80}2^{80} Yobibyte Yi 1024 zebibytes"},{"location":"setup/","title":"Overview","text":"<p>Note</p> <p>It is recommended that you use the hosted option for this class.  </p>"},{"location":"setup/#operating-system-dependencies","title":"Operating System Dependencies","text":"<p>See the pages hosted, macOS, Ubuntu, or Windows 10 for how to install the operating system specific dependencies for your computer.  </p>"},{"location":"setup/#clone-github-repository","title":"Clone Github Repository","text":"<p>Start by cloning or downloading this repository to your local computer.  You can clone this repository using the Github Desktop Client or using the Git command line.</p> <p>Clone using SSH</p> <pre><code>git clone git@github.com:bellevue-university/dsc650.git\n</code></pre> <p>Clone using HTTPS</p> <pre><code>git clone https://github.com/bellevue-university/dsc650.git\n</code></pre> <p>You will need access to this repository throughout the course, so place it in a reliable location.</p>"},{"location":"setup/hosted/","title":"Hosted Environments","text":""},{"location":"setup/hosted/#bellevue-university-data-science-cluster","title":"Bellevue University Data Science Cluster","text":"<p>Bellevue University hosts a JupyterHub instance that includes TensorFlow, PySpark, R, and other data science libraries. To access JupyterHub, start by going to https://workspace.bellevue.edu/. Download and install the VMware Horizon Client.</p> <p></p> <p>Once you have installed the client, select the add server option, and add <code>workspace.bellevue.edu</code>.</p> <p></p> <p>After you add the server, login to the client using your Bellevue University username and password.</p> <p></p> <p>Once you log in, you should see multiple desktops which should include the DSC Desktop. Select the DSC Desktop option which should take you to a Windows virtual machine that is pre-configured to use the JupyterHub instance. </p> <p></p> <p>Within the DSC Desktop instance, open either Firefox or Chrome.  If the homepage doesn't automatically take you to the Bellevue Data Science Cluster homepage, you can manually navigate to it at http://home.budsc.midwest-datascience.com/. </p> <p></p> <p>This homepage is the landing page for Data Science Cluster applications. Currently, JupyterHub is the only capability available to students, but more capabilities will be available in the future. To access JupyterHub instance, click on the JupyterHub logo</p> <p>You will be asked to log in using your Github account. You can use whatever Github account you would like. </p> <p></p> <p></p> <p>Once you log in, you should be taken to a JupyterHub notebook.  This notebook is backed by persistent storage with 50GB. </p> <p></p>"},{"location":"setup/hosted/#google-colaboratory","title":"Google Colaboratory","text":"<p>Colaboratory, or \"Colab\" for short, provides hosted Jupyter notebooks with free access to GPUs and Tensorflow. </p>"},{"location":"setup/hosted/#databricks-community-edition","title":"Databricks Community Edition","text":"<p>Databricks Community Edition provides free access to notebooks configured with PySpark. </p>"},{"location":"setup/macOS/","title":"macOS","text":"<p>Documentation in Progress</p> <p>Check back soon for more updates.</p>"},{"location":"setup/macOS/#package-manager","title":"Package Manager","text":"<p>If you are using macOS as your primary development environment, I recommend using a package manager like homebrew. </p> <p>A package manager is a tool that automates the process of installing, updating, configuring, and removing computer programs. Package managers are commonly used on Unix and Linux distributions. Debian Linux systems, like Ubuntu, use <code>aptitude</code>. Red Hat and Fedora systems use <code>yum</code>.  <code>MacPorts</code> and <code>homebrew</code> are two popular package managers for macOS. <code>Chocolatey</code> is a popular package manager for Windows. </p> <p>You can install Homebrew on your system by executing the following command in your terminal. </p> <pre><code>/bin/bash -c \"$(curl -fsSL https://raw.githubusercontent.com/Homebrew/install/master/install.sh)\"\n</code></pre> <p>I also recommend using Homebrew Cask to install graphical applications like Atom and Google Chrome. </p> <p>After you have installed Homebrew, update your package index. </p> <pre><code>brew update\n</code></pre> <p>Finally, install the following packages required for this course. </p> <pre><code>brew install apache-arrow\nbrew install apache-spark\nbrew install avro-tools\nbrew install git\nbrew install hadoop\nbrew install libtensorflow\nbrew install pandoc\nbrew install pandoc-citeproc\nbrew install pandoc-crossref\nbrew install parquet-tools\nbrew install protobuf\nbrew install snappy\n</code></pre> <p>Optionally, you can install the following packages using Homebrew Cask. </p> <pre><code>brew cask install anaconda\nbrew cask install atom\nbrew cask install github\nbrew cask install mactex\nbrew cask install miniconda\nbrew cask install virtualbox\n</code></pre>"},{"location":"setup/macOS/#jdk","title":"JDK","text":"<p>Spark and Hadoop use version 8 of the Java Development Kit (JDK 8). Download the latest version from Oracle and install on your local machine. Once completed, edit your shell profile \u2013 <code>$HOME/.bash_profile</code> if you are using Bash or <code>$HOME/.zshrc</code> if you are using <code>Zsh</code> \u2013 and add the following. </p> <pre><code>export JAVA_HOME=/Library/Java/JavaVirtualMachines/jdk1.8.0_211.jdk/Contents/Home\n</code></pre>"},{"location":"setup/macOS/#tensorflow","title":"TensorFlow","text":"<p>Note</p> <p>There is no GPU support for macOS</p>"},{"location":"setup/ubuntu/","title":"Ubuntu","text":"<p>Documentation in Progress</p> <p>Check back soon for more updates.</p>"},{"location":"setup/ubuntu/#system","title":"System","text":"<p>Edit <code>/etc/hosts</code>:</p> <pre><code>127.0.0.1 hostname\n</code></pre> <p>Install JDK, Scala and Git:</p> <pre><code>sudo apt install default-jdk scala git -y\n</code></pre> <p>Install Poetry:</p> <pre><code>curl -sSL https://raw.githubusercontent.com/python-poetry/poetry/master/get-poetry.py | python3\n</code></pre> <p>Install Oracle JDK:</p> <pre><code>sudo apt update\nsudo add-apt-repository ppa:webupd8team/java\nsudo apt update\nsudo apt install oracle-java8-installer oracle-java8-set-default\n</code></pre>"},{"location":"setup/ubuntu/#spark","title":"Spark","text":"<p>This guide provides more information on how to setup Spark on Ubuntu. </p> <p>Start by downloading Spark 2.4.5 for Hadoop 2.7. </p> <pre><code>curl -O https://www.apache.org/dyn/closer.lua/spark/spark-2.4.5/spark-2.4.5-bin-hadoop2.7.tgz\n</code></pre> <p>Extract the archive. </p> <pre><code>tar xvf spark-2.4.5-bin-hadoop2.7.tgz\n</code></pre> <p>Move it to <code>/opt/spark</code>.</p> <pre><code>sudo mv spark-2.4.5-bin-hadoop2.7/ /opt/spark\n</code></pre> <p>Update the environment variables by adding the following to your shell profile.  </p> <pre><code>export SPARK_HOME=/opt/spark\nexport PATH=$PATH:$SPARK_HOME/bin:$SPARK_HOME/sbin\nexport PYSPARK_PYTHON=/usr/bin/python3\n</code></pre> <p>Alternatively, add it to your profile using <code>echo</code>. </p> <pre><code>echo \"export SPARK_HOME=/opt/spark\" &gt;&gt; ~/.profile\necho \"export PATH=$PATH:$SPARK_HOME/bin:$SPARK_HOME/sbin\" &gt;&gt; ~/.profile\necho \"export PYSPARK_PYTHON=/usr/bin/python3\" &gt;&gt; ~/.profile\n</code></pre> <p>This assumes your profile is in <code>.profile</code>. It may also be in another location like <code>~/.bashrc</code> or <code>~/.zshrc</code>.  Activate your changes as follows. </p> <pre><code>source ~/.bashrc\n</code></pre> <p>Start a stand-alone server. </p> <pre><code>start-master.sh\n</code></pre> <p>The process will listen on 8080. </p> <pre><code>ss -tunelp | grep 8080\ntcp   LISTEN  0       1                           *:8080  </code></pre> <p>Start a worker process. </p> <pre><code>start-slave.sh spark://ubuntu:7077\n</code></pre> <p>You can stop the processes using the following commands. </p> <pre><code>stop-slave.sh\nstop-master.sh\n</code></pre>"},{"location":"setup/ubuntu/#tensorflow","title":"TensorFlow","text":"<p>Ubuntu 18.04 ships with Python 3 by default</p> <pre><code>sudo apt install python3-venv\n</code></pre> <p>Note</p> <p>If you have a dedicated NVIDIA GPU and want to take advantage of its processing power, instead of tensorflow install the tensorflow-gpu package which includes GPU support.</p>"},{"location":"setup/ubuntu/#gpu-support","title":"GPU Support","text":"<p>Check the following links to more information on GPU support. </p> <ul> <li>CUDA GPUs</li> <li>TensorFlow GPU Install</li> </ul> <pre><code># Add NVIDIA package repositories\nwget https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64/cuda-repo-ubuntu1804_10.1.243-1_amd64.deb\nsudo apt-key adv --fetch-keys https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64/7fa2af80.pub\nsudo dpkg -i cuda-repo-ubuntu1804_10.1.243-1_amd64.deb\nsudo apt-get update\nwget http://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64/nvidia-machine-learning-repo-ubuntu1804_1.0.0-1_amd64.deb\nsudo apt install ./nvidia-machine-learning-repo-ubuntu1804_1.0.0-1_amd64.deb\nsudo apt-get update\n\n# Install NVIDIA driver\nsudo apt-get install --no-install-recommends nvidia-driver-430\n# Reboot. Check that GPUs are visible using the command: nvidia-smi\n\n# Install development and runtime libraries (~4GB)\nsudo apt-get install --no-install-recommends \\\ncuda-10-1 \\\nlibcudnn7=7.6.4.38-1+cuda10.1  \\\nlibcudnn7-dev=7.6.4.38-1+cuda10.1\n\n\n# Install TensorRT. Requires that libcudnn7 is installed above.\nsudo apt-get install -y --no-install-recommends libnvinfer6=6.0.1-1+cuda10.1 \\\nlibnvinfer-dev=6.0.1-1+cuda10.1 \\\nlibnvinfer-plugin6=6.0.1-1+cuda10.1\n</code></pre>"},{"location":"setup/windows/","title":"Windows","text":"<p>Documentation in Progress</p> <p>Check back soon for more updates.</p>"},{"location":"setup/windows/#overview","title":"Overview","text":"<p>There are multiple options available for installing Spark, Hadoop, TensorFlow, and other Big Data/Deep Learning software on Windows 10. While it is possible to install these packages and use these packages on Windows, I strongly urge you to heed the warning of Fran\u00e7ois Chollet, author of Deep Learning With Python.</p> <p>Whether you\u2019re running locally or in the cloud, it\u2019s better to be using a Unix workstation. Although it\u2019s technically possible to use Keras on Windows (all three Keras backends support Windows), We don\u2019t recommend it. In the installation instructions in appendix A, we\u2019ll consider an Ubuntu machine. If you\u2019re a Windows user, the simplest solution to get everything running is to set up an Ubuntu dual boot on your machine. It may seem like a hassle, but using Ubuntu will save you a lot of time and trouble in the long run.</p>"},{"location":"setup/windows/#prerequisites","title":"Prerequisites","text":"<p>Install this software prior to setting up your environment.</p> <ul> <li>Atom Optional</li> <li>GitHub Desktop</li> <li>Git</li> <li>Anaconda</li> <li>Java Development Kit 8</li> <li>PyCharm</li> </ul>"},{"location":"setup/windows/#clone-github-repository","title":"Clone GitHub Repository","text":"<p>Using GitHub Desktop, clone the <code>bellevue-university/dsc650</code> repository by going to https://github.com/bellevue-university/dsc650 and selecting the <code>Open In Desktop</code> option.</p> <p></p> <p>Clone the repository to your local system by selecting the appropriate local directory. After selecting the directory, you will see a screen that shows the repository cloning to your local directory.  This process may take a long time (minutes to hours), so wait until it is completed.</p> <p></p> <p></p>"},{"location":"setup/windows/#import-environment","title":"Import Environment","text":"<p>Next, open Anaconda Navigator.  </p> <p></p> <p>Select the <code>channels</code> option to add the <code>conda-forge</code> channel.  </p> <p></p> <p></p> <p>After you have finished adding the Conda Forge channel, import a new environment by selecting the environments tab and the import option.  </p> <p></p> <p>Import the <code>environment.yaml</code> file from the <code>dsc650</code> repository to the <code>dsc650</code> environment. This will create an Anaconda environment with the appropriate dependencies.</p>"},{"location":"setup/windows/#open-pycharm-project","title":"Open PyCharm Project","text":"<p>Open PyCharm where you should see a screen welcoming you to Pycharm.</p> <p></p> <p>Select the open option and open the directory where you cloned the <code>dsc650</code> repository.</p> <p></p> <p>When you initially open the project, it may ask you to fix issues with Windows Defender.  Fix the issues by clicking the <code>fix</code> option and following the prompts.</p> <p></p> <p></p> <p>PyCharm should automatically use the previously created <code>dsc650</code> environment.  If not, go to the <code>project-interpreter</code> option in the menu and add the Conda environment.</p> <p></p> <p></p>"},{"location":"setup/windows/#set-sources-root","title":"Set Sources Root","text":"<p>After opening the project, right click the <code>dsc650</code> directory and add it as a sources root.</p> <p></p>"},{"location":"setup/windows/#set-java-home","title":"Set Java Home","text":"Variable Value JAVA_HOME C:\\Program Files\\Java\\jdk1.8.0_251 <p>Go the <code>edit system environment variables</code> in your control panel.</p> <p></p> <p>Under <code>System Properties -&gt; Advanced</code> select <code>Environment Variables</code>.</p> <p></p> <p>Change the environment variables for your user.</p> <p></p> <p>Exit out of PyCharm and re-open to ensure it sets the environment variables.  </p>"},{"location":"setup/windows/#run-examples","title":"Run Examples","text":"<p>Run the TensorFlow example.</p> <p></p> <p>Run the PySpark example.</p> <p></p> <p>Or you can run the PySpark example in the terminal.</p> <p></p>"},{"location":"setup/windows/#package-manager-optional","title":"Package Manager (Optional)","text":"<p>If you are using Windows as your primary development environment, I recommend using a package manager like Chocolatey.</p> <p>A package manager is a tool that automates the process of installing, updating, configuring, and removing computer programs. Package managers are commonly used on Unix and Linux distributions. Debian Linux systems, like Ubuntu, use <code>aptitude</code>. Red Hat and Fedora systems use <code>yum</code>.  <code>MacPorts</code> and <code>homebrew</code> are two popular package managers for macOS.</p> <p>Follow the Chocolatey installation guide to install the package manager on your system.  Once you have completed installing the package manager, you can install new software by running PowerShell as an administrator and using the <code>choco</code> command.  For example, the following commands will install the latest versions of Adobe Acrobat Reader, Google Chrome, and FireFox on your system.</p> <pre><code>choco install adobereader\nchoco install googlechrome\nchoco install firefox\n</code></pre> <p>You can upgrade all packages using <code>choco upgrade all</code> or upgrade individual packages using <code>choco upgrade firefox</code>.  Similarly, you can uninstall packages using <code>choco uninstall</code>.</p> <p>The following is a table of software you might find useful for this course.</p> Software Package Name Anaconda Distribution (Python 3.x) anaconda3 Git (Install) git.install GitHub Desktop github-desktop Graphviz graphviz Hadoop hadoop Java Development Kit 8 jdk8 JetBrains Toolbox App jetbrainstoolbox JetBrains DataGrip datagrip JetBrains PyCharm pycharm JetBrains PyCharm Educational pycharm-edu JetBrains PyCharm (Community Edition) 1 pycharm-community MikTeX miktex Pandoc pandoc Pandoc CrossRef pandoc-crossref PostgreSQL postgresql Protocol Buffers protoc Scala scala VirtualBox virtualbox <p>If you are interested to see what other packages are available, see Chocolatey packages for a list of community maintained packages.</p> <ol> <li> <p>While you can use the community version of PyCharm, JetBrains offers free educational licenses for students and teachers. See educational licenses for more details.\u00a0\u21a9</p> </li> </ol>"}]}